import { Injectable } from '@angular/core';
import { ReplaySubject } from 'rxjs';
import Flexsearch from 'flexsearch';

export type Notebook = {
  id: string
  name: string
}

export type Note = {
  id: string
  name: string
  text: string
  createdAt?: Date
  updatedAt?: Date,
  metadata?: string
  fileType?: string
}

export type AppAction = {
  name: string
  callback: () => void
}
export type SearchResultGroup = {
  name: string;
  notes?: Note[]
  actions?: AppAction[]
}

@Injectable()
export class NotebookService {

  private lastSyncedAt: Date = null;

  private notebooks: Notebook[] = [];
  private notes: Note[] = notes;

  notebooksChanges = new ReplaySubject<Notebook[]>(1);
  notesChanges = new ReplaySubject<SearchResultGroup[]>(1);
  openedNoteChanges = new ReplaySubject<Note>(1);
  queryChanges = new ReplaySubject<string>(1);
  focusSearchbar = new ReplaySubject<void>(1);
  private index: Flexsearch.Document<Note>;

  constructor() {
    this.index = new Flexsearch.Document<Note>({
      tokenize: "full",
      language: "en",
      preset: "match",
      cache: true,
      context: true,
      document: {
        id: "id",
        index: ["name", 'text'],
      }
    });

    this.notes
      .filter((_, index) => index < 100)
      .forEach(note => {
        this.index.add(note)
      });
    this.notesChanges.next([
      {
        name: 'Recent',
        notes: this.notes.filter((_, index) => index < 10)
      }
    ])
  }

  async createNotebook(name: string) {
    console.log('createNotebook')
    const notebook = {
      id: Date.now().toString(10),
      name
    };
    this.notebooks.push(notebook);

    await new Promise(resolve => setTimeout(resolve, 500));

    this.notebooksChanges.next(this.notebooks);
    return notebook;
  }

  search(query: string) {
    console.log('search', query);
    const results = this.index.search({
      query,
      suggest: true,
      enrich: true,
      limit: 30
    });
    console.log(results);
    const groups: SearchResultGroup[] = results.map(perField => {
      return {
        name: perField.field,
        notes: perField.result.map(id => this.notes.find(note => note.id == id))
      }
    })
    this.notesChanges.next(groups);
  }

  async createNote(name: string) {
    const note: Note = {
      id: Date.now().toString(),
      name,
      text: '',
      createdAt: new Date()
    };
    this.notes.push(note);
    this.search(name);
  }
}

const notes: Note[] = [
  {
    "name": "2019-05/9a7282.md",
    "id": "2019-05/9a7282",
    "text": "# Public Knowledge Repositories\n\n## Nikita Voloboev\nNikita Voloboev publishes \"everything he knows\" in a wiki. \nhttps://wiki.nikitavoloboev.xyz/\n\n## Jethro's Braindump\nhttps://braindump.jethro.dev/\n\n## Andy Matuschak\nhttps://notes.andymatuschak.org/About_these_notes\n",
    "metadata": ""
  },
  {
    "name": "2019-05/e656b6.md",
    "id": "2019-05/e656b6",
    "fileType": "markdown",
    "text": "# Talent\n\nThe role of talent is overestimated and only important in a later skill stage. Initially only disciplin and stamina are relevant. \n\nTalent erhöht die Wahrscheinlichkeit für Performance () und kann sich daher positiv auf [[[Motivation]]](../2020-10/3355db.md) aufwirken.",
    "metadata": ""
  },
  {
    "name": "2020-08/6614a9.md",
    "id": "2020-08/6614a9",
    "fileType": "markdown",
    "text": "# Bananas\n\nBananas are dependent on a chemical (ethylene or acetylene) in the air around them to ripen.\nIf you keep bananas in an acetylene/ethylene-free environment, they stay green.\nGreen bananas are quite hard and sturdy, which makes transporting them easy and cheap.\nWhen the time comes to sell the bananas, you just need to provide them with acetylene and they will turn yellow and pliable in about 12 hours. \n\n--\n\nWith an impending disease that is wiping out Cavendish (the most popular kind of banana), they may not be cheap for long!\n[http://freakonomics.com/podcast/bananas/] \n",
    "metadata": ""
  },
  {
    "name": "2020-09/0efef1.md",
    "id": "2020-09/0efef1",
    "fileType": "markdown",
    "text": "# Breakthrough Starshot Project\n\nProfessor Avi Loeb (mentioned in [[[After On Podcast]]](../2020-09/31e5b4.md)) is working on the BreakThrough Starshot project, which has the goal to send tiny probes to other solar systems within reasonable time. They use sun sails [?] to accelerate those probes to high speed.",
    "metadata": ""
  },
  {
    "name": "2020-09/2be01e.md",
    "id": "2020-09/2be01e",
    "fileType": "markdown",
    "text": "# Burrito\n\nA Burrito is the perfect dish for food delivery.\n- delicious vegetarian options\n- if not too moist, the textures stay fine for some hours (hypothesis)\n- it can be preproduced easily\n- it can be heated up in a microwave and still be delicious",
    "metadata": ""
  },
  {
    "name": "2020-09/315bac.md",
    "id": "2020-09/315bac",
    "fileType": "markdown",
    "text": "# City\n\n`Cities` as conglomerates of humans are multipliers in any kind. Cities are the reason for innovation, since its first appearance about 10.000 years ago, with upcoming of agriculture, humankind developed in an incredible way at the prize of [[[Complexity]]](../2020-10/e0c1ed.md)\n\nLiving in a close proximity with humans and animals is also the reason for many diseases [[[Diseases]]](../2020-09/53dd84.md)\n\n## Simulations\nSome aspects of a city are very mechanical and therefor simple to simulate [[[Simulation]]](../2020-10/0d2e14.md) . The game `Cities Skylines` gives you the possibility to optimize traffic [[[Traffic]]](../2020-10/44384b.md) [[[Ideas]]](../2020-10/93f794.md)",
    "metadata": ""
  },
  {
    "name": "2020-09/31e5b4.md",
    "id": "2020-09/31e5b4",
    "fileType": "markdown",
    "text": "# After On Podcast\n#podcast\n\nScience Fiction SciFi author Rob Reid is doing a fantastic podcast with guests talking about topic like astro physics, artificial intelligence or psychology.\n\n[https://after-on.com/]\n\n## Episode 40: Avi Loeb\n[https://after-on.com/episodes-31-60/040]\n\nThe episode with Astrophysicist Avi Loeb [[[Breakthrough Starshot Project]]](../2020-09/0efef1.md) about the first interstellar visitor Oumuamua is exceptionally interesting. Loeb claims that some observations of Oumuamua like its acceleration when leaving our sun, are so difficult to achieve in a natural settings, and he suggests Oumuamua to be an by aliens enginered object.\n\n\n## Episode 34: Ancient DNA\n[https://after-on.com/episodes-31-60/034]\n\nDavid Reich talks about analyzing human DNA [[[DNA (Draft)]]](../2020-10/007dc0.md) and identifying ancient parts in it, that has not mutated since thoudans of years, or originated in other other human \"tribes\" like the Neanthertals. \n\n",
    "metadata": ""
  },
  {
    "name": "2020-09/3755fe.md",
    "id": "2020-09/3755fe",
    "fileType": "markdown",
    "text": "# Social Media\n\nWhen social media occured beginning of 2000 Web 2.0 called the passive user for action to participate and create content. SM is the death zone of emotional stability.\n\nJonathan Haidt: \"Social Media is Giving Kids (female) anxiety\" [https://www.youtube.com/watch?v=CI6rX96oYnY]\n\n## Content Era\nThere was a period of positive technological innovations and developments (like RSS) that convoyed the blogging movement, where people actively created their content and created their networks they want to consume from.\n\n## Likes Based Era\nLater, relevant content creation was more and more done by professionals, or those who want to become one. The majority of users became curators of content.\n\n## Backlinks\n\n- [[[Entertainment]]](../2020-10/28b3f7.md)\n- [[[Mental States]]](../2020-10/ae8b0a.md)\n",
    "metadata": ""
  },
  {
    "name": "2020-09/3c093b.md",
    "id": "2020-09/3c093b",
    "fileType": "markdown",
    "text": "# Food\nHealthy food is crucial for any metabolism in order to survive.\n\n## Related\n- [[[Food Production]]](../2020-10/4cc4c3.md)  \n- [[[Perma Culture]]](../2020-09/e30ebf.md)",
    "metadata": ""
  },
  {
    "name": "2020-09/50bdba.md",
    "id": "2020-09/50bdba",
    "fileType": "markdown",
    "text": "# Exit Strategy\n\n## Motivation\n- ageism is real\n- standing on one leg is bad\n- one specialization is boring\n- old devs are more expensive\n- the job-market is overheated\n- every 5-years the number of devs double\n\n## Ideas for an Exit Strategy\n\n- Burrito Project [[[Burrito]]](../2020-09/2be01e.md)\n- [[[Ideas]]](../2020-10/93f794.md) ",
    "metadata": ""
  },
  {
    "name": "2020-09/53dd84.md",
    "id": "2020-09/53dd84",
    "fileType": "markdown",
    "text": "# Diseases\nMost of the well known diseases for humans originated in lifestock.\n\nLethality and Transmitability.\n\n- Mumps, origin: cattle\n- Masern, origin: cattle 600 BC\n- Sars\n- Mers\n- Aids, origin: monkey\n- Pocken, origin: cattle\n- Pest, origin rats/flea\n [[[Leben im Mittelalter: Der Schwarze Tod]]](../2021-02/e51a31.md)\n ",
    "metadata": ""
  },
  {
    "name": "2020-09/631201.md",
    "id": "2020-09/631201",
    "fileType": "markdown",
    "text": "# Brainstorming\n\nEffective Brainstorming is tied to [[[Creativity]]](../2020-09/97a797.md) \n\n\u003e Michael Diehl and Wolfgang Stroebe found that, overwhelmingly, groups brainstorming together produce fewer ideas than individuals working separately.\nhttps://en.wikipedia.org/wiki/Brainstorming#Challenges_to_effective_group_brainstorming\n\nTechniques https://www.mindtools.com/brainstm.html\n\n[[revisit 2weeks]]",
    "metadata": ""
  },
  {
    "name": "2020-09/7b894e.md",
    "id": "2020-09/7b894e",
    "fileType": "markdown",
    "text": "# nested template driven forms\n\nDeclare if a component should be isolated (default) or included in the parent form control;\n```\n  viewProviders: [ { provide: ControlContainer, useExisting: NgForm } ]\n  ```\n ",
    "metadata": ""
  },
  {
    "name": "2020-09/922860.md",
    "id": "2020-09/922860",
    "fileType": "markdown",
    "text": "# angular - keyvalue pipe\n\nthe KeyValue pipe converts the object or map into an array of key-value pairs\n\n```html\n\u003cdiv *ngFor=\"let item of arrTest | keyvalue:descOrder\"\u003e\n    Key: \u003cb\u003e{{item.key}}\u003c/b\u003e and Value: \u003cb\u003e{{item.value}}\u003c/b\u003e\n\u003c/div\u003e\n\n```\n\n| keyvalue: keepOrder -- self explainig",
    "metadata": ""
  },
  {
    "name": "2020-09/97a797.md",
    "id": "2020-09/97a797",
    "fileType": "markdown",
    "text": "# Creativity\nCreativity is usually defined as the ability to generate new ideas that are both highly innovative as well as highly useful.\n\u003e \"Creative people have to be creative or else they dye\"\n\u003e \"Art required a high creativity but little usability.\"\n\u003e \"Building a career on creativity is high risk high return strategy.\" - Jordan Peterson\n\nRead: https://web.archive.org/web/20181004151415mp_/https://www.supermemo.com/articles/genius.htm\n\n## Enablers\n(source https://web.archive.org/web/20181004151415mp_/https://www.supermemo.com/articles/genius.htm )\n - suitable state of mind: alert, excited, and excitable\n - suitable environment: minimum irrelevant interference from the outside world (e.g. ringing phone) and maximum creative stimulation (e.g. creative reading, incremental reading, brainstorming, etc.)\n - time: the more time you give for an idea to grow, the greater the likelihood of a breakthrough association; two hours separated by a period of sleep may do more than two continuous hours\n - motivation: there must be a need to come up with a solution and strong motivation to document and analyze the transition steps\n - curiosity: the mind must curiously stray into unexplored paths when new associations and unexpected solutions can be found\n - knowledge: knowledge in relevant areas\n\n\n## Backlinks\n\n- [[[Innovations]]](../2020-10/927dbb.md)\n- [[[Peter Kruse (Person)]]](../2020-10/794a82.md)\n",
    "metadata": ""
  },
  {
    "name": "2020-09/980f3c.md",
    "id": "2020-09/980f3c",
    "fileType": "markdown",
    "text": "# angular - forwardRef\n\nAllows to refer to references which are not yet defined.\n\nFor instance, forwardRef is used when the token which we need to refer to for the purposes of DI is declared, but not yet defined. It is also used when the token which we use when creating a query is not yet defined.\n\nError:\n\nObject {useExisting: undefined, token: function}\n\nSolution:\n\n```javascript\nexport const formControlBinding: any = {\n  provide: NgControl,\n  useExisting: forwardRef(() =\u003e FormControlDirective)\n};\n```\n\nhttps://blog.angularindepth.com/what-is-forwardref-in-angular-and-why-we-need-it-6ecefb417d48",
    "metadata": ""
  },
  {
    "name": "2020-09/984cc6.md",
    "id": "2020-09/984cc6",
    "fileType": "markdown",
    "text": "# Power Laws\nPower laws describe the relationship between two quantities, **y = ax^k**.\n\n\n[[[Zipf's Law]]](../2022-04/48ba29.md) \n[[[Price's Law]]](../2022-03/03ffd9.md) \n[[[Kleiber's Law of Life Expectation]]](../2022-04/503f67.md) \n\n\n--\n\nMandelbrot \n[https://www.youtube.com/watch?v=fCn8zs912OE]\n\n",
    "metadata": ""
  },
  {
    "name": "2020-09/9b109e.md",
    "id": "2020-09/9b109e",
    "fileType": "markdown",
    "text": "# Awesome Podcasts\n#podcast\n\n## Stuff You Should Know\nhttps://www.iheart.com/podcast/105-stuff-you-should-know-26940277/\n\n- You Are Not So Smart [https://youarenotsosmart.com/podcast/]\n- [[[After On Podcast]]](../2020-09/31e5b4.md)\n- [[[Mindscape Podcast]]](../2022-04/92375a.md) \n\n- Universe Today\n[https://podcasts.google.com/feed/aHR0cHM6Ly93d3cudW5pdmVyc2V0b2RheS5jb20vYXVkaW8ueG1s]\n\n- Bookworm [https://bookworm.fm/]\n",
    "metadata": ""
  },
  {
    "name": "2020-09/b8ced8.md",
    "id": "2020-09/b8ced8",
    "fileType": "markdown",
    "text": "# angular cheatsheet\n\n[[[angular - NgForOf Local variables]]](../2020-10/a01074.md) \n\n\n## Dependency Injection\n- forward-ref  [[[angular - forwardRef]]](../2020-09/980f3c.md) \n- ref-vars [[[angular - ref-vars]]](../2020-09/f9a68c.md) \n\n## Templates\n- nested template driven forms [[[nested template driven forms]]](../2020-09/7b894e.md) \n- keyvalue pipe [[[angular - keyvalue pipe]]](../2020-09/922860.md) ",
    "metadata": ""
  },
  {
    "name": "2020-09/d78bba.md",
    "id": "2020-09/d78bba",
    "fileType": "markdown",
    "text": "# Getting Ideas\n\nDeveloping ideas origin in an individual, often by cross-linking. It is not optimal to discuss an early stage idea in a group, cause it limits the space of possible solutions. This is one reason why [[[Brainstorming]]](../2020-09/631201.md) does not work . As a consequence the internet somehow \n\nIsaac Asimov wrote a famous essay on that topic\n [[[Isaac Asimov Asks, “How Do People Get New Ideas?”]]](../2022-01/bc02a6.md) \n\n\u003e ...\n\n\n## Goals\nLarge [[[Solution]]](../2022-05/050ae1.md) space\n\n\n\n## Challenges gathering Ideas\n\nIdeas have to evolve and this takes time. Once it has matured enough in your mind, it's fair to share it.\n\n[[[Explaining Ideas]]](../2022-03/f271b0.md) \n",
    "metadata": ""
  },
  {
    "name": "2020-09/d8d04f.md",
    "id": "2020-09/d8d04f",
    "fileType": "markdown",
    "text": "# untold notes\n#project\n\nThis project is an implementation of the zettelkasten [[[Zettelkasten]]](../2020-10/e3f984.md) with [[[untold: Markdown]]](../2022-04/b9b189.md) support.\n\nA version of private information retrieval https://en.wikipedia.org/wiki/Private_information_retrieval\n\n[[[untold: Issues and Ideas]]](../2022-04/67ff41.md) \n\n## Features\n- ocr\n- [[[untold: Markdown]]](../2022-04/b9b189.md) \n- [[[untold: Snapshots and Transclusion]]](../2022-04/d49126.md) \n- [[[untold: Multimedia]]](../2022-04/4e10f9.md) \n- [[[untold: Subscriptions]]](../2022-04/da0580.md) \n- [[[untold: Scripts and Automation]]](../2020-11/b8f618.md) \n- [[[untold: Import and Export]]](../2022-04/8e5f02.md) \n\n## Experience\n- [[[untold: log]]](../2022-04/638b66.md) \n\n## Dump\n- dark mode\n- audio to text from videos/audios\n- Suggest similar articles [[[More Like This]]](../2020-10/c56160.md)\n- Measure note quality and display in editor\n\n- Get inspired by \n  - [https://www.notion.so/db13644f08144495ad9877f217a161a1?v=ff6777802811416ba08dc114e0b11837]\n- generate feed.json: share just the links you used, late the full note\n- support encryption https://stackoverflow.com/a/8939327/807017\n    - check if cryptojs DES and ssl DES work interchangeable\n      encrypt `openssl aes256 -in Burrito\\ Mafia.decr.md -out Burrito\\ Mafia.encr.md`\n      decrypt `openssl aes256 -d -out Burrito\\ Mafia.encr.md -out Burrito\\ Mafia.decr.md`\n- import a website harvest, to contextuarize e.g. a wiki page\n- implement folgezettel [https://zettelkasten.de/posts/luhmann-folgezettel-truth/]\n- sidebar that shows similar notes, incoming/outgoing links\n- improve reading experience [https://nandovieira.com/working-with-dates-on-ruby-on-rails]\n- tables using tableflip\n\n\n## References\n- [https://strengejacke.wordpress.com/tag/zettelkasten/]\n- [https://zettelkasten.de/posts/]\n- [http://www.acuriousmix.com/2014/09/03/designing-a-personal-knowledgebase/#comment-1575169922]\n",
    "metadata": ""
  },
  {
    "name": "2020-09/e30ebf.md",
    "id": "2020-09/e30ebf",
    "fileType": "markdown",
    "text": "# Perma Culture\n\nRichard Perkins is running a profitable permaculture farm in sweden. In the podcast [[[Awesome Podcasts]]](../2020-09/9b109e.md) on future thinkers he explains how they sell their products in a lean way, without a lot of overhead.\nhttps://futurethinkers.org/richard-perkins-permaculture/\n\nPC is one sustainable attempt for [[[Degrowth]]](../2022-03/e9628e.md)",
    "metadata": ""
  },
  {
    "name": "2020-09/f9a68c.md",
    "id": "2020-09/f9a68c",
    "fileType": "markdown",
    "text": "# angular - ref-vars\n\n```\n@ViewChild('varName') to access \u003ctag #varName\u003e\n\nAdd types #heroForm=\"ngForm\"\n\n```",
    "metadata": ""
  },
  {
    "name": "2020-09/gt14a9.md",
    "id": "2020-09/gt14a9",
    "fileType": "markdown",
    "text": "# Music\n\n[[[Artists and Bands I like]]](../2022-03/f4d82a.md) \n\n",
    "metadata": ""
  },
  {
    "name": "2020-10/00367d.md",
    "id": "2020-10/00367d",
    "fileType": "markdown",
    "text": "# Ability to Learn\nThe general ability to learn the result of your current physological and mental abilities and the motivation. The specific ability depends on state-of-mind [[[Mental States]]](../2020-10/ae8b0a.md). Drugs like coffein or nicotin may help to increase your ability temporarily [[[Drugs]]](../2020-10/92c619.md). \n\n\u003e School teaches you how to learn.\n\n## Techniques\n- [[[Spaced Repetition]]](../2020-10/a816e3.md)\n- [[[Fake It Till You Make It (Draft)]]](../2020-10/c1040c.md)\n- [[[Efficient Learning (Draft)]]](../2020-10/132c46.md)\n\n## Backlinks\n\n- [[[Motivation]]](../2020-10/3355db.md)\n",
    "metadata": ""
  },
  {
    "name": "2020-10/007dc0.md",
    "id": "2020-10/007dc0",
    "fileType": "markdown",
    "text": "# DNA\n\nDNA is not programming, cause it has no execution, it is just a cat model.",
    "metadata": ""
  },
  {
    "name": "2020-10/023280.md",
    "id": "2020-10/023280",
    "fileType": "markdown",
    "text": "# Vendor Lock-In\n\n[[[Marketing]]](../2022-03/9df388.md) trick to keep customers (retention).\n\nCustomers stick with a bad solution due to the Sunk-Cost Fallacy [[[Cognitive Biase]]](../2020-10/f0dc95.md) \n",
    "metadata": ""
  },
  {
    "name": "2020-10/0334a2.md",
    "id": "2020-10/0334a2",
    "fileType": "markdown",
    "text": "# Enterprise Scaling Knowledge \nat Airbnb\n\n[https://medium.com/airbnb-engineering/scaling-knowledge-at-airbnb-875d73eff091]\n\n**Reproducibility** — There should be no opportunity for code forks. The entire set of queries, transforms, visualizations, and write-up should be contained in each contribution and be up to date with the results.\n— The entirety of the work, from the query of our core ETL tables, to the transforms, visualizations, and write-up, is contained in one file, the Jupyter notebook, RMarkdown, or markdown file.\n\n**Quality** — No piece of research should be shared without being reviewed for correctness and precision.\n— By leaning on GitHub’s functionality of pull requests prior to publishing, peer review and version control is put directly into the flow of work.\n\n**Consumability** — The results should be understandable to readers besides the author. Aesthetics should be consistent and on brand across research.\n\n**Discoverability** — Anyone should be able to find, navigate, and stay up to date on the existing set of work on a topic.\n-\u003e folder organization, tags, sturctured note that contains authors, tags, tldr\n\n**Learning** — In line with reproducibility, other researchers should be able to expand their abilities with tools and techniques from others’ work.\n— By having previous work easily accessible, it becomes easier to learn from each other. For example, one can review each other’s queries, see the code used for a creative visualization, and discover new data sources. This exposes Data Scientists to new methodologies and coding techniques, speeds up on-boarding, and makes it possible for people outside our team to learn about our field.\n\n\nThey have a process in place that make sure that all research is high quality and consumable, \\\"our process combines the code review of engineering with the peer review of academia\\\"\n\nAs in code reviews, we check for code correctness and best practices and tools. As in peer reviews, we check for methodological improvements, connections with preexisting work, and precision in expository claims. We typically don’t aim for a research post to cover every corner of investigation, but instead prefer quick iterations that are correct and transparent about their limitations.\n",
    "metadata": ""
  },
  {
    "name": "2020-10/090bb5.md",
    "id": "2020-10/090bb5",
    "fileType": "markdown",
    "text": "# Read\n- [[[Scientific Realism (Read)]]](../2020-10/3b11ff.md)\n- [[[Epistemology (Read)]]](../2020-10/0aede0.md)\n- [[[Problem of Induction (Read)]]](../2020-10/132ea1.md)\n\n",
    "metadata": ""
  },
  {
    "name": "2020-10/0aede0.md",
    "id": "2020-10/0aede0",
    "fileType": "markdown",
    "text": "# Epistemology (Read)\n\nhttps://en.wikipedia.org/wiki/Epistemology\n",
    "metadata": ""
  },
  {
    "name": "2020-10/0d2e14.md",
    "id": "2020-10/0d2e14",
    "fileType": "markdown",
    "text": "# Simulation\n\n## Backlinks\n- [[[Model (Draft)]]](../2020-10/163bbd.md)\n",
    "metadata": ""
  },
  {
    "name": "2020-10/0dd73d.pdf",
    "id": "2020-10/0dd73d",
    "fileType": "pdf",
    "text": "",
    "metadata": "{\"nodeId\":\"323032302d31302f306464373364\",\"pdf\":{\"pages\":[{\"texts\":[\"Identifying the Influential Bloggers in a Community\",\"Nitin Agarwal, Huan Liu, Lei Tang\",\"Arizona State University\",\"Tempe, AZ 85287, USA\",\"{\",\"Nitin.Agarwal.2, Huan.Liu,\",\"Lei.Tang\",\"}\",\"@asu.edu\",\"Philip S. Yu\",\"University of Illinois at Chicago\",\"Chicago, IL 60607, USA\",\"psyu@cs.uic.edu\",\"ABSTRACT\",\"Blogging becomes a popular way for a Web user to publish\",\"information on the Web. Bloggers write blog posts, share\",\"their likes and dislikes, voice their opinions, provide sug\",\"ges-\",\"tions, report news, and form groups in Blogosphere. Blog-\",\"gers form their virtual communities of similar interests. A\",\"c-\",\"tivities happened in Blogosphere affect the external world.\",\"One way to understand the development on Blogosphere is\",\"to find influential blog sites. There are many non-influential\",\"blog sites which form the “the long tail”. Regardless of a\",\"blog site being influential or not, there are influential blog\",\"-\",\"gers.  Inspired by the high impact of the influentials in a\",\"physical community, we study a novel problem of identify-\",\"ing influential bloggers at a blog site. Active bloggers are\",\"not necessarily influential. Influential bloggers can impac\",\"t\",\"fellow bloggers in various ways. In this paper, we discuss\",\"the challenges of identifying influential bloggers, invest\",\"igate\",\"what constitutes influential bloggers, present a prelimina\",\"ry\",\"model attempting to quantify an influential blogger, and\",\"pave the way for building a robust model that allows for\",\"finding various types of the influentials. To illustrate thes\",\"e\",\"issues, we conduct experiments with data from a real-world\",\"blog site, evaluate multi-facets of the problem of identify\",\"-\",\"ing influential bloggers, and discuss unique challenges. We\",\"conclude with interesting findings and future work.\",\"Categories and Subject Descriptors:\",\"J.4 [Social and\",\"Behavioral Science]: Economics, Sociology\",\"General Terms:\",\"Algorithm, Design, Experimentation, Hu-\",\"man Factors, Measurement, Performance, Verification.\",\"Keywords:\",\"Social Networks, Blogosphere, Influential Blog-\",\"gers.\",\"1.  INTRODUCTION\",\"The advent of participatory Web applications (or Web\",\"2.0 [23]) has created online media that turn the former mass\",\"information consumers to the present information produc-\",\"ers [9]. Examples include blogs, wikis, social annotation a\",\"nd\",\"Permission to make digital or hard copies of all or part of thi\",\"s work for\",\"personal or classroom use is granted without fee provided th\",\"at copies are\",\"not made or distributed for profit or commercial advantage an\",\"d that copies\",\"bear this notice and the full citation on the first page. To cop\",\"y otherwise, to\",\"republish, to post on servers or to redistribute to lists, re\",\"quires prior specific\",\"permission and/or a fee.\",\"WSDM’08,\",\"Febr uar y 11–12, 2008, Pal o A l to, C al i f or\",\"ni a, U S A .\",\"Copyright 2008 ACM 978-1-59593-927-9/08/0002 ...\",\"$\",\"5.00.\",\"tagging, media sharing, and other such services. A “blog”\",\"is a weblog at a website where the entries by individuals\",\"are displayed in reverse chronological order. A typical blo\",\"g\",\"can combine text, images, and links to other blogs and to\",\"Web pages. These entries can be blog posts or comments -\",\"the follow-up posts linked to some specific posts. Blogging\",\"is becoming a popular means for mass Web users to ex-\",\"press, communicate, share, collaborate, debate, and reflec\",\"t.\",\"Blogosphere is the virtual universe that contains all blogs\",\".\",\"Bloggers, the blog writers, loosely form their special inte\",\"r-\",\"est communities where they share thoughts, express opin-\",\"ions, debate ideas, and offer suggestions interactively. Bl\",\"o-\",\"gosphere provides a conducive platform to build the\",\"virtual\",\"communities\",\"of special interests. It reshapes business mod-\",\"els [26], inspires viral marketing [25], provides trend ana\",\"ly-\",\"sis and sales prediction [11, 22], aids counter-terrorism e\",\"f-\",\"forts [3] and acts as grassroot information sources [27].\",\"In a physical world, according to [14], 83% of people pre-\",\"fer consulting family, friends or an expert over traditiona\",\"l\",\"advertising before trying a new restaurant, 71% of people\",\"do the same before buying a prescription drug or visiting a\",\"place, and 61% of people talk to family, friends or an ex-\",\"pert before watching a movie. In short, before people buy\",\"or make decisions, they talk, and they listen to other’s ex-\",\"perience, opinions, and suggestions.  The latter affect the\",\"former in their decision making, and are aptly termed as\",\"the\",\"influentials\",\"[14].  Influence has always been unabated\",\"interest in business and society. As the pervasive presence\",\"and ease of use of the Web, an increasing number of peo-\",\"ple with different backgrounds flock to the Web - a virtual\",\"world to conduct many previously inconceivable activities\",\"from shopping, to making friends, and to publishing.  As\",\"we draw parallels between physical and virtual communi-\",\"ties, among citizens of the blogosphere, we are intrigued by\",\"the questions like whether there exist the influentials in a\",\"virtual community (a blog), who they are, and how to find\",\"them.\",\"Since the bloggers can be connected in a virtual commu-\",\"nity anywhere anytime, the identification of the influential\",\"bloggers can benefit all in developing innovative business o\",\"p-\",\"portunities, forging political agendas, discussing socia\",\"l and\",\"societal issues, and lead to many interesting applications\",\".\",\"For example, the influentials are often\",\"market-movers\",\". Since\",\"they can influence buying decisions of the fellow bloggers,\",\"identifying them can help companies better understand the\",\"key concerns and new trends about products interesting to\",\"them, and smartly affect them with additional information\",\"and consultation to turn them into unofficial spokesmen.\",\"207\"],\"page\":1},{\"texts\":[\"As reported in [5], approximately 64% advertising compa-\",\"nies have acknowledged this phenomenon and are shifting\",\"their focus toward blog advertising.\",\"The influentials could also\",\"sway\",\"opinions in political cam-\",\"paigns, elections, and affect reactions to government poli-\",\"cies [4].  Tapping on the influentials can help understand\",\"the changing interests, foresee potential pitfalls and lik\",\"ely\",\"gains, and adapt plans timely and pro-actively (not just re-\",\"actively). The influentials can also help in customer suppor\",\"t\",\"and troubleshooting since their solutions are trustworthy\",\"be-\",\"cause of the sense of authority these influentials possess. F\",\"or\",\"example, Macromedia\",\"1\",\"aggregates, categorizes and searches\",\"the blog posts of 500 people who write about Macromedia’s\",\"technology.  Instead of going through every blog post, an\",\"excellent entry point is to start with the influentials’ post\",\"s.\",\"Some recent numbers from Technorati\",\"2\",\"show a 100% in-\",\"crease in the size of Blogosphere every six months, “..., abo\",\"ut\",\"1.6 Million postings per day, or about 18.6 posts per sec-\",\"ond”\",\"3\",\". Blogosphere has grown over 60 times during the past\",\"three years. Since new blog posts are being generated with\",\"such a blazing fast rate, novel ways have to be developed in\",\"order to keep track of everything happening in Blogosphere.\",\"Researchers have studied the influence in Blogosphere about\",\"influential\",\"blog sites\",\"[8, 18] (more in Section 2). Regardless\",\"of a blog site being influential or not, a multi-authored blog\",\"site can have its influential bloggers. Influential bloggers\",\"of\",\"a site have impact on the fellow bloggers as in a physical\",\"community. In this paper, we address a novel problem of\",\"identifying\",\"influential bloggers\",\"on a blog site and investigate\",\"its issues and challenges.\",\"•\",\"Are there influential bloggers as in a physical commu-\",\"nity? Are they simply active bloggers?\",\"•\",\"What measures should be used to define influential\",\"bloggers? A solution can be subjective, depending on\",\"the need for identifying influential bloggers.\",\"•\",\"How to find influential bloggers? As there is no train-\",\"ing data to tell us who are influential bloggers or not,\",\"it is infeasible to apply classification. Combining the\",\"statistics collected for each blogger, can we create a\",\"robust model that quantitatively tells how influential\",\"a blogger is?\",\"•\",\"Can we tune/adjust the model to identify different\",\"classes of influential bloggers to satisfy various needs?\",\"In the following, we first review the literature and differ-\",\"entiate this work from the existing ones. In Section 3, we\",\"study the statistics collectable from a blog site, and define\",\"the problem of identifying influential bloggers. In Section 4,\",\"we propose a preliminary model that allows for evaluating\",\"different key measures for identifying the influentials and\",\"can be adapted to look for different types of influential blog-\",\"gers. In Section 5, we conduct an empirical study to evaluate\",\"many aspects of the proposed approach and its effectiveness,\",\"and observe how the key measures work with a correlation\",\"study.  Finally we conclude our work with some possible\",\"future directions in Section 6.\",\"1\",\"http://weblogs.macromedia.com/\",\"2\",\"http://technorati.com/\",\"3\",\"http://www.sifry.com/alerts/archives/000436.html\",\"2.  RELATED WORK\",\"Blogosphere expands speedily since its inception. This has\",\"attracted a surge of research on Blogosphere. Most studies\",\"are conducted in terms of social networks.  Emergence of\",\"communities in the network can be found frequently at a\",\"microscopic level [19]. Researchers work on community de-\",\"tection to explore various communities in Blogosphere [7,\",\"28, 20, 3]. Authors in [2] consider influence a characteris-\",\"tic of virtual communities, among others like membership,\",\"reinforcement of needs, shared emotional connection, whos\",\"e\",\"presence governs the establishment of a community. Link\",\"structures and overlapping between different sub-communit\",\"ies\",\"are used to help identify influence between them.\",\"2.1  Ranking Blogs vs. Webpage Ranking\",\"The problem of ranking blog sites or bloggers differs from\",\"that of finding authoritative webpages. As pointed out in [18\",\"],\",\"blog sites in the blogosphere are very sparsely linked and it\",\"is not suitable to rank blog sites using Web ranking algo-\",\"rithms\",\"like PageRank [24] and HITS [16].  The Random\",\"Surfer model of webpage ranking algorithms [24] does not\",\"work well for sparsely linked structures. The temporal as-\",\"pect is most significant in blog domain. While a webpage\",\"may acquire authority over time (its adjacency matrix gets\",\"denser), a blog post or a blogger’s influence diminishes over\",\"time. This is due to the fact that the adjacency matrix of\",\"blogs (considered as a graph) will get sparser as thousands\",\"of new sparsely-linked blog posts appear every day.\",\"Some recent work [18] suggests to add implicit links to\",\"increase the density of link information based on topics. If\",\"two blogs are talking about the same topic, an edge can be\",\"added between these two blogs based on the topic similar-\",\"ity or\",\"information epidemics\",\". However, constructing links\",\"based on the topics still remains an area of research.\",\"2.2  Influential Blog Sites\",\"Finding\",\"influential blog sites\",\"in the blogosphere is an im-\",\"portant research problem, which studies how some blog sites\",\"influence the external world and within the blogosphere [8].\",\"It is perpendicular to the problem of identifying influentia\",\"l\",\"bloggers.  Given the nature of the blogosphere, influential\",\"blog sites are few. A large number of non-influential sites\",\"belong to the long tail [1] where abundant new business,\",\"marketing, and development opportunities can be explored.\",\"Our work is about identifying influential bloggers at a blog\",\"site regardless of the site being influential or not. We briefl\",\"y\",\"review some work on influential blog sites.\",\"Gruhl et al [10] study information diffusion of various top-\",\"ics in the blogosphere from individual to individual, draw-\",\"ing on the theory of infectious diseases. A general cascade\",\"model is adopted.  They associate ‘read’ probability and\",\"‘copy’ probability with each edge of the blogger graph in-\",\"dicating the tendency to read one’s blog post and copy it,\",\"respectively. They also parameterize the stickiness of a to\",\"pic\",\"which is analogous to the\",\"virulence\",\"of a disease.\",\"An interesting problem related to viral marketing [25, 15]\",\"is how to maximize the total influence in the network (of\",\"blog sites) by selecting a fixed number of nodes in the net-\",\"work. A greedy approach can be adopted to select the most\",\"influential node in each iteration after removing the select\",\"ed\",\"nodes. This greedy approach outperforms PageRank, HITS\",\"and ranking by number of citations, and is robust in filtering\",\"splogs (spam blogs) [12].\",\"208\"],\"page\":2},{\"texts\":[\"The work discussed in this paper is about\",\"identifying influ-\",\"ential bloggers at one blog site\",\"and differs from those briefly\",\"reviewed above. A blog site is a special type of social net-\",\"work that contains information such as outlinks (other blog\",\"posts it is referring to), inlinks (other blog posts that are\",\"citing this blog post), comments which is not present in a\",\"general social network. Identifying the influential blogge\",\"rs\",\"at a blog site requires the integrated use of the information\",\"specific to a blog site.\",\"Influential bloggers are not necessarily active bloggers at\",\"a blog site. Many blog websites list top bloggers or top blog\",\"posts in some time frame (e.g., monthly). Those top lists are\",\"usually based on some traffic information (e.g., how many\",\"posts a blogger posted, or how many comments a blog post\",\"received) [8]. Certainly these statistics would leave out t\",\"hose\",\"blog sites or bloggers who were not active.\",\"With the speedy growth of the blogosphere, it is increas-\",\"ingly difficult, if at all possible, to manually track the deve\",\"l-\",\"opment and happenings in the blogosphere. This work is an\",\"effort to help understand the blogosphere. In the following,\",\"we first study the concept of influential bloggers at a blog\",\"site and then propose a preliminary model of computation-\",\"ally identifying influential bloggers.\",\"3.  INFLUENTIAL BLOGGERS\",\"Blogs can be categorized into two major types:\",\"individual\",\"and\",\"community blogs\",\". Individual blogs are single-authored\",\"who record their thoughts, express their opinions, and of-\",\"fer suggestions or ideas.  Others can comment on a blog\",\"post, but cannot start a new line of blog posts. These are\",\"more like diary entries or personal experiences. Examples\",\"of individual blogs are Sifry’s Alerts:  David Sifry’s mus-\",\"ings\",\"4\",\"(Founder \u0026 CEO, Technorati), Ratcliffe Blog–Mitch’s\",\"Open Notebook\",\"5\",\", The Webquarters\",\"6\",\", etc.  A community\",\"blog is where each blogger can not only comment on some\",\"blog posts, but also start some topic lines.  Examples of\",\"community blog sites are Google’s Official Blog\",\"7\",\", The Un-\",\"official Apple Weblog\",\"8\",\", Boing Boing: A Directory of Won-\",\"derful Things\",\"9\",\"etc. For an individual blog, the host is the\",\"only one who initiates and leads the discussions and thus\",\"is naturally the influential blogger of his/her site.  For a\",\"community blog where many have equal opportunities to\",\"participate, we study who are the influentials in a virtual\",\"community. Henceforth, blogs refer to community blogs.\",\"Each blog post is often associated with some metadata\",\"like post’s author, post annotations, post’s date and time,\",\"number of comments. In addition, one can also collect cer-\",\"tain statistics from the blog website for example,\",\"outlinks\",\"- posts or articles to which the author has referred;\",\"inlinks\",\"- other posts that refer to this post, post length;\",\"average\",\"length of comments\",\"per post; and the rate at which com-\",\"ments are posted on a blog post.  Since a long blog post\",\"can simply contain many outlinks, outlinks are normalized\",\"by the length of the blog post. Inlinks are collected using\",\"Technorati API\",\"10\",\".\",\"4\",\"http://www.sifry.com/alerts/\",\"5\",\"http://www.ratcliffeblog.com/\",\"6\",\"http://webquarters.blogspot.com/\",\"7\",\"http://googleblog.blogspot.com/\",\"8\",\"http://www.tuaw.com/\",\"9\",\"http://boingboing.net/\",\"10\",\"http://technorati.com/developers/api/cosmos.html\",\"In the simplest case, one can approximate an influential\",\"blogger with an active blogger who posts frequently. Since\",\"this is not the case in a physical world where a voluble per-\",\"son is not necessarily or seldom influential, we are inquisit\",\"ive\",\"whether we can employ the above metadata and statistics\",\"to identify influential bloggers.  The search for influential\",\"bloggers boils down to the question on how to define an in-\",\"fluential blogger. First, active bloggers are not necessari\",\"ly\",\"influential and influential bloggers can be inactive. Hence,\",\"we categorically divide bloggers into four types: active an\",\"d\",\"influential, active and non-influential, inactive and influe\",\"n-\",\"tial, and inactive and non-influential. Second, while activ\",\"e\",\"bloggers can be simply defined by how frequently a blogger\",\"posts, it is a more complex matter how to define an influen-\",\"tial blogger with the aid of the above mentioned statistics.\",\"Recognizing the subjective nature of defining an influence\",\"blogger, we propose a preliminary model to quantify the\",\"properties of the influential bloggers by combining various\",\"statistics collectable from a blog site and assigning influe\",\"nce\",\"scores to each blogger and their blog posts.  Next, we in-\",\"vestigate how these statistics can be used in various ways\",\"to adjust the model for different purposes. In this work, we\",\"first develop an intuitive model that goes beyond the post\",\"frequency and allows the use of the combination of statistic\",\"s.\",\"Then we demonstrate how to use this model to identify in-\",\"fluential bloggers who may or may not be active, and further\",\"investigate how to further refine and evolve the preliminary\",\"model in finding various types of influential bloggers.\",\"An intuitive way of defining an influential blogger is to\",\"check if the blogger has any influential blog post, i.e.,\",\"A\",\"blogger can be influential if s/he has more than one influ-\",\"ential blog post.\",\"Assume we have an influence score\",\"11\",\"for\",\"a post\",\"p\",\"i\",\",\",\"I\",\"(\",\"p\",\"i\",\"). For a blogger\",\"b\",\"k\",\"who has\",\"N\",\"blog posts,\",\"{\",\"p\",\"1\",\", p\",\"2\",\", ..., p\",\"N\",\"}\",\", their influence scores can be ranked in de-\",\"scending order, and her influence index,\",\"iIndex(\",\"b\",\"k\",\")\",\"can be\",\"defined as max(\",\"I\",\"(\",\"p\",\"i\",\")), where 1\",\"≤\",\"i\",\"≤\",\"N\",\". Given a set\",\"U\",\"of\",\"M\",\"bloggers,\",\"{\",\"b\",\"1\",\", b\",\"2\",\", ..., b\",\"M\",\"}\",\", the problem of identifying influ-\",\"ential bloggers is defined as determining an ordered subset\",\"V\",\"of\",\"K\",\"12\",\"bloggers,\",\"{\",\"b\",\"j\",\"1\",\", b\",\"j\",\"2\",\", ..., b\",\"j\",\"K\",\"}\",\"that are ordered ac-\",\"cording to their\",\"iIndex\",\"such that\",\"V\",\"⊆\",\"U\",\"and\",\"K\",\"≤\",\"M\",\",\",\"i.e.\",\"iIndex\",\"(\",\"b\",\"j\",\"1\",\")\",\"≥\",\"iIndex\",\"(\",\"b\",\"j\",\"2\",\")\",\"≥\",\"...\",\"≥\",\"iIndex\",\"(\",\"b\",\"j\",\"K\",\").\",\"V\",\"containss\",\"K\",\"most\",\"influential bloggers\",\". For all the blog\",\"posts\",\"{\",\"p\",\"1\",\", p\",\"2\",\", ..., p\",\"L\",\"}\",\"by all\",\"M\",\"bloggers,\",\"influential blog\",\"posts\",\"are those whose influence scores are greater than\",\"iIndex\",\"(\",\"b\",\"j\",\"K\",\") or,\",\"I\",\"(\",\"p\",\"l\",\")\",\"≥\",\"iIndex\",\"(\",\"b\",\"j\",\"K\",\") for 1\",\"≤\",\"l\",\"≤\",\"L\",\". Hence,\",\"we have the following corollary: those bloggers who pub-\",\"lished blog posts that satisfy\",\"I\",\"(\",\"p\",\"l\",\")\",\"≥\",\"iIndex\",\"(\",\"b\",\"j\",\"K\",\"), for 1\",\"≤\",\"l\",\"≤\",\"L\",\"will be called influential bloggers because their\",\"iIndex\",\"will be greater than or equal to\",\"iIndex\",\"(\",\"b\",\"j\",\"K\",\").\",\"Having formulated the problem of identifying influential\",\"bloggers, we now study the intuitive characteristics that h\",\"elp\",\"define\",\"iIndex\",\"and\",\"I\",\"so as to build an experimental model\",\"that can gauge the influence to distinguish between “influ-\",\"ential” and “activeness” properties of bloggers.\",\"4.  IDENTIFYING THE INFLUENTIALS\",\"We first present some desirable properties related to blog-\",\"post influence which can be approximately defined by col-\",\"lectable statistics, next propose a preliminary model of id\",\"en-\",\"tifying the influentials using these statistics, then discu\",\"ss\",\"11\",\"These concepts are defined mathematically in Section 4.2.\",\"12\",\"Note that\",\"K\",\"is a user specified parameter.\",\"209\"],\"page\":3},{\"texts\":[\"some interesting issues that can be evaluated by experiment\",\"-\",\"ing the preliminary model.\",\"4.1  An initial set of intuitive properties\",\"Following [14], one is influential if s/he is recognized by\",\"fellow citizens, can generate follow-up activities, has no\",\"vel\",\"perspectives or ideas, and is often eloquent. Below we ex-\",\"amine how this initial set of intuitive properties can be ap-\",\"proximated by some collectable statistics.\",\"•\",\"Recognition - An influential blog post is recognized\",\"by many.  This can be equated to the case that an\",\"influential post\",\"p\",\"is referenced in many other posts,\",\"or its number of inlinks (\",\"ι\",\") is large. The influence of\",\"those posts that refer to\",\"p\",\"can have different impact:\",\"the more influential the referring posts are, the more\",\"influential the referred post becomes.\",\"•\",\"Activity Generation - A blog post’s capability of gener-\",\"ating activity can be indirectly measured by how many\",\"comments it receives, the amount of discussion it initi-\",\"ates. In other words, few or no comment suggests little\",\"interest of fellow bloggers, thus non-influential. Hence,\",\"a large number of comments (\",\"γ\",\") indicates that the post\",\"affects\",\"many such that they care to write comments,\",\"and therefore, the post can be influential. There are\",\"increasing concerns over spam comments that do not\",\"add any value to the blog posts or blogger’s influence.\",\"Fighting spam is outside the scope of this work and\",\"recent research can be found in [17, 21].\",\"•\",\"Novelty - Novel ideas exert more influence as suggested\",\"in [14]. Hence, the number of outlinks is an indicator\",\"of a post’s novelty. A large number of outlinks (\",\"θ\",\") may\",\"suggest that a post refers to many other blog posts or\",\"articles, indicating that it is less likely to be novel.\",\"The number of outlinks is negatively correlated with\",\"the number of comments which means more outlinks\",\"reduces people’s attention. This is confirmed later in\",\"Section 5.2.5.\",\"•\",\"Eloquence - An influential is often eloquent [14]. This\",\"property is most difficult to approximate using some\",\"statistics.  Given the informal nature of the blogo-\",\"sphere, there is no incentive for a blogger to write a\",\"lengthy piece that bores the readers.  Hence, a long\",\"post often suggests some necessity of doing so. There-\",\"fore, we use the length of a post (\",\"λ\",\") as a heuristic\",\"measure for checking if a post is influential or not. The\",\"blog post length is positively correlated with number of\",\"comments which means longer blog posts attract peo-\",\"ple’s attention. This is confirmed later in Section 5.2.5.\",\"The above four form an initial set of properties possessed\",\"by an influential post. There are certainly some other po-\",\"tential properties. It is also evident that each of the above\",\"four may not be sufficient on its own, and they should be\",\"used jointly in identifying influential bloggers.  For exam-\",\"ple, a high\",\"θ\",\"and a poor\",\"λ\",\"could identify a messenger blog\",\"post. Starting with this initial set, we next build a prelimi\",\"-\",\"nary model that allows us to examine, analyze, modify, and\",\"extend the model.\",\"4.2  Influence graph - a preliminary model\",\"Blog-post influence can be visualized in terms of an influ-\",\"ence graph or\",\"i-graph\",\"in which the influence of a blog post\",\"flows among the nodes. Each node of an i-graph represents\",\"a single blog post characterized by the four properties (or\",\"parameters):\",\"ι, θ, γ\",\"and\",\"λ\",\". i-graph is a directed graph with\",\"ι\",\"and\",\"θ\",\"representing the incoming and outgoing influence\",\"flows of a node, respectively. Hence, if\",\"I\",\"denotes the influ-\",\"ence of a node (or blog post\",\"p\",\"), then\",\"InfluenceF low\",\"across\",\"that node is given by,\",\"InfluenceF low\",\"(\",\"p\",\") =\",\"w\",\"in\",\"|\",\"ι\",\"|\",\"∑\",\"m\",\"=1\",\"I\",\"(\",\"p\",\"m\",\")\",\"−\",\"w\",\"out\",\"|\",\"θ\",\"|\",\"∑\",\"n\",\"=1\",\"I\",\"(\",\"p\",\"n\",\")  (1)\",\"where\",\"w\",\"in\",\"and\",\"w\",\"out\",\"are the weights that can be used to\",\"adjust the contribution of incoming and outgoing influence,\",\"respectively.\",\"p\",\"m\",\"denotes all the blog posts that link to the\",\"blog post\",\"p\",\", where 1\",\"≤\",\"m\",\"≤|\",\"ι\",\"|\",\"; and\",\"p\",\"n\",\"denotes all the\",\"blog posts that are referred by the blog post\",\"p\",\", where 1\",\"≤\",\"n\",\"≤|\",\"θ\",\"|\",\".\",\"|\",\"ι\",\"|\",\"and\",\"|\",\"θ\",\"|\",\"are the total numbers of inlinks and\",\"outlinks of post\",\"p\",\".\",\"InfluenceF low\",\"measures the difference\",\"between the total incoming influence of all inlinks and the\",\"total outgoing influence by all outlinks of the blog post\",\"p\",\".\",\"InfluenceF low\",\"accounts for the part of influence of a blog\",\"post that depends upon inlinks and outlinks. From Eq. 1,\",\"it is clear that the more inlinks a blog post acquires the\",\"more recognized it is, hence the more influential it gets; and\",\"an excessive number of outlinks jeopardizes the novelty of a\",\"blog post which affects its influence.\",\"As discussed earlier, the influence (\",\"I\",\") of a blog post is also\",\"proportional to the number of comments (\",\"γ\",\"p\",\") posted on that\",\"blog post. We can define the influence of a blog post,\",\"p\",\"as,\",\"I\",\"(\",\"p\",\")\",\"∝\",\"w\",\"com\",\"γ\",\"p\",\"+\",\"InfluenceF low\",\"(\",\"p\",\")\",\"(\",\"2\",\")\",\"where\",\"w\",\"com\",\"denotes the weight that can be used to regulate\",\"the contribution of the number of comments (\",\"γ\",\"p\",\") towards\",\"the influence of the blog post\",\"p\",\". We consider an additive\",\"model because additive function is good to determine the\",\"combined value of each alternative [6]. It also supports pre\",\"f-\",\"erential independence of all the parameters involved in the\",\"final decision.  Since most decision problems like the one\",\"at hand are multi-objective, a way to evaluate trade-offs be-\",\"tween the objectives is needed. A weighted additive functio\",\"n\",\"can be used for this purpose [13].\",\"From the discussion in Section 4.1, we consider blog post\",\"quality as one of the parameters that may affect influence\",\"of the blog post. Although there are many measures that\",\"quantify the goodness of a blog post such as fluency, rhetoric\",\"skills, vocabulary usage, and blog content analysis\",\"13\",\", for the\",\"sake of simplicity, we here use the length of the blog post\",\"as a heuristic measure of the goodness of a blog post in\",\"the context of blogging.  We define a weight function,\",\"w\",\",\",\"which rewards or penalizes the influence score of a blog post\",\"depending on the length (\",\"λ\",\") of the post. The weight func-\",\"tion could be replaced with appropriate content and literar\",\"y\",\"analysis tools. Combining Eq. 1 and Eq. 2, the influence of\",\"a blog post,\",\"p\",\", can thus be defined as,\",\"I\",\"(\",\"p\",\") =\",\"w\",\"(\",\"λ\",\")\",\"×\",\"(\",\"w\",\"com\",\"γ\",\"p\",\"+\",\"InfluenceF low\",\"(\",\"p\",\"))    (3)\",\"13\",\"A reason we did not adopt any of these is their computation\",\"is beyond the scope of this work.  We use some simpler\",\"measure to examine its effect in determining influence.\",\"210\"],\"page\":4},{\"texts\":[\"The above equation gives an influence score to each blog\",\"post.  Note that the four weights can take more complex\",\"forms and can be tuned. We will evaluate and discuss their\",\"effects further in the empirical study.\",\"Now we consider how to use\",\"I\",\"to determine whether a\",\"blogger is influential or not. According to the definition of\",\"influential blogger in Section 3, a blogger can be considered\",\"influential if s/he has at least one influential blog post. We\",\"use the blog post with maximum influence score as the rep-\",\"resentative\",\"14\",\"and assign its influence score as the\",\"blogger\",\"influence index\",\"or\",\"iIndex\",\". For a blogger\",\"B\",\", we can calcu-\",\"late the influence score for each of\",\"B\",\"’s\",\"N\",\"posts and use the\",\"maximum influence score as the blogger’s\",\"iIndex\",\", or\",\"iIndex\",\"(\",\"B\",\") = max(\",\"I\",\"(\",\"p\",\"i\",\"))\",\"(4)\",\"where 1\",\"≤\",\"i\",\"≤\",\"N\",\". With\",\"iIndex\",\", we can rank bloggers on a\",\"blog site. The top\",\"k\",\"among the total bloggers are the most\",\"influential ones. Thresholding is another way to find influ-\",\"ential bloggers whose\",\"iIndices\",\"are greater than a threshold.\",\"However, determining a proper threshold is crucial to the\",\"success of such a strategy and requires more research.\",\"4.3  Issues of identifying the influentials\",\"The preliminary model presents a palpable way of iden-\",\"tifying influential bloggers and allows us to address many\",\"relevant issues such as evaluation, feasibility, efficacy, s\",\"ub-\",\"jectivity, and extension.\",\"•\",\"Can we use this model to differentiate influential blog-\",\"gers from active bloggers? We study the existence of\",\"influential bloggers at a blog site by applying the pre-\",\"liminary model.\",\"•\",\"How can we evaluate the model’s performance in iden-\",\"tifying the influential bloggers?  Are influential blog\",\"posts indeed different from non-influential blog posts?\",\"•\",\"How can we properly determine the weights when com-\",\"bining the four parameters in\",\"iIndex\",\"? If one changes\",\"the value of a weight, will the change significantly af-\",\"fect the ranking of influential bloggers?  How these\",\"weights can help find special influential bloggers?\",\"•\",\"How do we handle the subjectivity aspect of the prob-\",\"lem of identifying influential bloggers as different peo-\",\"ple may have disparate preferences?  Since we have\",\"access to the whole history of the blog site, we look\",\"into these questions by consecutively studying the in-\",\"fluentials in multiple 30-day windows.  Can we also\",\"employ the model to find any temporal patterns of the\",\"influential bloggers?\",\"•\",\"Are all the four parameters necessary? We design and\",\"perform a correlation study. Some of the parameters\",\"may be correlated with each other, so one of them may\",\"be redundant.  Pairwise correlation analysis is thus\",\"conducted.\",\"•\",\"How can we extend the preliminary model? Are there\",\"any other parameters that can be incorporated in a\",\"refined model?\",\"14\",\"There could be other ways.  For example, if one wants\",\"to differentiate a productive influential blogger from non-\",\"prolific one, one might use another measure.\",\"In the next, we set out to use the proposed model in an\",\"empirical study, attempt to experimentally address these\",\"issues, report preliminary results, and suggest new lines o\",\"f\",\"research in finding influential bloggers.\",\"5.  FURTHER STUDY \u0026 EXPERIMENTS\",\"We first discuss the need for experimental data, and select\",\"a real-world blog site for experiments; and second, we de-\",\"sign various experiments with the preliminary model using\",\"iIndex\",\", and answer the questions raised in Section 4.3 based\",\"on the experimental results. In the process, we develop and\",\"elaborate an evaluation procedure for effective comparison\",\".\",\"5.1  Data collection\",\"Data collection is one of the critical tasks in this work.\",\"To our best knowledge, our effort is the first attempt to find\",\"influential bloggers. Hence, there are no available blog dat\",\"a\",\"sets for the purposes of our experiments. We need to collect\",\"real-world data.\",\"There exist many blog sites. Some like Google’s Official\",\"Blog site act as a notice board for important announce-\",\"ments rather than for discussions, sharing opinions, ideas\",\"and thoughts; some do not provide most of the statistics\",\"needed in our work, although they can be obtained via some\",\"additional work (more explanation later).  A few publicly\",\"available blog datasets like the BuzzMetric dataset\",\"15\",\"were\",\"designed for different research experiments so there is no\",\"way to obtain some key statistics required in this work.\",\"Therefore, we crawled a real-world blog site that provides\",\"the most statistics required in our experiments.  The ad-\",\"vantages of of doing so include (1) minimizing our effort on\",\"figuring out ways to obtain the needed statistics, and (2)\",\"maximizing the reproducibility of our experiments indepen\",\"-\",\"dently. The Unofficial Apple Weblog (TUAW) site is such\",\"a site that satisfies these requirements. This blog site pro-\",\"vides most needed information like blogger identification,\",\"date and time of posting, number of comments, and out-\",\"links. The only missing piece of information at TUAW is\",\"the\",\"inlinks\",\"information, which we can obtain using Techno-\",\"rati API\",\"16\",\". We crawled the TUAW blog site and retrieved\",\"all the blog posts published since it was set up. We have\",\"collected over 10\",\",\",\"000 posts till January 31, 2007. We keep\",\"the complete history of the TUAW blog site and update it\",\"incrementally. All the statistics obtained after crawling\",\"is\",\"stored in a relational database for fast retrieval later\",\"17\",\".\",\"5.2  Results and discussions\",\"The following subsections introduce the experiments, re-\",\"sults, and discussions corresponding to the questions rais\",\"ed\",\"in the Section 4.3.\",\"5.2.1  Influential Bloggers and Active Bloggers\",\"Many blog sites publish a list of top bloggers based on\",\"their activities on the blog site. The ranking is often made\",\"according to the number of blog posts each blogger sub-\",\"mitted over a period of time. In this paper, we call these\",\"people\",\"active\",\"bloggers. Since the top bloggers on the blog\",\"site TUAW are those from the last 30 days, we define our\",\"15\",\"http://www.nielsenbuzzmetrics.com/cgm.asp\",\"16\",\"http://technorati.com/developers/api/cosmos.html\",\"17\",\"This dataset will be made available upon request for re-\",\"search purposes.\",\"211\"],\"page\":5},{\"texts\":[\"Top\",\"5\",\"TUAW Bloggers\",\"Top\",\"5\",\"Influential Bloggers\",\"Erica Sadun\",\"Erica Sadun\",\"Scott McNulty\",\"Dan Lurie\",\"Mat Lu\",\"David Chartier\",\"David Chartier\",\"Scott McNulty\",\"Michael Rose\",\"Laurie A. Duncan\",\"Table 1: Two lists of the top\",\"5\",\"bloggers according to\",\"TUAW and our model, respectively.\",\"study window of 30 days as well. Using the number of posts\",\"of a blogger posted is obviously an oversimplified indicator\",\",\",\"which basically says the most frequent blogger is an influen-\",\"tial one. Such a status can be achieved by simply submitting\",\"many posts, as even junk posts are counted. Hence, an ac-\",\"tive blogger may not be an influential one; and in the same\",\"spirit, an influential blogger need not be an active one. In\",\"other words, the most active\",\"k\",\"bloggers are not necessarily\",\"the top influential one, and an inactive blogger can still be\",\"an influential one.\",\"In our first experiment, we generate a list of top-\",\"k\",\"bloggers\",\"using the preliminary model proposed in Section 3. We set\",\"the default values of all the weights as 1 assuming they are\",\"equally important.  An in-depth study of these weights is\",\"in Section 5.2.2.  By setting\",\"k\",\"= 5, we compare the top\",\"5 influential bloggers with the top 5 bloggers published at\",\"TUAW. Table 1 presents two lists of top 5 bloggers according\",\"to TUAW and based on the proposed model using\",\"iIndex\",\":\",\"the first column contains the top 5 bloggers published by\",\"TUAW and the second column lists the top 5 influential\",\"bloggers. Names in\",\"italics\",\"are the bloggers present in both\",\"lists.  Three out of 5 TUAW top bloggers are also among\",\"the top 5 influential bloggers identified by our model. This\",\"set of bloggers suggests that some of the bloggers can be\",\"both active and influential.  Some active bloggers are not\",\"influential and some influential bloggers are not active. For\",\"instance, ‘Mat Lu’ and ‘Michael Rose’ in the TUAW list, so\",\"they are active; and ‘Dan Lurie’ and ‘Laurie A. Duncan’ in\",\"the list of the influentials, but they are not active.\",\"In total, there could be four types of bloggers: both active\",\"and influential, active but non-influential, influential but\",\"in-\",\"active, inactive and non-influential. Since we have all the\",\"needed statistics, we can delve into the numbers and scru-\",\"tinize their differences of the first three groups of bloggers\",\".\",\"Their detailed statistics are presented in Table 2.\",\"Inactive\",\"and non-influential bloggers\",\"seldom submit blog posts and\",\"submitted posts do not influence others, so this group does\",\"not show up in Table 2.\",\"•\",\"Active and influential bloggers\",\"who actively post and\",\"some of them are influential posts.  ‘Erica Sadun’,\",\"‘David Chartier’ and ’Scott McNulty’ are of this cat-\",\"egory.  This can be verified by the large number of\",\"posts and the large number of comments and citations\",\"by other bloggers.  For instance, ‘Erica Sadun’ sub-\",\"mitted 152 posts in the last 30 days, among which 9\",\"of them are influential, attracting a large number of\",\"readers evidenced by 75 comments and 80 citations.\",\"•\",\"Inactive but influential bloggers\",\". These bloggers sub-\",\"mit a few but influential posts. ‘Dan Lurie’ published\",\"only 16 posts (much fewer than 152 posts comparing\",\"with ‘Erica Sadun’, an active influential blogger) in\",\"the last 30 days. Dan was not selected by TUAW as\",\"a top blogger. A closer look at his blog posts reveals\",\"that 4 of his blog posts are influential, i.e., 25% of the\",\"blog posts by ‘Dan Lurie’ are influential. One of his\",\"influential posts is about iPhone\",\"18\",\", which attracted a\",\"large number of bloggers to comment and intrigued a\",\"heated discussion of the new product (77 comments\",\"and 33 inlinks). Its length is 1417 bytes, and there are\",\"no outlinks. All these numbers suggest that the post\",\"is detailed, innovative, and interesting to other blog-\",\"gers. By reading the content, we notice that the post\",\"is a detailed account of his personal experience rather\",\"than extracts from external news sources. This kind\",\"of posts allows a reader to experience something new,\",\"thus often results in many comments and discussions.\",\"•\",\"Active but non-influential bloggers\",\". These bloggers post\",\"actively, but their posts may not generate sufficient\",\"interests to be ranked as the top 5 influentials. ‘Mat\",\"Lu’ and ‘Michael Rose’ were ranked 3\",\"rd\",\"and 4\",\"th\",\"top\",\"bloggers by TUAW, as they submitted 73 and 58 blog\",\"posts in the last 30 days (around 2 posts a day), re-\",\"spectively. Though these are much more than the 16\",\"posts of ‘Dan Lurie’, they are not among the top 5 in-\",\"fluential bloggers because their other statistics are not\",\"comparable with those of the influentials (i.e., having\",\"fewer comments and inlinks, and more outlinks).\",\"A closer look at two influential blog posts.\",\"Here we further study the most influential blog posts by\",\"number one (‘Erica Sadun’) and number five (‘Laurie A.\",\"Duncan’) influential bloggers, respectively. The most influ-\",\"ential blog post by ‘Erica Sadun’ is on keynote speech of\",\"Apple Inc. CEO, Steve Jobs\",\"19\",\"which fostered overwhelm-\",\"ing discussions through 63 comments and 80 inlinks.  By\",\"reviewing the comments, we observe that most people ap-\",\"preciated her efforts and found the blog post extremely infor\",\"-\",\"mative. The blog post was the first one dispensing a minute-\",\"by-minute description of the much-awaited keynote speech,\",\"new products, and services Apple would launch. The blog\",\"post was well-written and did not borrow information from\",\"any other sources. The most influential blog post by ‘Lau-\",\"rie A. Duncan’ detailed the violation of license agreements\",\"by macZOT\",\"20\",\"with a developer\",\"21\",\". This incident instigated a\",\"lot of discussion through 57 comments and 20 inlinks. Many\",\"people commented and cited this blog post, and agreed with\",\"the miserable state of license agreements, being appalled b\",\"y\",\"how big companies could exploit small developers by find-\",\"ing loopholes in the laws. Similar sentiments expressed in a\",\"surge of comments are an important feature of many influ-\",\"ential blog posts. The above study of two most influential\",\"posts shows the efficacy of the proposed model.\",\"5.2.2  Evaluating the Model\",\"As we know, there is no training and testing data for us\",\"to evaluate the efficacy of the proposed model. The absence\",\"of ground truth about influential bloggers presents another\",\"18\",\"http://www.tuaw.com/2007/01/09/iphone-will-not-allo\",\"w-\",\"user-installable-applic ations/\",\"19\",\"http://www.tuaw.com/2007/01/09/macworld-2007-\",\"keynote-liveblog/\",\"20\",\"http://www.maczot.com/\",\"21\",\"http://www.tuaw.com/2007/01/04/xpad-developer-says-\",\"maczot-and-brian-ball-ripped-him-off/\",\"212\"],\"page\":6},{\"texts\":[\"Max\",\"Average\",\"Max\",\"Average\",\"Max\",\"Average\",\"Max\",\"Average\",\"Erica Sadun\",\"75\",\"11.0197\",\"80   10.1316\",\"2935  830.0066\",\"15    2.5329\",\"David Chartier\",\"56\",\"11.3088\",\"32\",\"10.25\",\"3529  1054.912\",\"14    4.3529\",\"Scott McNulty\",\"112\",\"11.5607\",\"33    8.9252\",\"2246  623.2991\",\"12    2.5888\",\"Dan Lurie\",\"96\",\"19.6316\",\"37   10.2632\",\"1569  793.7368\",\"4    2.3158\",\"Laurie A. Duncan\",\"65\",\"16.2895\",\"34   10.6053\",\"2888  993.8947\",\"11    3.4737\",\"Mat Lu\",\"42\",\"8.0294\",\"29   10.0147\",\"1699  771.1471\",\"12    4.1029\",\"Michael Rose\",\"31\",\"8.7273\",\"21    9.6061\",\"1378  735.9848\",\"15    6.1515\",\"Number of Comments\",\"Number of Inlinks\",\"Length of Blog P\",\"ost   Number of Outlinks\",\"2\",\"0\",\"Total Number\",\"of Blog Posts\",\"Influential\",\"Blog Posts\",\"Inactive + Influential\",\"Active + Non-Influential\",\"152\",\"58\",\"0\",\"16\",\"26\",\"73\",\"4\",\"9\",\"68\",\"4\",\"107\",\"3\",\"Active + Influential\",\"Table 2: Comparison of statistics between different blogger\",\"s.\",\"Bloggers\",\"Active\",\"Inactive\",\"Influential\",\"S1: 17\",\"S2: 7\",\"Non-influential\",\"S3: 3\",\"S4: 0/1\",\"Table 3: Intersection of Digg and\",\"top 20 from our model.\",\"Bloggers\",\"Active\",\"Inactive\",\"Influential\",\"S1: 71\",\"S2: 14\",\"Non-influential\",\"S3: 8\",\"S4: 7\",\"Table 4: Distribution of 100 Digg\",\"blog posts.\",\"Bloggers\",\"Active\",\"Inactive\",\"Influential\",\"S1: 327\",\"S2: 42\",\"Non-influential\",\"S3: 131\",\"S4: 35\",\"Table  5:   Distribution  of  535\",\"TUAW blog posts.\",\"challenge. The key issue is how to find a reasonable refer-\",\"ence point for which four different types of bloggers can be\",\"evaluated so that we can observe their tangible differences.\",\"As an alternative to the ground truth, we resort to another\",\"Web2.0 site Digg (http://www.digg.com/) to provide a ref-\",\"erence point.  According to Digg, “Digg is all about user\",\"powered content. Everything is submitted and voted on by\",\"the Digg community. Share, discover, bookmark, and pro-\",\"mote stuff that’s important to you!”. As people read articles\",\"or blog posts, they can give their votes in the form of digg\",\"and these votes are recorded on Digg servers. This means,\",\"blog posts that appear on Digg are liked by their readers.\",\"The higher the digg score for a blog post is, the more it is\",\"liked. In a way, Digg can be considered as a large online\",\"user survey. Though only submitted blog posts are voted,\",\"Digg offers a way for us to evaluate the blog posts of the\",\"four types. Digg provides an API to extract data from their\",\"database for a window of 30 days.  We used this API to\",\"obtain the data for the month of January 2007. Given the\",\"nature of Digg, a not-liked blog post will not be submitted\",\"thus will not appear in Digg. For January 2007, there were\",\"in total 535 blog posts submitted on TUAW. As Digg only\",\"returns top 100 voted posts, we use these 100 blog posts at\",\"Digg as our benchmark in evaluation.\",\"We take the four categories of bloggers, viz.  1.  Active\",\"and Influential, 2. Inactive and Influential, 3. Active and\",\"Non-influential, and 4. Inactive and Non-influential and cat\",\"-\",\"egorize their posts into S1, S2, S3, and S4, respectively. We\",\"rank the blog posts of each category based on the influence\",\"score and pick top 20 blog posts from each of the first three\",\"categories. We randomly pick 20 blog posts from the last\",\"category in which bloggers are neither active nor influentia\",\"l.\",\"Next we compare these four sets of 20 blog posts with the\",\"Digg set of 100 blog posts to see how many posts in each\",\"set also appear in the Digg set. The results are shown in\",\"Table 3. From the table, we can see that S1 has 17 out of 20\",\"in the Digg set, and S4 has 0 or 1 found in the Digg set de-\",\"pending on randomization. The results show the differences\",\"among the four categories of bloggers and our model iden-\",\"tifies the influentials whose blog posts are more liked than\",\"others according to Digg. For reference purposes, we also\",\"provide the distributions of 100 Digg and 535 TUAW blog\",\"posts in Tables 4 and 5, respectively. Note that we selected\",\"top 5 active and 5 influential bloggers (Table 1), in which 3\",\"are both active and influential (Table 2). We observe from\",\"Tables 3, 4 and 5 that influential bloggers are more likely\",\"to be liked than active bloggers. More detailed discussion\",\"is omitted due to space limit. An interesting case is about\",\"S4 in Table 4 which has 7 blog posts liked by people even\",\"though they were non-influential and inactive. A closer ex-\",\"amination reveals that one of the bloggers in S4 was ranked\",\"6th in the list of influential bloggers and 4 of his blog posts\",\"appeared in Digg. In other words, increasing the number of\",\"top influential bloggers will change the current distributio\",\"n.\",\"5.2.3  Influential vs. Non-Influential Blog Posts\",\"Here we study the contrast in the characteristics between\",\"influential and non-influential blog posts. Using the defini-\",\"tion of influential blog posts from Section 3, we pick influen-\",\"tial blog posts submitted by the influential bloggers listed\",\"in\",\"Table 1. Rest of the blog posts are treated as non-influential\",\"blog posts.  Totally we have 22 influential and 513 non-\",\"influential blog posts for January 2007. Similar to Table 2,\",\"we compare the max and average statistics for all the four\",\"parameters (comments, inlinks, blog post length, and out-\",\"links) for both influential and non-influential blog posts an\",\"d\",\"report the results in Table 6. It shows influential blog posts\",\"are much longer in length and have far more comments.\",\"There are a lot more inlinks in influential blog posts, but\",\"the number of outlinks is a weaker piece of evidence, though\",\"the influential blog posts have slightly smaller number of\",\"outlinks.\",\"5.2.4  Effects and usages of weights\",\"There are four weights in our preliminary model to regu-\",\"late the contribution of four parameters toward the calcula\",\"-\",\"tion of the influence score using Eq 1 \u0026 Eq 3. To recall,\",\"w\",\"in\",\"is for the influence from incoming links,\",\"w\",\"out\",\"for the influ-\",\"ence from outgoing links,\",\"w\",\"(\",\"λ\",\") for the “goodness” of a blog\",\"post, and\",\"w\",\"comm\",\"for the number of comments. All weights\",\"take real values in [0\",\",\",\"1]. We now study how the change of\",\"their values will affect the ranking of the influentials.\",\"One may notice that\",\"w\",\"(\",\"λ\",\") simply scales the influence score\",\"of a blog post, so varying\",\"w\",\"(\",\"λ\",\") is not expected to affect the\",\"ranking of influential bloggers, but to scale up or down the\",\"influence scores. This is verified by conducting experiments\",\"in which the other three weights are fixed and only\",\"w\",\"(\",\"λ\",\") is\",\"varied. We observe that the relative ordering of the influen-\",\"tial bloggers remain the same while their influence score is\",\"scaled up or down. Although this weight is immaterial for\",\"identifying the influentials at one blog site, it can be used i\",\"n\",\"comparing the influential bloggers of different blog sites for\",\"213\"],\"page\":7},{\"texts\":[\"Total Number\",\"Max\",\"Avg\",\"Max\",\"Avg\",\"Max\",\"Avg\",\"Max\",\"Avg\",\"of Blog Posts\",\"Influential Blog Posts\",\"112\",\"74.18\",\"80\",\"38.63\",\"3529\",\"1999.32\",\"15\",\"3.36\",\"22\",\"Non-influential Blog Posts\",\"69\",\"10.84\",\"39\",\"8.96\",\"1930\",\"703.74\",\"27\",\"4.3\",\"513\",\"Number of Comments\",\"Number of Inlinks\",\"Length of Blog P\",\"ost\",\"Number of Outlinks\",\"Table 6: Comparison of statistics between Influential and no\",\"n-influential blog posts.\",\"sean bonner\",\"jason mccabe calacanis\",\"gregory han\",\"greg scher\",\"alberto escarlate\",\"judith meskill\",\"david touve\",\"michael sciannamea\",\"pariah s. burke\",\"barb dybwad\",\"marc orchant\",\"laurie a. duncan\",\"scott granneman\",\"scott mcnulty\",\"c.k. sample, iii\",\"jay savage\",\"dave caolo\",\"david chartier\",\"victor agreda, jr.\",\"damien barrett\",\"fabienne serriere\",\"jan kabili\",\"dan pourhadi\",\"conrad quilty-harper\",\"dan lurie\",\"erica sadun\",\"Feb-04\",\"1\",\"Mar-04\",\"1    2    3    4    5\",\"Apr-04\",\"1\",\"3\",\"2    4    5\",\"May-04\",\"1\",\"3    4    2\",\"5\",\"Jun-04\",\"1    3    2\",\"Jul-04\",\"1\",\"2\",\"3\",\"Aug-04\",\"2\",\"3\",\"4    1\",\"Sep-04\",\"3\",\"5    1    2\",\"4\",\"Oct-04\",\"2\",\"1\",\"Nov-04\",\"4\",\"1    2    3\",\"Dec-04\",\"3    2\",\"1    4\",\"Jan-05\",\"4    1\",\"2\",\"3\",\"Feb-05\",\"5\",\"1\",\"2    4    3\",\"Mar-05\",\"3\",\"1    5    2    4\",\"Apr-05\",\"1    4    2    3\",\"May-05\",\"1\",\"3    2\",\"Jun-05\",\"3\",\"1    2    4    5\",\"Jul-05\",\"5\",\"1    2    4    3\",\"Aug-05\",\"1\",\"5    2    3    4\",\"Sep 05\",\"3\",\"1\",\"2\",\"4\",\"5\",\"S\",\"ep-\",\"05\",\"3\",\"1\",\"2\",\"4\",\"5\",\"Oct-05\",\"3    1\",\"5    2    4\",\"Nov-05\",\"4    1    3    2\",\"5\",\"Dec-05\",\"2\",\"3\",\"1\",\"4\",\"5\",\"Jan-06\",\"5    2\",\"1\",\"4\",\"3\",\"Feb-06\",\"3    5\",\"1    4\",\"2\",\"Mar-06\",\"5\",\"4\",\"2\",\"3\",\"1\",\"Apr-06\",\"3\",\"4\",\"5\",\"1\",\"2\",\"May-06\",\"1\",\"5    2    3\",\"4\",\"Jun-06\",\"3    5\",\"1    2    4\",\"Jul-06\",\"2\",\"1\",\"4\",\"3\",\"5\",\"Aug-06\",\"4\",\"2    5    3\",\"1\",\"Sep-06\",\"3\",\"4    1    2    5\",\"Oct-06\",\"2\",\"5\",\"1    3\",\"4\",\"Nov-06\",\"2\",\"3\",\"1    4\",\"5\",\"Dec-06\",\"5\",\"2\",\"4\",\"3\",\"1\",\"Jan-07\",\"5\",\"4\",\"3\",\"2    1\",\"Figure 1:  Influential Bloggers’ blogging behavior\",\"over the whole TUAW blog history.\",\"normalization purposes (outside the scope of this work).\",\"For the remaining three weights,\",\"w\",\"comm\",\",\",\"w\",\"in\",\"and\",\"w\",\"out\",\",\",\"we fix two of them and observe how the ranking changes by\",\"varying the third weight. Fixing\",\"w\",\"in\",\"and\",\"w\",\"out\",\"and varying\",\"w\",\"comm\",\"from 0\",\".\",\"0 to 1\",\".\",\"0 in steps of 0\",\".\",\"1, we observe that the\",\"model stabilizes for\",\"w\",\"comm\",\"≥\",\"0\",\".\",\"6, i.e., it does not change\",\"the ranking of the influential bloggers. While varying\",\"w\",\"in\",\"and\",\"w\",\"out\",\"respectively, we observe that the model stabilizes\",\"when\",\"w\",\"in\",\"≥\",\"0\",\".\",\"9 and\",\"w\",\"out\",\"≥\",\"0\",\".\",\"2. To summarize, we obtain\",\"the same ranking of influential bloggers as shown in the right\",\"column of Table 1 for\",\"w\",\"comm\",\"≥\",\"0\",\".\",\"6\",\", w\",\"in\",\"≥\",\"0\",\".\",\"9\",\", w\",\"out\",\"≥\",\"0\",\".\",\"2.\",\"Clearly, the value change of the above three weights can\",\"lead to different rankings.  This allows one to adjust the\",\"weights of the model to attain different goals. With the pre-\",\"liminary model of default setting, we can tune these weights\",\"in identifying influential bloggers with different character\",\"-\",\"istics. For example, by setting\",\"w\",\"in\",\"and\",\"w\",\"out\",\"to 0, we can\",\"obtain influential bloggers based on the number of comments\",\"a blogger’s post obtained. Similarly we can obtain the blog\",\"post that received most citations or the blog post including\",\"the least outlinks. If one wants to emphasize one aspect, one\",\"can tune weights and obtain ranking to reflect that aspect.\",\"The increase of\",\"w\",\"out\",\"is one way to discourage the citations\",\"of other blog posts, in a way, encouraging a post with inde-\",\"pendent ideas. In short, these weights provide a means to\",\"further evolve and expand the preliminary model for a wide\",\"range of applications.\",\"5.2.5  Temporal patterns of the influentials\",\"Above, we study the influential bloggers with a time win-\",\"dow of 30 days (or monthly).  For a blog site that has a\",\"reasonably long history, we can also study the temporal pat-\",\"terns of its influential bloggers. The blog site TUAW pro-\",\"vides blogging data since its inception February 2004. We\",\"hence apply our model to identify top 5 influential bloggers\",\"with a moving 30-day window until January 2007, and there\",\"is no overlap between two consecutive windows.  In total,\",\"there are 26 influential bloggers during Feb.2004-Jan.2007.\",\"The temporal patterns of the influentials can be observed\",\"from a matrix in Figure 1. Influential bloggers are ordered\",\"according to the time they were recognized as influential\",\"vertically(column-wise), and the rows represent the progr\",\"es-\",\"sion of time. The (\",\"i\",\",\",\"j\",\")-th cell in this matrix stores the rank\",\"of the\",\"j\",\"th\",\"blogger in the\",\"i\",\"th\",\"time window. For example, the\",\"first cell (\",\"sean bonner\",\",\",\"Feb-04\",\") shows that\",\"Sean Bonner\",\"was\",\"ranked top 1 among the influential bloggers list in February\",\"2004\",\"22\",\". Black cells represent that the particular blogger was\",\"not among the top 5 for that time period. The color gra-\",\"dient represents rank of a influential blogger, a darker colo\",\"r\",\"representing a better rank.\",\"We can observe some different temporal patterns for the\",\"influentials in Figure 1. Among all the 26 bloggers, 17 are\",\"influential for at least 4 months. We broadly categorize the\",\"influential bloggers into the following:\",\"Long-term influentials\",\"They steadily maintain the sta-\",\"tus of being influential for a very long time.\",\"Scott\",\"McNulty\",\"is the best example of this category:\",\"Scott\",\"McNulty\",\"is steadily influential from Jan-05 till Jan-07.\",\"They can be considered “authority” in the community.\",\"Average-term influentials\",\"They maintain their influence\",\"status for 4-5 months. Examples of such bloggers from\",\"Figure 1 are “Sean Bonner”, “Gregory Han”, and “Barb\",\"Dybward”.\",\"Transient influentials\",\"They are influential for a\",\"very\",\"short\",\"time period (only one or two months). Examples are\",\"Michael Sciannamea\",\",\",\"Fabienne Serriere\",\"and\",\"Dan Pourhadi\",\".\",\"For instance,\",\"Fabienne Serriere\",\"was influential in Jan-\",\"06 and never became influential again.\",\"Burgeoning influentials\",\"They are emerging as influential\",\"bloggers recently. Bloggers that belong to this cate-\",\"gory are\",\"Dan Lurie\",\"and\",\"Erica Sadun\",\". They are the\",\"influentials worthy more follow-up examinations.\",\"Disparate bloggers can present different temporal pat-\",\"terns. Long-term influentials are more influential than othe\",\"r\",\"bloggers as they are more trustworthy as compared to other\",\"bloggers based on a long time of history. Burgeoning influ-\",\"entials have potential to become long-term ones. But it is\",\"difficult to say these things about transient influentials as\",\"they might become influential by chance. Certainly, there\",\"could be many other temporal patterns depending on a par-\",\"ticular application. The categories presented here are som\",\"e\",\"examples.  Many potential applications can be developed\",\"using categories. When we want to know about a new blog\",\"site, the best way to approach it is to look at its long-term\",\"influentials as they have lasting influence in the community.\",\"22\",\"In early stage of the blog site, there are a few cases in which\",\"there was little blogging activity such as\",\"Feb-04\",\",\",\"Oct-04\",\", and\",\"Nov-04\",\", resulting in fewer than 5 influentials.\",\"214\"],\"page\":8},{\"texts\":[\"0\",\"10\",\"20\",\"30\",\"40\",\"50\",\"60\",\"70\",\"80\",\"90\",\"0\",\"50\",\"100\",\"150\",\"Comments\",\"Inlinks\",\"0\",\"10\",\"20\",\"30\",\"40\",\"50\",\"60\",\"70\",\"80\",\"90\",\"0\",\"5\",\"10\",\"15\",\"20\",\"Outlinks\",\"Inlinks\",\"0\",\"10\",\"20\",\"30\",\"40\",\"50\",\"60\",\"70\",\"80\",\"90\",\"0\",\"1000\",\"2000\",\"3000\",\"4000\",\"Blog Post Length\",\"Inlinks\",\"(\",\"a\",\") r=0.297\",\"(\",\"b\",\") r=0.003\",\"(\",\"c\",\") r=0.203\",\"0\",\"20\",\"40\",\"60\",\"80\",\"100\",\"120\",\"0\",\"5\",\"10\",\"15\",\"20\",\"Outlinks\",\"Comments\",\"0\",\"20\",\"40\",\"60\",\"80\",\"100\",\"120\",\"0\",\"1000\",\"2000\",\"3000\",\"4000\",\"Blog Post Length\",\"Comments\",\"0\",\"500\",\"1000\",\"1500\",\"2000\",\"2500\",\"3000\",\"3500\",\"4000\",\"0\",\"5\",\"10\",\"15\",\"20\",\"Outlinks\",\"Blog Post Length\",\"(\",\"d\",\") r=-0.111\",\"(\",\"e\",\") r=0.208\",\"(\",\"f\",\") r=0.351\",\"Figure 2: Pairwise correlation plots of the four parameters\",\"(\",\"ι\",\",\",\"θ\",\",\",\"λ\",\", and\",\"γ\",\") of the blog posts.\",\"The blog posts of those average-term influentials can be used\",\"to understand the changing topics. The blog posts of bur-\",\"geoning influentials might contain the trendy buzz. With\",\"accumulated blogging data, we can also learn to predict if\",\"a burgeoning influential will more likely become long-term,\",\"average-term, or transient.\",\"5.2.6  Further Experiments\",\"We conduct more experiments to (1) examine the pair-\",\"wise correlations of the four factors; and (2) study another\",\"statistics - the rate of comments to extend our model.\",\"Correlation analysis.\",\"We perform pairwise correlation\",\"analysis between the parameters to further examine whether\",\"there is any redundant parameter. With four parameters,\",\"there are 6 pairwise correlations as shown in Figure 2(a)-\",\"(f). The number below each scatter plot is the correlation\",\"coefficient. We observe that there is no strong correlation\",\"between any pair of parameters.  In other words, none of\",\"the parameters can be covered by another one. We notice\",\"that 5 of 6 scatter plots show positive correlations, but the\",\"(d) scatter plot shows some negative correlation, which sug\",\"-\",\"gests that more outlinks in a blog post somehow mean fewer\",\"comments the post receives, and vice versa. This supports\",\"that links among blog posts are different from web links\",\"(Section 2).\",\"Rate of comments.\",\"This parameter seems a good indi-\",\"cator on how influential a post is. If a post receives many\",\"comments in a short period (i.e., it exhibits a spike), it has\",\"apparently generated a lot of response, indicating that the\",\"post is potentially influential. However, is the opposite tr\",\"ue\",\"too, i.e., the observation of a flat distribution of comment\",\"rates of a blog post implies a non-influential post? We con-\",\"duct a case study and present the results in Figures 3 and 4\",\"with comment rates of two influential blog posts: one related\",\"to the newly publicized iPhone release and the other about\",\"a competition held at Apple Inc. Figure 3 exhibits a spiky\",\"type of user response. Most of the comments were submitted\",\"during the first hour (over 50) after the blog post was pub-\",\"lished. On the other hand, comment rates in Figure 4 are\",\"relatively “flat”, around 10 comments per hour even after 7\",\"or 8 hours of the blog post submission. Since the spiky pat-\",\"tern is not a necessary characteristic of an influential post\",\",\",\"more research is needed to explore how to incorporate the\",\"comment rate. We envision that this parameter can be used\",\"to build a more refined model for special time-critical appli\",\"-\",\"cations like disaster prevention and management, emergenc\",\"y\",\"handling.\",\"Other extensions to the preliminary model include 1).\",\"study of spam comments filtering to prevent spam attacks\",\"using techniques mentioned in [17, 21], 2). study more ap-\",\"propriate blog post quality estimation techniques involv-\",\"ing content and literary analysis, and 3).  study different\",\"functions to non-linearly penalize influence due to outlink\",\"s.\",\"This basically means assigning negligibly small penalty if\",\"few outlinks are present and very high penalty for outra-\",\"geous number of outlinks. This is required to avoid penal-\",\"izing those novel blog posts that refer to a few blog posts\",\"to support their explanation. One such function could be\",\"exponential which would replace\",\"w\",\"out\",\"∑\",\"|\",\"θ\",\"|\",\"n\",\"=1\",\"I\",\"(\",\"p\",\"n\",\") in Eq. 1\",\"with exp(\",\"w\",\"out\",\"∑\",\"|\",\"θ\",\"|\",\"n\",\"=1\",\"I\",\"(\",\"p\",\"n\",\")). We would have to investigate\",\"thoroughly the role of\",\"w\",\"out\",\"in such a scenario.\",\"6.  CONCLUSIONS AND FUTURE WORK\",\"Blogosphere is one of the fastest growing, social media.\",\"The virtual communities in the blogosphere are not con-\",\"strained by physical proximity and allow for a new form of\",\"efficient communications. The influential bloggers naturall\",\"y\",\"exert their influence on other members, lead trends, and af-\",\"fect group interests in a community. They are the conduits\",\"of information in their communities. With many great suc-\",\"cesses of Web 2.0 applications, more and more people take\",\"215\"],\"page\":9},{\"texts\":[\"0\",\"10\",\"20\",\"30\",\"40\",\"50\",\"60\",\"2\",\"to 3 P\",\"M\",\", 1/9/0\",\"7\",\"3\",\"to 4 P\",\"M\",\", 1/9/0\",\"7\",\"4\",\"to 5 P\",\"M\",\", 1/9/0\",\"7\",\"5\",\"to 6 P\",\"M\",\", 1/9/0\",\"7\",\"6\",\"to 7 P\",\"M\",\", 1/9/0\",\"7\",\"8\",\"to 9 P\",\"M\",\", 1/9/0\",\"7\",\"9\",\"to 10\",\"P\",\"M, 1/9/\",\"0\",\"7\",\"1\",\"0\",\"t\",\"o\",\"1\",\"1 P\",\"M, 1/9\",\"/0\",\"7\",\"1\",\"1\",\"t\",\"o\",\"1\",\"2 P\",\"M, 1/9\",\"/0\",\"7\",\"3\",\"to 4 AM\",\", 1/\",\"10/0\",\"7\",\"5\",\"to 6 AM\",\", 1/\",\"10/0\",\"7\",\"9\",\"to 10\",\"A\",\"M\",\", 1\",\"/10/0\",\"7\",\"1\",\"0\",\"t\",\"o\",\"1\",\"1 AM,\",\"1/10/0\",\"7\",\"5\",\"to 6 P\",\"M\",\", 1/10/\",\"0\",\"7\",\"6\",\"to 7 P\",\"M\",\", 1/10/\",\"0\",\"7\",\"1\",\"2\",\"t\",\"o\",\"1\",\"P\",\"M, 1/11\",\"/0\",\"7\",\"9\",\"to 10\",\"P\",\"M, 1/11\",\"/0\",\"7\",\"1\",\"2\",\"t\",\"o\",\"1\",\"P\",\"M, 1/13\",\"/0\",\"7\",\"Date and Time\",\"Number of Comments\",\"Figure 3: Spiky comments reaction on a blog post\",\"related to iPhone.\",\"0\",\"10\",\"20\",\"30\",\"40\",\"50\",\"60\",\"3\",\"to 4 P\",\"M\",\", 2/7/0\",\"6\",\"4\",\"to 5 P\",\"M\",\", 2/7/0\",\"6\",\"5\",\"to 6 P\",\"M\",\", 2/7/0\",\"6\",\"6\",\"to 7 P\",\"M\",\", 2/7/0\",\"6\",\"7\",\"to 8 P\",\"M\",\", 2/7/0\",\"6\",\"8\",\"to 9 P\",\"M\",\", 2/7/0\",\"6\",\"9\",\"to 10\",\"P\",\"M, 2/7/\",\"0\",\"6\",\"1\",\"0\",\"t\",\"o\",\"1\",\"1 P\",\"M, 2/7\",\"/0\",\"6\",\"1\",\"2\",\"t\",\"o\",\"1\",\"A\",\"M\",\", 2\",\"/8/0\",\"6\",\"2\",\"to 3 AM\",\", 2/\",\"8/0\",\"6\",\"3\",\"to 4 AM\",\", 2/\",\"8/0\",\"6\",\"6\",\"to 7 AM\",\", 2/\",\"8/0\",\"6\",\"1\",\"1\",\"t\",\"o\",\"1\",\"2 AM,\",\"2/8/0\",\"6\",\"1\",\"2\",\"t\",\"o\",\"1\",\"P\",\"M, 2/8/\",\"0\",\"6\",\"2\",\"to 3 P\",\"M\",\", 2/8/0\",\"6\",\"8\",\"to 9 P\",\"M\",\", 2/8/0\",\"6\",\"9\",\"to 10\",\"P\",\"M, 2/8/\",\"0\",\"6\",\"1\",\"0\",\"t\",\"o\",\"1\",\"1 P\",\"M, 2/8\",\"/0\",\"6\",\"1\",\"1\",\"t\",\"o\",\"1\",\"2 P\",\"M, 2/9\",\"/0\",\"6\",\"Date and Time\",\"Number of Comments\",\"Figure 4: “Flat” comments reaction on a blog post\",\"related to some competition in Apple Inc.\",\"part in one form or another of activities in virtual commu-\",\"nities. Finding the influential bloggers will not only allow\",\"us to better understand interesting activities happening i\",\"n\",\"a virtual world, but also present unique opportunities for\",\"industry, sales, and advertisements.  With the speedy ex-\",\"pansion of the blogosphere, it is vital to develop novel tool\",\"s\",\"that facilitate people to participate, connect, and explor\",\"e.\",\"We address a novel problem of identifying influential blog-\",\"gers at a blog site by presenting a preliminary model of iden-\",\"tifying influential bloggers of a community blog site. Our\",\"work differs from existing works on blogosphere influence\",\"over traditional media, influential blog sites, and influenc\",\"e\",\"maximization within the blogosphere.  Influential bloggers\",\"can exist at many blog sites, regardless of these sites being\",\"influential or not. We examine essential issues of identifyi\",\"ng\",\"influential bloggers, evaluate the effects of various collec\",\"table\",\"statistics from a blog site on determining blog-post influen\",\"ce,\",\"develop unique experiments using another Web2.0 applica-\",\"tion, and conduct experiments by using the whole history of\",\"blog posts of a real-world blog site. The extensive but still\",\"preliminary work demonstrates that (1) influential blogger\",\"s\",\"are not necessarily active bloggers, (2) our model can effec-\",\"tively find influential bloggers, (3) by tuning the weights as-\",\"sociated with the parameters of the preliminary model, one\",\"can examine how different parameters impact the influence\",\"ranking for different needs, and (4) the preliminary model\",\"can serve as a baseline in identifying influential bloggers a\",\"nd\",\"can be extended by incorporating additional parameters to\",\"discover different patterns. We expect that the preliminary\",\"model will evolve to address many new needs arising from\",\"the real (or rather virtual) world.\",\"7.  REFERENCES\",\"[1] Chris Anderson.\",\"The long tail : why the future of\",\"business is selling less of more\",\". New York : Hyperion,\",\"2006.\",\"[2] Alvin Chin and Mark Chignell. A social hypertext\",\"model for finding community in blogs. In\",\"HYPERTEXT ’06: Proceedings of the seventeenth\",\"conference on Hypertext and hypermedia\",\", pages 11–22,\",\"New York, NY, USA, 2006. ACM Press.\",\"[3] T. Coffman and S. Marcus. Dynamic classification of\",\"groups through social network analysis and HMMs. In\",\"Proceedings of IEEE Aerospace Conference\",\", 2004.\",\"[4] Daniel Drezner and Henry Farrell. The power and\",\"politics of blogs. In\",\"American Political Science\",\"Association Annual Conference\",\", 2004.\",\"[5] T. Elkin. Just an online minute... online forecast.\",\"http://publications.mediapost.com/index.cfm?fuseact\",\"ion\",\"=Articles.showArticle art aid=29803.\",\"[6] Gerald D. Fensterer.\",\"Planning and Assessing Stability\",\"Operations: A Proposed Value Focus Thinking\",\"Approach\",\". PhD thesis, Air Force Institute of\",\"Technology, 2007.\",\"[7] G. W. Flake, S. Lawrence, and C. L. Giles. Efficient\",\"identification of web communities. In\",\"6th International\",\"Conference on Knowledge Discovery and Data Mining\",\",\",\"2000.\",\"[8] Kathy E. Gill. How can we measure the influence of\",\"the blogosphere? In\",\"Proceedings of the WWW’04:\",\"workshop on the Weblogging Ecosystem: Aggregation,\",\"Analysis and Dynamics\",\", 2004.\",\"[9] Dan Gillmor.\",\"We the Media: Grassroots Journalism\",\"by the People, for the People\",\". O’Reilly, 2006.\",\"[10] D. Gruhl, David Liben-Nowell, R. Guha, and\",\"A. Tomkins. Information diffusion through blogspace.\",\"SIGKDD Exploration Newsletter\",\", 6(2):43–52, 2004.\",\"[11] Daniel Gruhl, R. Guha, Ravi Kumar, Jasmine Novak,\",\"and Andrew Tomkins. The predictive power of online\",\"chatter. In\",\"KDD ’05: Proceeding of the eleventh ACM\",\"SIGKDD international conference on Knowledge\",\"discovery in data mining\",\", pages 78–87, New York, NY,\",\"USA, 2005. ACM Press.\",\"[12] Akshay Java, Pranam Kolari, Tim Finin, and Tim\",\"Oates. Modeling the spread of influence on the\",\"blogosphere. In\",\"Proceedings of the 15th International\",\"World Wide Web Conference\",\", 2006.\",\"[13] R. L. Keeney and H. Raiffa.\",\"Decisions with Multiple\",\"Objectives: Preferences and Value Tradeoffs\",\".\",\"Cambridge University Press, 1993.\",\"[14] Ed Keller and Jon Berry.\",\"One American in ten tells\",\"the other nine how to vote, where to eat and, what to\",\"buy. They are The Influentials\",\". The Free Press, 2003.\",\"216\"],\"page\":10},{\"texts\":[\"[15] David Kempe, Jon Kleinberg, and Eva Tardos.\",\"Maximizing the spread of influence through a social\",\"network. In\",\"Proceedings of the KDD\",\", pages 137–146,\",\"New York, NY, USA, 2003. ACM Press.\",\"[16] J. Kleinberg. Authoritative sources in a hyperlinked\",\"environment. In\",\"9th ACM-SIAM Symposium on\",\"Discrete Algorithms\",\", 1998.\",\"[17] P. Kolari, T. Finin, and A. Joshi. SVMs for the\",\"blogosphere: Blog identification and splog detection.\",\"In\",\"AAAI Spring Symposium on Computational\",\"Approaches to Analyzing Weblogs\",\", 2006.\",\"[18] Apostolos Kritikopoulos, Martha Sideri, and Iraklis\",\"Varlamis. Blogrank: ranking weblogs based on\",\"connectivity and similarity features. In\",\"AAA-IDEA\",\"’06: Proceedings of the 2nd international workshop on\",\"Advanced architectures and algorithms for internet\",\"delivery and applications\",\", page 8, 2006.\",\"[19] Ravi Kumar, Jasmine Novak, Prabhakar Raghavan,\",\"and Andrew Tomkins. On the Bursty Evolution of\",\"Blogspace. In\",\"Proceedings of the 12th international\",\"conference on World Wide Web\",\", pages 568–576, New\",\"York, NY, USA, 2003. ACM Press.\",\"[20] Yu-Ru Lin, Hari Sundaram, Yun Chi, Jun Tatemura,\",\"and Belle Tseng. Discovery of blog communities based\",\"on mutual awareness. In\",\"Proceedings of the 3rd annual\",\"workshop on webloging ecosystem: aggreation, analysis\",\"and dynamics\",\", 2006.\",\"[21] Yu-Ru Lin, Hari Sundaram, Yun Chi, Junichi\",\"Tatemura, and Belle L. Tseng. Splog detection using\",\"self-similarity analysis on blog temporal dynamics. In\",\"Proceedings of the 3rd international workshop on\",\"Adversarial information retrieval on the web\",\"(AIRWeb)\",\", pages 1–8, New York, NY, USA, 2007.\",\"ACM Press.\",\"[22] Gilad Mishne and Maarten de Rijke. Deriving\",\"wishlists from blogs show us your blog, and we’ll tell\",\"you what books to buy. In\",\"Proceedings of the 15th\",\"international conference on World Wide Web\",\", pages\",\"925–926, New York, NY, USA, 2006. ACM Press.\",\"[23] Tim O’Reilly. What is Web 2.0 - design patterns and\",\"business models for the next generation of software.\",\"http://www.oreillynet.com/pub/a/oreilly/tim/news/\",\"2005/09/30/what-is-web-20.html, September 2005.\",\"[24] Lawrence Page, Sergey Brin, Rajeev Motwani, and\",\"Terry Winograd. The pagerank citation ranking:\",\"Bringing order to the web. Technical report, Stanford\",\"Digital Library Technologies Project, 1998.\",\"[25] Matthew Richardson and Pedro Domingos. Mining\",\"knowledge-sharing sites for viral marketing. In\",\"Proceedings of the eighth ACM SIGKDD international\",\"conference on Knowledge Discovery and Data mining\",\",\",\"pages 61–70, New York, NY, USA, 2002. ACM Press.\",\"[26] Robert Scoble and Shel Israel.\",\"Naked conversations :\",\"how blogs are changing the way businesses talk with\",\"customers\",\". John Wiley, 2006.\",\"[27] Mike Thelwall. Bloggers under the London attacks:\",\"Top information sources and topics. In\",\"Proceedings of\",\"the 3rd annual workshop on webloging ecosystem:\",\"aggreation, analysis and dynamics\",\", 2006.\",\"[28] Ying Zhou and Joseph Davis. Community discovery\",\"and analysis in blogspace. In\",\"Proceedings of the 15th\",\"international conference on World Wide Web\",\", pages\",\"1017–1018, New York, NY, USA, 2006. ACM Press.\",\"217\",\"View publication stats\",\"View publication stats\"],\"page\":11}],\"caption\":\"\"}}"
  },
  {
    "name": "2020-10/0f047c.md",
    "id": "2020-10/0f047c",
    "fileType": "markdown",
    "text": "# System Theory\n\n## Backlinks\n\n- [[[System Behavior]]](../2020-10/4161a4.md)\n - [[[Peter Kruse]]](../2020-10/794a82.md)",
    "metadata": ""
  },
  {
    "name": "2020-10/10be95.md",
    "id": "2020-10/10be95",
    "fileType": "markdown",
    "text": "# Jordan Perterson (Person)\n#person\n\nCandadian clinic psychologist.\n\n[[[Creativity]]](../2020-09/97a797.md)\n",
    "metadata": ""
  },
  {
    "name": "2020-10/132c46.md",
    "id": "2020-10/132c46",
    "fileType": "markdown",
    "text": "# Efficient Learning\nvs [[[Efficient Unlearning]]](../2022-04/cbf9ad.md) \n\n## When\n- State-dependent memory [https://en.wikipedia.org/wiki/State-dependent_memory]\n- Train your brain to get into the mental state of learning [[[Mental States]]](../2020-10/ae8b0a.md)\n- The more active you are in your learning, the more effective is memorization\n\n## What\n- [[[Concept]]](../2022-01/b9f7a6.md)  are more important than facts [[[Concepts vs. Facts]]](../2022-01/7e400a.md) \n[[[Supervised Learning]]](../2020-10/486523.md)\n\n## Repetition\nKey element of learning is repetition. To achieve efficiency, since the amount if knowledge cannot be modified, any *efficient learning method* can just optimize the repetition part. [[[SuperMemo.com]]](../2020-10/b6e745.md) and Rote learning are two examples.",
    "metadata": ""
  },
  {
    "name": "2020-10/132ea1.md",
    "id": "2020-10/132ea1",
    "fileType": "markdown",
    "text": "# Problem of Induction (Read)\n\nhttps://en.wikipedia.org/wiki/Problem_of_induction\nhttps://plato.stanford.edu/entries/induction-problem/\n",
    "metadata": ""
  },
  {
    "name": "2020-10/163bbd.md",
    "id": "2020-10/163bbd",
    "fileType": "markdown",
    "text": "# Model\nAbstraction of reality.\n\nA mathematical or computational model simulates [[[Simulation]]](../2020-10/0d2e14.md) some, desired features of the reality. They are always a simplification. Those models can be used to extrapolate and predict behavior to a certain degree.\n\nSince Models are always just an approximation they may suffer from overfitting, which means the model is too approximated to the training data and conclusion based on that model might be wrong. Overfitting is quite common in machine learning, where a model is derived from a small set of data in the training or supervised learning phase.",
    "metadata": ""
  },
  {
    "name": "2020-10/18eaa8.md",
    "id": "2020-10/18eaa8",
    "fileType": "markdown",
    "text": "# Interesting People\n- George Hotz: Engineer at Tesla\n\n\n",
    "metadata": ""
  },
  {
    "name": "2020-10/19313a.pdf",
    "id": "2020-10/19313a",
    "fileType": "pdf",
    "text": "",
    "metadata": "{\"nodeId\":\"323032302d31302f313933313361\",\"pdf\":{\"pages\":[{\"texts\":[\"Chapter 3\",\"Finding Similar Items\",\"A fundamental data-mining problem is to examine data for “si\",\"milar” items. We\",\"shall take up applications in Section 3.1, but an example wou\",\"ld be looking at a\",\"collection of Web pages and finding near-duplicate pages. Th\",\"ese pages could be\",\"plagiarisms, for example, or they could be mirrors that have\",\"almost the same\",\"content but differ in information about the host and about oth\",\"er mirrors.\",\"We begin by phrasing the problem of similarity as one of findin\",\"g sets with\",\"a relatively large intersection. We show how the problem of fi\",\"nding textually\",\"similar documents can be turned into such a set problem by the\",\"technique known\",\"as “shingling.”  Then, we introduce a technique called “minh\",\"ashing,” which\",\"compresses large sets in such a way that we can still deduce th\",\"e similarity of\",\"the underlying sets from their compressed versions. Other t\",\"echniques that work\",\"when the required degree of similarity is very high are cover\",\"ed in Section 3.9.\",\"Another important problem that arises when we search for sim\",\"ilar items of\",\"any kind is that there may be far too many pairs of items to test\",\"each pair for\",\"their degree of similarity, even if computing the similarit\",\"y of any one pair can be\",\"made very easy. That concern motivates a technique called “l\",\"ocality-sensitive\",\"hashing,” for focusing our search on pairs that are most like\",\"ly to be similar.\",\"Finally, we explore notions of “similarity” that are not exp\",\"ressible as inter-\",\"section of sets. This study leads us to consider the theory of\",\"distance measures\",\"in arbitrary spaces. It also motivates a general framework f\",\"or locality-sensitive\",\"hashing that applies for other definitions of “similarity.”\",\"3.1  Applications of Near-Neighbor Search\",\"We shall focus initially on a particular notion of “similari\",\"ty”: the similarity of\",\"sets by looking at the relative size of their intersection. T\",\"his notion of similarity\",\"is called “Jaccard similarity,” and will be introduced in Se\",\"ction 3.1.1. We then\",\"examine some of the uses of finding similar sets. These includ\",\"e finding textually\",\"similar documents and collaborative filtering by finding sim\",\"ilar customers and\",\"similar products. In order to turn the problem of textual sim\",\"ilarity of documents\",\"55\"],\"page\":1},{\"texts\":[\"56\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"into one of set intersection, we use a technique called “shin\",\"gling,” which is\",\"introduced in Section 3.2.\",\"3.1.1  Jaccard Similarity of Sets\",\"The\",\"Jaccard similarity\",\"of sets\",\"S\",\"and\",\"T\",\"is\",\"|\",\"S\",\"∩\",\"T\",\"|\",\"/\",\"|\",\"S\",\"∪\",\"T\",\"|\",\", that is, the ratio\",\"of the size of the intersection of\",\"S\",\"and\",\"T\",\"to the size of their union. We shall\",\"denote the Jaccard similarity of\",\"S\",\"and\",\"T\",\"by\",\"SIM\",\"(\",\"S, T\",\").\",\"Example 3.1:\",\"In Fig. 3.1 we see two sets\",\"S\",\"and\",\"T\",\". There are three elements\",\"in their intersection and a total of eight elements that appe\",\"ar in\",\"S\",\"or\",\"T\",\"or both.\",\"Thus,\",\"SIM\",\"(\",\"S, T\",\") = 3\",\"/\",\"8.\",\"2\",\"\\u0000\",\"\\u0000\",\"\\u0001\",\"\\u0001\",\"\\u0000\",\"\\u0000\",\"\\u0001\",\"\\u0001\",\"\\u0000\",\"\\u0000\",\"\\u0001\",\"\\u0001\",\"\\u0000\",\"\\u0000\",\"\\u0001\",\"\\u0001\",\"\\u0000\",\"\\u0000\",\"\\u0001\",\"\\u0001\",\"\\u0000\",\"\\u0000\",\"\\u0001\",\"\\u0001\",\"\\u0000\",\"\\u0000\",\"\\u0001\",\"\\u0001\",\"\\u0000\",\"\\u0000\",\"\\u0001\",\"\\u0001\",\"T\",\"S\",\"Figure 3.1: Two sets with Jaccard similarity 3/8\",\"3.1.2  Similarity of Documents\",\"An important class of problems that Jaccard similarity addr\",\"esses well is that\",\"of finding textually similar documents in a large corpus such\",\"as the Web or a\",\"collection of news articles. We should understand that the a\",\"spect of similarity\",\"we are looking at here is character-level similarity, not “s\",\"imilar meaning,” which\",\"requires us to examine the words in the documents and their us\",\"es. That problem\",\"is also interesting but is addressed by other techniques, wh\",\"ich we hinted at in\",\"Section 1.3.1. However, textual similarity also has import\",\"ant uses. Many of\",\"these involve finding duplicates or near duplicates. First,\",\"let us observe that\",\"testing whether two documents are exact duplicates is easy;\",\"just compare the\",\"two documents character-by-character, and if they ever diff\",\"er then they are not\",\"the same. However, in many applications, the documents are n\",\"ot identical, yet\",\"they share large portions of their text. Here are some exampl\",\"es:\"],\"page\":2},{\"texts\":[\"3.1.  APPLICATIONS OF NEAR-NEIGHBOR SEARCH\",\"57\",\"Plagiarism\",\"Finding plagiarized documents tests our ability to find text\",\"ual similarity. The\",\"plagiarizer may extract only some parts of a document for his\",\"own. He may alter\",\"a few words and may alter the order in which sentences of the or\",\"iginal appear.\",\"Yet the resulting document may still contain 50% or more of th\",\"e original. No\",\"simple process of comparing documents character by charact\",\"er will detect a\",\"sophisticated plagiarism.\",\"Mirror Pages\",\"It is common for important or popular Web sites to be duplicat\",\"ed at a number\",\"of hosts, in order to share the load. The pages of these\",\"mirror\",\"sites will be\",\"quite similar, but are rarely identical. For instance, they\",\"might each contain\",\"information associated with their particular host, and the\",\"y might each have\",\"links to the other mirror sites but not to themselves. A relat\",\"ed phenomenon\",\"is the appropriation of pages from one class to another. Thes\",\"e pages might\",\"include class notes, assignments, and lecture slides. Simi\",\"lar pages might change\",\"the name of the course, year, and make small changes from year\",\"to year. It\",\"is important to be able to detect similar pages of these kinds\",\", because search\",\"engines produce better results if they avoid showing two pag\",\"es that are nearly\",\"identical within the first page of results.\",\"Articles from the Same Source\",\"It is common for one reporter to write a news article that gets\",\"distributed,\",\"say through the Associated Press, to many newspapers, which\",\"then publish\",\"the article on their Web sites. Each newspaper changes the ar\",\"ticle somewhat.\",\"They may cut out paragraphs, or even add material of their own\",\". They most\",\"likely will surround the article by their own logo, ads, and l\",\"inks to other articles\",\"at their site. However, the core of each newspaper’s page wil\",\"l be the original\",\"article. News aggregators, such as Google News, try to find al\",\"l versions of such\",\"an article, in order to show only one, and that task requires fi\",\"nding when two\",\"Web pages are textually similar, although not identical.\",\"1\",\"3.1.3  Collaborative Filtering as a Similar-Sets Problem\",\"Another class of applications where similarity of sets is ve\",\"ry important is called\",\"collaborative filtering\",\", a process whereby we recommend to users items that were\",\"liked by other users who have exhibited similar tastes.  We sh\",\"all investigate\",\"collaborative filtering in detail in Section 9.3, but for the\",\"moment let us see\",\"some common examples.\",\"1\",\"News aggregation also involves finding articles that are abo\",\"ut the same topic, even though\",\"not textually similar.  This problem too can yield to a simila\",\"rity search, but it requires\",\"techniques other than Jaccard similarity of sets.\"],\"page\":3},{\"texts\":[\"58\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"On-Line Purchases\",\"Amazon.com has millions of customers and sells millions of i\",\"tems. Its database\",\"records which items have been bought by which customers. We c\",\"an say two cus-\",\"tomers are similar if their sets of purchased items have a hig\",\"h Jaccard similarity.\",\"Likewise, two items that have sets of purchasers with high Ja\",\"ccard similarity\",\"will be deemed similar. Note that, while we might expect mirr\",\"or sites to have\",\"Jaccard similarity above 90%, it is unlikely that any two cus\",\"tomers have Jac-\",\"card similarity that high (unless they have purchased only o\",\"ne item). Even a\",\"Jaccard similarity like 20% might be unusual enough to ident\",\"ify customers with\",\"similar tastes. The same observation holds for items; Jacca\",\"rd similarities need\",\"not be very high to be significant.\",\"Collaborative filtering requires several tools, in additio\",\"n to finding similar\",\"customers or items, as we discuss in Chapter 9.  For example, t\",\"wo Amazon\",\"customers who like science-fiction might each buy many scien\",\"ce-fiction books,\",\"but only a few of these will be in common. However, by combinin\",\"g similarity-\",\"finding with clustering (Chapter 7), we might be able to disco\",\"ver that science-\",\"fiction books are mutually similar and put them in one group. T\",\"hen, we can\",\"get a more powerful notion of customer-similarity by asking\",\"whether they made\",\"purchases within many of the same groups.\",\"Movie Ratings\",\"NetFlix records which movies each of its customers rented, a\",\"nd also the ratings\",\"assigned to those movies by the customers. We can see movies a\",\"s similar if they\",\"were rented or rated highly by many of the same customers, and\",\"see customers\",\"as similar if they rented or rated highly many of the same movi\",\"es. The same\",\"observations that we made for Amazon above apply in this situ\",\"ation: similarities\",\"need not be high to be significant, and clustering movies by ge\",\"nre will make\",\"things easier.\",\"In addition, the matter of ratings introduces a new element.\",\"Some options\",\"are:\",\"1. Ignore low-rated customer/movie pairs; that is, treat th\",\"ese events as if\",\"the customer never rented the movie.\",\"2. When comparing customers, imagine two set elements for ea\",\"ch movie,\",\"“liked” and “hated.” If a customer rated a movie highly, put t\",\"he “liked”\",\"for that movie in the customer’s set. If they gave a low rating\",\"to a movie,\",\"put “hated” for that movie in their set. Then, we can look for h\",\"igh Jaccard\",\"similarity among these sets. We can do a similar trick when co\",\"mparing\",\"movies.\",\"3. If ratings are 1-to-5-stars, put a movie in a customer’s se\",\"t\",\"n\",\"times if\",\"they rated the movie\",\"n\",\"-stars. Then, use\",\"Jaccard similarity for bags\",\"when\",\"measuring the similarity of customers. The Jaccard similar\",\"ity for bags\",\"B\",\"and\",\"C\",\"is defined by counting an element\",\"n\",\"times in the intersection if\"],\"page\":4},{\"texts\":[\"3.2.  SHINGLING OF DOCUMENTS\",\"59\",\"n\",\"is the minimum of the number of times the element appears in\",\"B\",\"and\",\"C\",\". In the union, we count the element the sum of the number of tim\",\"es it\",\"appears in\",\"B\",\"and in\",\"C\",\".\",\"Example 3.2:\",\"The bag-similarity of bags\",\"{\",\"a, a, a, b\",\"}\",\"and\",\"{\",\"a, a, b, b, c\",\"}\",\"is 1/3.\",\"The intersection counts\",\"a\",\"twice and\",\"b\",\"once, so its size is 3. The size of the union\",\"of two bags is always the sum of the sizes of the two bags, or 9 in\",\"this case.\",\"2\",\"3.1.4  Exercises for Section 3.1\",\"Exercise 3.1.1:\",\"Compute the Jaccard similarities of each pair of the followi\",\"ng\",\"three sets:\",\"{\",\"1\",\",\",\"2\",\",\",\"3\",\",\",\"4\",\"}\",\",\",\"{\",\"2\",\",\",\"3\",\",\",\"5\",\",\",\"7\",\"}\",\", and\",\"{\",\"2\",\",\",\"4\",\",\",\"6\",\"}\",\".\",\"Exercise 3.1.2:\",\"Compute the Jaccard bag similarity of each pair of the fol-\",\"lowing three bags:\",\"{\",\"1\",\",\",\"1\",\",\",\"1\",\",\",\"2\",\"}\",\",\",\"{\",\"1\",\",\",\"1\",\",\",\"2\",\",\",\"2\",\",\",\"3\",\"}\",\", and\",\"{\",\"1\",\",\",\"2\",\",\",\"3\",\",\",\"4\",\"}\",\".\",\"!! Exercise 3.1.3:\",\"Suppose we have a universal set\",\"U\",\"of\",\"n\",\"elements, and we\",\"choose two subsets\",\"S\",\"and\",\"T\",\"at random, each with\",\"m\",\"of the\",\"n\",\"elements. What\",\"is the expected value of the Jaccard similarity of\",\"S\",\"and\",\"T\",\"?\",\"3.2  Shingling of Documents\",\"The most effective way to represent documents as sets, for the\",\"purpose of iden-\",\"tifying lexically similar documents is to construct from th\",\"e document the set\",\"of short strings that appear within it. If we do so, then docum\",\"ents that share\",\"pieces as short as sentences or even phrases will have many co\",\"mmon elements\",\"in their sets, even if those sentences appear in different ord\",\"ers in the two docu-\",\"ments. In this section, we introduce the simplest and most co\",\"mmon approach,\",\"shingling, as well as an interesting variation.\",\"3.2.1\",\"k\",\"-Shingles\",\"A document is a string of characters. Define a\",\"k\",\"-shingle for a document to be\",\"any substring of length\",\"k\",\"found within the document. Then, we may associate\",\"with each document the set of\",\"k\",\"-shingles that appear one or more times within\",\"that document.\",\"Example 3.3:\",\"Suppose our document\",\"D\",\"is the string\",\"abcdabd\",\", and we pick\",\"k\",\"= 2. Then the set of 2-shingles for\",\"D\",\"is\",\"{\",\"ab\",\",\",\"bc\",\",\",\"cd\",\",\",\"da\",\",\",\"bd\",\"}\",\".\",\"Note that the substring\",\"ab\",\"appears twice within\",\"D\",\", but appears only once\",\"as a shingle. A variation of shingling produces a bag, rather\",\"than a set, so each\",\"shingle would appear in the result as many times as it appears\",\"in the document.\",\"However, we shall not use bags of shingles here.\",\"2\",\"There are several options regarding how white space (blank,\",\"tab, newline,\",\"etc.) is treated. It probably makes sense to replace any sequ\",\"ence of one or more\"],\"page\":5},{\"texts\":[\"60\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"white-space characters by a single blank. That way, we disti\",\"nguish shingles that\",\"cover two or more words from those that do not.\",\"Example 3.4:\",\"If we use\",\"k\",\"= 9, but eliminate whitespace altogether, then we\",\"would see some lexical similarity in the sentences “\",\"The plane was ready for\",\"touch down\",\"”. and “\",\"The quarterback scored a touchdown\",\"”. However, if we\",\"retain the blanks, then the first has shingles\",\"touch dow\",\"and\",\"ouch down\",\", while\",\"the second has\",\"touchdown\",\". If we eliminated the blanks, then both would have\",\"touchdown\",\".\",\"2\",\"3.2.2  Choosing the Shingle Size\",\"We can pick\",\"k\",\"to be any constant we like. However, if we pick\",\"k\",\"too small, then\",\"we would expect most sequences of\",\"k\",\"characters to appear in most documents.\",\"If so, then we could have documents whose shingle-sets had hi\",\"gh Jaccard simi-\",\"larity, yet the documents had none of the same sentences or ev\",\"en phrases. As\",\"an extreme example, if we use\",\"k\",\"= 1, most Web pages will have most of the\",\"common characters and few other characters, so almost all We\",\"b pages will have\",\"high similarity.\",\"How large\",\"k\",\"should be depends on how long typical documents are and how\",\"large the set of typical characters is. The important thing t\",\"o remember is:\",\"•\",\"k\",\"should be picked large enough that the probability of any giv\",\"en shingle\",\"appearing in any given document is low.\",\"Thus, if our corpus of documents is emails, picking\",\"k\",\"= 5 should be fine.\",\"To see why, suppose that only letters and a general white-spa\",\"ce character ap-\",\"pear in emails (although in practice, most of the printable A\",\"SCII characters\",\"can be expected to appear occasionally).  If so, then there wo\",\"uld be 27\",\"5\",\"=\",\"14,348,907 possible shingles. Since the typical email is mu\",\"ch smaller than 14\",\"million characters long, we would expect\",\"k\",\"= 5 to work well, and indeed it does.\",\"However, the calculation is a bit more subtle. Surely, more t\",\"han 27 charac-\",\"ters appear in emails, However, all characters do not appear\",\"with equal proba-\",\"bility. Common letters and blanks dominate, while ”z” and ot\",\"her letters that\",\"have high point-value in Scrabble are rare. Thus, even short\",\"emails will have\",\"many 5-shingles consisting of common letters, and the chanc\",\"es of unrelated\",\"emails sharing these common shingles is greater than would b\",\"e implied by the\",\"calculation in the paragraph above. A good rule of thumb is to\",\"imagine that\",\"there are only 20 characters and estimate the number of\",\"k\",\"-shingles as 20\",\"k\",\". For\",\"large documents, such as research articles, choice\",\"k\",\"= 9 is considered safe.\",\"3.2.3  Hashing Shingles\",\"Instead of using substrings directly as shingles, we can pic\",\"k a hash function\",\"that maps strings of length\",\"k\",\"to some number of buckets and treat the resulting\",\"bucket number as the shingle. The set representing a documen\",\"t is then the\"],\"page\":6},{\"texts\":[\"3.2.  SHINGLING OF DOCUMENTS\",\"61\",\"set of integers that are bucket numbers of one or more\",\"k\",\"-shingles that appear\",\"in the document. For instance, we could construct the set of 9\",\"-shingles for a\",\"document and then map each of those 9-shingles to a bucket num\",\"ber in the\",\"range 0 to 2\",\"32\",\"−\",\"1.  Thus, each shingle is represented by four bytes instead\",\"of nine. Not only has the data been compacted, but we can now ma\",\"nipulate\",\"(hashed) shingles by single-word machine operations.\",\"Notice that we can differentiate documents better if we use 9-\",\"shingles and\",\"hash them down to four bytes than to use 4-shingles, even thou\",\"gh the space used\",\"to represent a shingle is the same. The reason was touched upo\",\"n in Section 3.2.2.\",\"If we use 4-shingles, most sequences of four bytes are unlike\",\"ly or impossible to\",\"find in typical documents. Thus, the effective number of differ\",\"ent shingles is\",\"much less than 2\",\"32\",\"−\",\"1. If, as in Section 3.2.2, we assume only 20 characters are\",\"frequent in English text, then the number of different 4-shin\",\"gles that are likely\",\"to occur is only (20)\",\"4\",\"= 160,000. However, if we use 9-shingles, there are many\",\"more than 2\",\"32\",\"likely shingles. When we hash them down to four bytes, we can\",\"expect almost any sequence of four bytes to be possible, as wa\",\"s discussed in\",\"Section 1.3.2.\",\"3.2.4  Shingles Built from Words\",\"An alternative form of shingle has proved effective for the pr\",\"oblem of identifying\",\"similar news articles, mentioned in Section 3.1.2. The expl\",\"oitable distinction for\",\"this problem is that the news articles are written in a rather\",\"different style than\",\"are other elements that typically appear on the page with the\",\"article. News\",\"articles, and most prose, have a lot of stop words (see Sectio\",\"n 1.3.1), the most\",\"common words such as “and,” “you,” “to,” and so on. In many app\",\"lications,\",\"we want to ignore stop words, since they don’t tell us anythin\",\"g useful about\",\"the article, such as its topic.\",\"However, for the problem of finding similar news articles, it\",\"was found that\",\"defining a shingle to be a stop word followed by the next two wor\",\"ds, regardless\",\"of whether or not they were stop words, formed a useful set of s\",\"hingles. The\",\"advantage of this approach is that the news article would the\",\"n contribute more\",\"shingles to the set representing the Web page than would the s\",\"urrounding ele-\",\"ments. Recall that the goal of the exercise is to find pages tha\",\"t had the same\",\"articles, regardless of the surrounding elements. By biasi\",\"ng the set of shingles\",\"in favor of the article, pages with the same article and differ\",\"ent surrounding\",\"material have higher Jaccard similarity than pages with the\",\"same surrounding\",\"material but with a different article.\",\"Example 3.5:\",\"An ad might have the simple text “\",\"Buy Sudzo\",\".” However, a\",\"news article with the same idea might read something like “\",\"A\",\"spokesperson\",\"for the\",\"Sudzo Corporation revealed today\",\"that\",\"studies\",\"have\",\"shown\",\"it is\",\"good\",\"for\",\"people\",\"to\",\"buy Sudzo products\",\".” Here, we have italicized all the\",\"likely stop words, although there is no set number of the most\",\"frequent words\",\"that should be considered stop words. The first three shingle\",\"s made from a\",\"stop word and the next two following are:\"],\"page\":7},{\"texts\":[\"62\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"A spokesperson for\",\"for the Sudzo\",\"the Sudzo Corporation\",\"There are nine shingles from the sentence, but none from the “\",\"ad.”\",\"2\",\"3.2.5  Exercises for Section 3.2\",\"Exercise 3.2.1:\",\"What are the first ten 3-shingles in the first sentence of Sec-\",\"tion 3.2?\",\"Exercise 3.2.2:\",\"If we use the stop-word-based shingles of Section 3.2.4, and\",\"we take the stop words to be all the words of three or fewer lett\",\"ers, then what\",\"are the shingles in the first sentence of Section 3.2?\",\"Exercise 3.2.3:\",\"What is the largest number of\",\"k\",\"-shingles a document of\",\"n\",\"bytes can have? You may assume that the size of the alphabet is\",\"large enough\",\"that the number of possible strings of length\",\"k\",\"is at least as\",\"n\",\".\",\"3.3  Similarity-Preserving Summaries of Sets\",\"Sets of shingles are large. Even if we hash them to four bytes e\",\"ach, the space\",\"needed to store a set is still roughly four times the space tak\",\"en by the document.\",\"If we have millions of documents, it may well not be possible t\",\"o store all the\",\"shingle-sets in main memory.\",\"2\",\"Our goal in this section is to replace large sets by much small\",\"er represen-\",\"tations called “signatures.” The important property we nee\",\"d for signatures is\",\"that we can compare the signatures of two sets and estimate th\",\"e Jaccard sim-\",\"ilarity of the underlying sets from the signatures alone. It\",\"is not possible that\",\"the signatures give the exact similarity of the sets they rep\",\"resent, but the esti-\",\"mates they provide are close, and the larger the signatures t\",\"he more accurate\",\"the estimates. For example, if we replace the 200,000-byte h\",\"ashed-shingle sets\",\"that derive from 50,000-byte documents by signatures of 100\",\"0 bytes, we can\",\"usually get within a few percent.\",\"3.3.1  Matrix Representation of Sets\",\"Before explaining how it is possible to construct small sign\",\"atures from large\",\"sets, it is helpful to visualize a collection of sets as their\",\"characteristic matrix\",\".\",\"The columns of the matrix correspond to the sets, and the rows\",\"correspond to\",\"elements of the universal set from which elements of the sets\",\"are drawn. There\",\"is a 1 in row\",\"r\",\"and column\",\"c\",\"if the element for row\",\"r\",\"is a member of the set for\",\"column\",\"c\",\". Otherwise the value in position (\",\"r, c\",\") is 0.\",\"2\",\"There is another serious concern: even if the sets fit in main m\",\"emory, the number of pairs\",\"may be too great for us to evaluate the similarity of each pair\",\". We take up the solution to\",\"this problem in Section 3.4.\"],\"page\":8},{\"texts\":[\"3.3.  SIMILARITY-PRESERVING SUMMARIES OF SETS\",\"63\",\"Element\",\"S\",\"1\",\"S\",\"2\",\"S\",\"3\",\"S\",\"4\",\"a\",\"1\",\"0\",\"0\",\"1\",\"b\",\"0\",\"0\",\"1\",\"0\",\"c\",\"0\",\"1\",\"0\",\"1\",\"d\",\"1\",\"0\",\"1\",\"1\",\"e\",\"0\",\"0\",\"1\",\"0\",\"Figure 3.2: A matrix representing four sets\",\"Example 3.6:\",\"In Fig. 3.2 is an example of a matrix representing sets chosen\",\"from the universal set\",\"{\",\"a, b, c, d, e\",\"}\",\". Here,\",\"S\",\"1\",\"=\",\"{\",\"a, d\",\"}\",\",\",\"S\",\"2\",\"=\",\"{\",\"c\",\"}\",\",\",\"S\",\"3\",\"=\",\"{\",\"b, d, e\",\"}\",\",\",\"and\",\"S\",\"4\",\"=\",\"{\",\"a, c, d\",\"}\",\". The top row and leftmost columns are not part of the matrix,\",\"but are present only to remind us what the rows and columns rep\",\"resent.\",\"2\",\"It is important to remember that the characteristic matrix i\",\"s unlikely to be\",\"the way the data is stored, but it is useful as a way to visualiz\",\"e the data. For one\",\"reason not to store data as a matrix, these matrices are almos\",\"t always\",\"sparse\",\"(they have many more 0’s than 1’s) in practice. It saves space\",\"to represent a\",\"sparse matrix of 0’s and 1’s by the positions in which the 1’s a\",\"ppear. For another\",\"reason, the data is usually stored in some other format for ot\",\"her purposes.\",\"As an example, if rows are products, and columns are customer\",\"s, represented\",\"by the set of products they bought, then this data would reall\",\"y appear in a\",\"database table of purchases.  A tuple in this table would list\",\"the item, the\",\"purchaser, and probably other details about the purchase, s\",\"uch as the date and\",\"the credit card used.\",\"3.3.2  Minhashing\",\"The signatures we desire to construct for sets are composed o\",\"f the results of a\",\"large number of calculations, say several hundred, each of w\",\"hich is a “minhash”\",\"of the characteristic matrix. In this section, we shall lear\",\"n how a minhash is\",\"computed in principle, and in later sections we shall see how\",\"a good approxi-\",\"mation to the minhash is computed in practice.\",\"To\",\"minhash\",\"a set represented by a column of the characteristic matrix, p\",\"ick\",\"a permutation of the rows. The minhash value of any column is t\",\"he number of\",\"the first row, in the permuted order, in which the column has a 1\",\".\",\"Example 3.7:\",\"Let us suppose we pick the order of rows\",\"beadc\",\"for the matrix\",\"of Fig. 3.2. This permutation defines a minhash function\",\"h\",\"that maps sets to\",\"rows. Let us compute the minhash value of set\",\"S\",\"1\",\"according to\",\"h\",\". The first\",\"column, which is the column for set\",\"S\",\"1\",\", has 0 in row\",\"b\",\", so we proceed to row\",\"e\",\",\",\"the second in the permuted order. There is again a 0 in the colu\",\"mn for\",\"S\",\"1\",\", so\",\"we proceed to row\",\"a\",\", where we find a 1. Thus.\",\"h\",\"(\",\"S\",\"1\",\") =\",\"a\",\".\",\"Although it is not physically possible to permute very large\",\"characteristic\",\"matrices, the minhash function\",\"h\",\"implicitly reorders the rows of the matrix of\"],\"page\":9},{\"texts\":[\"64\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"Element\",\"S\",\"1\",\"S\",\"2\",\"S\",\"3\",\"S\",\"4\",\"b\",\"0\",\"0\",\"1\",\"0\",\"e\",\"0\",\"0\",\"1\",\"0\",\"a\",\"1\",\"0\",\"0\",\"1\",\"d\",\"1\",\"0\",\"1\",\"1\",\"c\",\"0\",\"1\",\"0\",\"1\",\"Figure 3.3: A permutation of the rows of Fig. 3.2\",\"Fig. 3.2 so it becomes the matrix of Fig. 3.3. In this matrix, w\",\"e can read off\",\"the values of\",\"h\",\"by scanning from the top until we come to a 1. Thus, we see\",\"that\",\"h\",\"(\",\"S\",\"2\",\") =\",\"c\",\",\",\"h\",\"(\",\"S\",\"3\",\") =\",\"b\",\", and\",\"h\",\"(\",\"S\",\"4\",\") =\",\"a\",\".\",\"2\",\"3.3.3  Minhashing and Jaccard Similarity\",\"There is a remarkable connection between minhashing and Jac\",\"card similarity\",\"of the sets that are minhashed.\",\"•\",\"The probability that the minhash function for a random permu\",\"tation of\",\"rows produces the same value for two sets equals the Jaccard s\",\"imilarity\",\"of those sets.\",\"To see why, we need to picture the columns for those two sets. I\",\"f we restrict\",\"ourselves to the columns for sets\",\"S\",\"1\",\"and\",\"S\",\"2\",\", then rows can be divided into three\",\"classes:\",\"1. Type\",\"X\",\"rows have 1 in both columns.\",\"2. Type\",\"Y\",\"rows have 1 in one of the columns and 0 in the other.\",\"3. Type\",\"Z\",\"rows have 0 in both columns.\",\"Since the matrix is sparse, most rows are of type\",\"Z\",\". However, it is the ratio\",\"of the numbers of type\",\"X\",\"and type\",\"Y\",\"rows that determine both\",\"SIM\",\"(\",\"S\",\"1\",\", S\",\"2\",\")\",\"and the probability that\",\"h\",\"(\",\"S\",\"1\",\") =\",\"h\",\"(\",\"S\",\"2\",\"). Let there be\",\"x\",\"rows of type\",\"X\",\"and\",\"y\",\"rows of type\",\"Y\",\". Then\",\"SIM\",\"(\",\"S\",\"1\",\", S\",\"2\",\") =\",\"x/\",\"(\",\"x\",\"+\",\"y\",\"). The reason is that\",\"x\",\"is the size\",\"of\",\"S\",\"1\",\"∩\",\"S\",\"2\",\"and\",\"x\",\"+\",\"y\",\"is the size of\",\"S\",\"1\",\"∪\",\"S\",\"2\",\".\",\"Now, consider the probability that\",\"h\",\"(\",\"S\",\"1\",\") =\",\"h\",\"(\",\"S\",\"2\",\"). If we imagine the rows\",\"permuted randomly, and we proceed from the top, the probabil\",\"ity that we shall\",\"meet a type\",\"X\",\"row before we meet a type\",\"Y\",\"row is\",\"x/\",\"(\",\"x\",\"+\",\"y\",\").  But if the\",\"first row from the top other than type\",\"Z\",\"rows is a type\",\"X\",\"row, then surely\",\"h\",\"(\",\"S\",\"1\",\") =\",\"h\",\"(\",\"S\",\"2\",\"). On the other hand, if the first row other than a type\",\"Z\",\"row\",\"that we meet is a type\",\"Y\",\"row, then the set with a 1 gets that row as its minhash\",\"value. However the set with a 0 in that row surely gets some row\",\"further down\",\"the permuted list. Thus, we know\",\"h\",\"(\",\"S\",\"1\",\")\",\"6\",\"=\",\"h\",\"(\",\"S\",\"2\",\") if we first meet a type\",\"Y\",\"row.\",\"We conclude the probability that\",\"h\",\"(\",\"S\",\"1\",\") =\",\"h\",\"(\",\"S\",\"2\",\") is\",\"x/\",\"(\",\"x\",\"+\",\"y\",\"), which is also the\",\"Jaccard similarity of\",\"S\",\"1\",\"and\",\"S\",\"2\",\".\"],\"page\":10},{\"texts\":[\"3.3.  SIMILARITY-PRESERVING SUMMARIES OF SETS\",\"65\",\"3.3.4  Minhash Signatures\",\"Again think of a collection of sets represented by their char\",\"acteristic matrix\",\"M\",\".\",\"To represent sets, we pick at random some number\",\"n\",\"of permutations of the\",\"rows of\",\"M\",\". Perhaps 100 permutations or several hundred permutations\",\"will do.\",\"Call the minhash functions determined by these permutation\",\"s\",\"h\",\"1\",\", h\",\"2\",\", . . . , h\",\"n\",\".\",\"From the column representing set\",\"S\",\", construct the\",\"minhash signature\",\"for\",\"S\",\", the\",\"vector [\",\"h\",\"1\",\"(\",\"S\",\")\",\", h\",\"2\",\"(\",\"S\",\")\",\", . . . , h\",\"n\",\"(\",\"S\",\")]. We normally represent this list of hash-values\",\"as a column. Thus, we can form from matrix\",\"M\",\"a\",\"signature matrix\",\", in which\",\"the\",\"i\",\"th column of\",\"M\",\"is replaced by the minhash signature for (the set of) the\",\"i\",\"th column.\",\"Note that the signature matrix has the same number of columns\",\"as\",\"M\",\"but\",\"only\",\"n\",\"rows. Even if\",\"M\",\"is not represented explicitly, but in some compressed\",\"form suitable for a sparse matrix (e.g., by the locations of i\",\"ts 1’s), it is normal\",\"for the signature matrix to be much smaller than\",\"M\",\".\",\"3.3.5  Computing Minhash Signatures\",\"It is not feasible to permute a large characteristic matrix e\",\"xplicitly. Even picking\",\"a random permutation of millions or billions of rows is time-\",\"consuming, and\",\"the necessary sorting of the rows would take even more time. T\",\"hus, permuted\",\"matrices like that suggested by Fig. 3.3, while conceptuall\",\"y appealing, are not\",\"implementable.\",\"Fortunately, it is possible to simulate the effect of a random\",\"permutation by\",\"a random hash function that maps row numbers to as many bucket\",\"s as there\",\"are rows. A hash function that maps integers 0\",\",\",\"1\",\", . . . , k\",\"−\",\"1 to bucket numbers\",\"0 through\",\"k\",\"−\",\"1 typically will map some pairs of integers to the same bucket\",\"and\",\"leave other buckets unfilled. However, the difference is unim\",\"portant as long as\",\"k\",\"is large and there are not too many collisions. We can maintai\",\"n the fiction\",\"that our hash function\",\"h\",\"“permutes” row\",\"r\",\"to position\",\"h\",\"(\",\"r\",\") in the permuted\",\"order.\",\"Thus, instead of picking\",\"n\",\"random permutations of rows, we pick\",\"n\",\"randomly\",\"chosen hash functions\",\"h\",\"1\",\", h\",\"2\",\", . . . , h\",\"n\",\"on the rows. We construct the signature\",\"matrix by considering each row in their given order. Let\",\"SIG\",\"(\",\"i, c\",\") be the element\",\"of the signature matrix for the\",\"i\",\"th hash function and column\",\"c\",\". Initially, set\",\"SIG\",\"(\",\"i, c\",\") to\",\"∞\",\"for all\",\"i\",\"and\",\"c\",\". We handle row\",\"r\",\"by doing the following:\",\"1. Compute\",\"h\",\"1\",\"(\",\"r\",\")\",\", h\",\"2\",\"(\",\"r\",\")\",\", . . . , h\",\"n\",\"(\",\"r\",\").\",\"2. For each column\",\"c\",\"do the following:\",\"(a) If\",\"c\",\"has 0 in row\",\"r\",\", do nothing.\",\"(b) However, if\",\"c\",\"has 1 in row\",\"r\",\", then for each\",\"i\",\"= 1\",\",\",\"2\",\", . . . , n\",\"set\",\"SIG\",\"(\",\"i, c\",\")\",\"to the smaller of the current value of\",\"SIG\",\"(\",\"i, c\",\") and\",\"h\",\"i\",\"(\",\"r\",\").\"],\"page\":11},{\"texts\":[\"66\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"Row\",\"S\",\"1\",\"S\",\"2\",\"S\",\"3\",\"S\",\"4\",\"x\",\"+ 1 mod 5\",\"3\",\"x\",\"+ 1 mod 5\",\"0\",\"1\",\"0\",\"0\",\"1\",\"1\",\"1\",\"1\",\"0\",\"0\",\"1\",\"0\",\"2\",\"4\",\"2\",\"0\",\"1\",\"0\",\"1\",\"3\",\"2\",\"3\",\"1\",\"0\",\"1\",\"1\",\"4\",\"0\",\"4\",\"0\",\"0\",\"1\",\"0\",\"0\",\"3\",\"Figure 3.4: Hash functions computed for the matrix of Fig. 3.\",\"2\",\"Example 3.8:\",\"Let us reconsider the characteristic matrix of Fig. 3.2, whi\",\"ch\",\"we reproduce with some additional data as Fig. 3.4.  We have re\",\"placed the\",\"letters naming the rows by integers 0 through 4. We have also c\",\"hosen two hash\",\"functions:\",\"h\",\"1\",\"(\",\"x\",\") =\",\"x\",\"+1 mod 5 and\",\"h\",\"2\",\"(\",\"x\",\") = 3\",\"x\",\"+1 mod 5. The values of these\",\"two functions applied to the row numbers are given in the last\",\"two columns of\",\"Fig. 3.4. Notice that these simple hash functions are true pe\",\"rmutations of the\",\"rows, but a true permutation is only possible because the num\",\"ber of rows, 5, is\",\"a prime. In general, there will be collisions, where two rows\",\"get the same hash\",\"value.\",\"Now, let us simulate the algorithm for computing the signatu\",\"re matrix.\",\"Initially, this matrix consists of all\",\"∞\",\"’s:\",\"S\",\"1\",\"S\",\"2\",\"S\",\"3\",\"S\",\"4\",\"h\",\"1\",\"∞\",\"∞\",\"∞\",\"∞\",\"h\",\"2\",\"∞\",\"∞\",\"∞\",\"∞\",\"First, we consider row 0 of Fig. 3.4. We see that the values of\",\"h\",\"1\",\"(0) and\",\"h\",\"2\",\"(0) are both 1. The row numbered 0 has 1’s in the columns for set\",\"s\",\"S\",\"1\",\"and\",\"S\",\"4\",\", so only these columns of the signature matrix can change. As\",\"1 is less than\",\"∞\",\", we do in fact change both values in the columns for\",\"S\",\"1\",\"and\",\"S\",\"4\",\". The current\",\"estimate of the signature matrix is thus:\",\"S\",\"1\",\"S\",\"2\",\"S\",\"3\",\"S\",\"4\",\"h\",\"1\",\"1\",\"∞\",\"∞\",\"1\",\"h\",\"2\",\"1\",\"∞\",\"∞\",\"1\",\"Now, we move to the row numbered 1 in Fig. 3.4. This row has 1 onl\",\"y in\",\"S\",\"3\",\", and its hash values are\",\"h\",\"1\",\"(1) = 2 and\",\"h\",\"2\",\"(1) = 4. Thus, we set\",\"SIG\",\"(1\",\",\",\"3) to 2\",\"and\",\"SIG\",\"(2\",\",\",\"3) to 4. All other signature entries remain as they are becaus\",\"e their\",\"columns have 0 in the row numbered 1. The new signature matrix\",\":\",\"S\",\"1\",\"S\",\"2\",\"S\",\"3\",\"S\",\"4\",\"h\",\"1\",\"1\",\"∞\",\"2\",\"1\",\"h\",\"2\",\"1\",\"∞\",\"4\",\"1\",\"The row of Fig. 3.4 numbered 2 has 1’s in the columns for\",\"S\",\"2\",\"and\",\"S\",\"4\",\", and\",\"its hash values are\",\"h\",\"1\",\"(2) = 3 and\",\"h\",\"2\",\"(2) = 2. We could change the values in the\"],\"page\":12},{\"texts\":[\"3.3.  SIMILARITY-PRESERVING SUMMARIES OF SETS\",\"67\",\"signature for\",\"S\",\"4\",\", but the values in this column of the signature matrix, [1\",\",\",\"1], are\",\"each less than the corresponding hash values [3\",\",\",\"2]. However, since the column\",\"for\",\"S\",\"2\",\"still has\",\"∞\",\"’s, we replace it by [3\",\",\",\"2], resulting in:\",\"S\",\"1\",\"S\",\"2\",\"S\",\"3\",\"S\",\"4\",\"h\",\"1\",\"1\",\"3\",\"2\",\"1\",\"h\",\"2\",\"1\",\"2\",\"4\",\"1\",\"Next comes the row numbered 3 in Fig. 3.4. Here, all columns bu\",\"t\",\"S\",\"2\",\"have\",\"1, and the hash values are\",\"h\",\"1\",\"(3) = 4 and\",\"h\",\"2\",\"(3) = 0. The value 4 for\",\"h\",\"1\",\"exceeds\",\"what is already in the signature matrix for all the columns, s\",\"o we shall not\",\"change any values in the first row of the signature matrix. How\",\"ever, the value\",\"0 for\",\"h\",\"2\",\"is less than what is already present, so we lower\",\"SIG\",\"(2\",\",\",\"1),\",\"SIG\",\"(2\",\",\",\"3) and\",\"SIG\",\"(2\",\",\",\"4) to 0. Note that we cannot lower\",\"SIG\",\"(2\",\",\",\"2) because the column for\",\"S\",\"2\",\"in\",\"Fig. 3.4 has 0 in the row we are currently considering. The res\",\"ulting signature\",\"matrix:\",\"S\",\"1\",\"S\",\"2\",\"S\",\"3\",\"S\",\"4\",\"h\",\"1\",\"1\",\"3\",\"2\",\"1\",\"h\",\"2\",\"0\",\"2\",\"0\",\"0\",\"Finally, consider the row of Fig. 3.4 numbered 4.\",\"h\",\"1\",\"(4) = 0 and\",\"h\",\"2\",\"(4) = 3.\",\"Since row 4 has 1 only in the column for\",\"S\",\"3\",\", we only compare the current\",\"signature column for that set, [2\",\",\",\"0] with the hash values [0\",\",\",\"3]. Since 0\",\"\u003c\",\"2, we\",\"change\",\"SIG\",\"(1\",\",\",\"3) to 0, but since 3\",\"\u003e\",\"0 we do not change\",\"SIG\",\"(2\",\",\",\"3). The final\",\"signature matrix is:\",\"S\",\"1\",\"S\",\"2\",\"S\",\"3\",\"S\",\"4\",\"h\",\"1\",\"1\",\"3\",\"0\",\"1\",\"h\",\"2\",\"0\",\"2\",\"0\",\"0\",\"We can estimate the Jaccard similarities of the underlying s\",\"ets from this\",\"signature matrix. Notice that columns 1 and 4 are identical,\",\"so we guess that\",\"SIM\",\"(\",\"S\",\"1\",\", S\",\"4\",\") = 1\",\".\",\"0. If we look at Fig. 3.4, we see that the true Jaccard similari\",\"ty\",\"of\",\"S\",\"1\",\"and\",\"S\",\"4\",\"is 2/3. Remember that the fraction of rows that agree in the\",\"signature matrix is only an estimate of the true Jaccard simi\",\"larity, and this\",\"example is much too small for the law of large numbers to assur\",\"e that the\",\"estimates are close. For additional examples, the signatur\",\"e columns for\",\"S\",\"1\",\"and\",\"S\",\"3\",\"agree in half the rows (true similarity 1/4), while the signa\",\"tures of\",\"S\",\"1\",\"and\",\"S\",\"2\",\"estimate 0 as their Jaccard similarity (the correct value).\",\"2\",\"3.3.6  Exercises for Section 3.3\",\"Exercise 3.3.1:\",\"Verify the theorem from Section 3.3.3, which relates the Jac\",\"-\",\"card similarity to the probability of minhashing to equal va\",\"lues, for the partic-\",\"ular case of Fig. 3.2.\"],\"page\":13},{\"texts\":[\"68\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"(a) Compute the Jaccard similarity of each of the pairs of col\",\"umns in Fig. 3.2.\",\"!\",\"(b) Compute, for each pair of columns of that figure, the fract\",\"ion of the 120\",\"permutations of the rows that make the two columns hash to the\",\"same\",\"value.\",\"Exercise 3.3.2:\",\"Using the data from Fig. 3.4, add to the signatures of the\",\"columns the values of the following hash functions:\",\"(a)\",\"h\",\"3\",\"(\",\"x\",\") = 2\",\"x\",\"+ 4.\",\"(b)\",\"h\",\"4\",\"(\",\"x\",\") = 3\",\"x\",\"−\",\"1.\",\"Element\",\"S\",\"1\",\"S\",\"2\",\"S\",\"3\",\"S\",\"4\",\"0\",\"0\",\"1\",\"0\",\"1\",\"1\",\"0\",\"1\",\"0\",\"0\",\"2\",\"1\",\"0\",\"0\",\"1\",\"3\",\"0\",\"0\",\"1\",\"0\",\"4\",\"0\",\"0\",\"1\",\"1\",\"5\",\"1\",\"0\",\"0\",\"0\",\"Figure 3.5: Matrix for Exercise 3.3.3\",\"Exercise 3.3.3:\",\"In Fig. 3.5 is a matrix with six rows.\",\"(a) Compute the minhash signature for each column if we use th\",\"e following\",\"three hash functions:\",\"h\",\"1\",\"(\",\"x\",\") = 2\",\"x\",\"+ 1 mod 6;\",\"h\",\"2\",\"(\",\"x\",\") = 3\",\"x\",\"+ 2 mod 6;\",\"h\",\"3\",\"(\",\"x\",\") = 5\",\"x\",\"+ 2 mod 6.\",\"(b) Which of these hash functions are true permutations?\",\"(c) How close are the estimated Jaccard similarities for the\",\"six pairs of columns\",\"to the true Jaccard similarities?\",\"! Exercise 3.3.4:\",\"Now that we know Jaccard similarity is related to the proba-\",\"bility that two sets minhash to the same value, reconsider Ex\",\"ercise 3.1.3. Can\",\"you use this relationship to simplify the problem of computi\",\"ng the expected\",\"Jaccard similarity of randomly chosen sets?\",\"! Exercise 3.3.5:\",\"Prove that if the Jaccard similarity of two columns is 0, then\",\"then minhashing always gives a correct estimate of the Jacca\",\"rd similarity.\",\"!! Exercise 3.3.6:\",\"One might expect that we could estimate the Jaccard simi-\",\"larity of columns without using all possible permutations o\",\"f rows. For example,\",\"we could only allow cyclic permutations; i.e., start at a ran\",\"domly chosen row\",\"r\",\", which becomes the first in the order, followed by rows\",\"r\",\"+ 1,\",\"r\",\"+ 2, and so\"],\"page\":14},{\"texts\":[\"3.4.  LOCALITY-SENSITIVE HASHING FOR DOCUMENTS\",\"69\",\"on, down to the last row, and then continuing with the first row\",\", second row,\",\"and so on, down to row\",\"r\",\"−\",\"1. There are only\",\"n\",\"such permutations if there are\",\"n\",\"rows. However, these permutations are not sufficient to estim\",\"ate the Jaccard\",\"similarity correctly. Give an example of a two-column matri\",\"x where averaging\",\"over all the cyclic permutations does not give the Jaccard si\",\"milarity.\",\"! Exercise 3.3.7:\",\"Suppose we want to use a map-reduce framework to compute\",\"minhash signatures. If the matrix is stored in chunks that co\",\"rrespond to some\",\"columns, then it is quite easy to exploit parallelism. Each M\",\"ap task gets some\",\"of the columns and all the hash functions, and computes the mi\",\"nhash signatures\",\"of its given columns. However, suppose the matrix were chunk\",\"ed by rows, so\",\"that a Map task is given the hash functions and a set of rows to w\",\"ork on. Design\",\"Map and Reduce functions to exploit map-reduce with data in t\",\"his form.\",\"3.4  Locality-Sensitive Hashing for Documents\",\"Even though we can use minhashing to compress large document\",\"s into small\",\"signatures and preserve the expected similarity of any pair\",\"of documents, it\",\"still may be impossible to find the pairs with greatest simila\",\"rity efficiently. The\",\"reason is that the number of pairs of documents may be too larg\",\"e, even if there\",\"are not too many documents.\",\"Example 3.9:\",\"Suppose we have a million documents, and we use signatures\",\"of length 250. Then we use 1000 bytes per document for the sign\",\"atures, and\",\"the entire data fits in a gigabyte – less than a typical main mem\",\"ory of a laptop.\",\"However, there are\",\"(\",\"1\",\",\",\"000\",\",\",\"000\",\"2\",\")\",\"or half a trillion pairs of documents. If it takes a\",\"microsecond to compute the similarity of two signatures, th\",\"en it takes almost\",\"six days to compute all the similarities on that laptop.\",\"2\",\"If our goal is to compute the similarity of every pair, there i\",\"s nothing we\",\"can do to reduce the work, although parallelism can reduce th\",\"e elapsed time.\",\"However, often we want only the most similar pairs or all pair\",\"s that are above\",\"some lower bound in similarity. If so, then we need to focus ou\",\"r attention only\",\"on pairs that are likely to be similar, without investigatin\",\"g every pair. There is\",\"a general theory of how to provide such focus, called\",\"locality-sensitive hashing\",\"(LSH) or\",\"near-neighbor search\",\".  In this section we shall consider a specific form\",\"of LSH, designed for the particular problem we have been stud\",\"ying: documents,\",\"represented by shingle-sets, then minhashed to short signa\",\"tures. In Section 3.6\",\"we present the general theory of locality-sensitive hashin\",\"g and a number of\",\"applications and related techniques.\",\"3.4.1  LSH for Minhash Signatures\",\"One general approach to LSH is to “hash” items several times,\",\"in such a way that\",\"similar items are more likely to be hashed to the same bucket t\",\"han dissimilar\"],\"page\":15},{\"texts\":[\"70\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"items are. We then consider any pair that hashed to the same bu\",\"cket for any\",\"of the hashings to be a\",\"candidate pair\",\". We check only the candidate pairs for\",\"similarity. The hope is that most of the dissimilar pairs wil\",\"l never hash to the\",\"same bucket, and therefore will never be checked. Those diss\",\"imilar pairs that\",\"do hash to the same bucket are\",\"false positives\",\"; we hope these will be only a\",\"small fraction of all pairs.. We also hope that most of the tru\",\"ly similar pairs\",\"will hash to the same bucket under at least one of the hash func\",\"tions. Those\",\"that do not are\",\"false negatives\",\"; we hope these will be only a small fraction of\",\"the truly similar pairs.\",\"If we have minhash signatures for the items, an effective way t\",\"o choose the\",\"hashings is to divide the signature matrix into\",\"b\",\"bands consisting  of\",\"r\",\"rows\",\"each. For each band, there is a hash function that takes vecto\",\"rs of\",\"r\",\"integers\",\"(the portion of one column within that band) and hashes them t\",\"o some large\",\"number of buckets. We can use the same hash function for all th\",\"e bands, but\",\"we use a separate bucket array for each band, so columns with t\",\"he same vector\",\"in different bands will not hash to the same bucket.\",\"1 0 0 0 2\",\"3 2 1 2 2\",\"0 1 3 1 1\",\". . .\",\". . .\",\"band 1\",\"band 2\",\"band 3\",\"band 4\",\"Figure 3.6: Dividing a signature matrix into four bands of th\",\"ree rows per band\",\"Example 3.10:\",\"Figure 3.6 shows part of a signature matrix of 12 rows divided\",\"into four bands of three rows each. The second and fourth of th\",\"e explicitly\",\"shown columns each have the column vector [0\",\",\",\"2\",\",\",\"1] in the first band, so they\",\"will definitely hash to the same bucket in the hashing for the fi\",\"rst band. Thus,\",\"regardless of what those columns look like in the other three\",\"bands, this pair\",\"of columns will be a candidate pair. It is possible that other\",\"columns, such as\",\"the first two shown explicitly, will also hash to the same buck\",\"et according to\",\"the hashing of the first band. However, since their column vec\",\"tors are different,\",\"[1\",\",\",\"3\",\",\",\"0] and [0\",\",\",\"2\",\",\",\"1], and there are many buckets for each hashing, we expect the\",\"chances of an accidental collision to be very small. We shall\",\"normally assume\",\"that two vectors hash to the same bucket if and only if they are\",\"identical.\",\"Two columns that do not agree in band 1 have three other chance\",\"s to become\",\"a candidate pair; they might be identical in any one of these o\",\"ther bands.\"],\"page\":16},{\"texts\":[\"3.4.  LOCALITY-SENSITIVE HASHING FOR DOCUMENTS\",\"71\",\"However, observe that the more similar two columns are, the m\",\"ore likely it is\",\"that they will be identical in some band. Thus, intuitively t\",\"he banding strategy\",\"makes similar columns much more likely to be candidate pairs\",\"than dissimilar\",\"pairs.\",\"2\",\"3.4.2  Analysis of the Banding Technique\",\"Suppose we use\",\"b\",\"bands of\",\"r\",\"rows each, and suppose that a particular pair of\",\"documents have Jaccard similarity\",\"s\",\". Recall from Section 3.3.3 that the prob-\",\"ability the minhash signatures for these documents agree in\",\"any one particular\",\"row of the signature matrix is\",\"s\",\". We can calculate the probability that these\",\"documents (or rather their signatures) become a candidate p\",\"air as follows:\",\"1. The probability that the signatures agree in all rows of on\",\"e particular\",\"band is\",\"s\",\"r\",\".\",\"2. The probability that the signatures do not agree in at leas\",\"t one row of a\",\"particular band is 1\",\"−\",\"s\",\"r\",\".\",\"3. The probability that the signatures do not agree in all row\",\"s of any of the\",\"bands is (1\",\"−\",\"s\",\"r\",\")\",\"b\",\".\",\"4. The probability that the signatures agree in all the rows o\",\"f at least one\",\"band, and therefore become a candidate pair, is 1\",\"−\",\"(1\",\"−\",\"s\",\"r\",\")\",\"b\",\".\",\"It may not be obvious, but regardless of the chosen constants\",\"b\",\"and\",\"r\",\", this\",\"function has the form of an\",\"S-curve\",\", as suggested in Fig. 3.7. The\",\"threshold\",\", that\",\"is, the value of similarity\",\"s\",\"at which the rise becomes steepest, is a function of\",\"b\",\"and\",\"r\",\". An approximation to the threshold is (1\",\"/b\",\")\",\"1\",\"/r\",\". For example, if\",\"b\",\"= 16\",\"and\",\"r\",\"= 4, then the threshold is approximately 1/2, since the 4th ro\",\"ot of 16 is\",\"2.\",\"Example 3.11:\",\"Let us consider the case\",\"b\",\"= 20 and\",\"r\",\"= 5. That is, we suppose\",\"we have signatures of length 100, divided into twenty bands o\",\"f five rows each.\",\"Figure 3.8 tabulates some of the values of the function 1\",\"−\",\"(1\",\"−\",\"s\",\"5\",\")\",\"20\",\". Notice\",\"that the threshold, the value of\",\"s\",\"at which the curve has risen halfway, is just\",\"slightly more than 0.5. Also notice that the curve is not exac\",\"tly the ideal step\",\"function that jumps from 0 to 1 at the threshold, but the slope\",\"of the curve\",\"in the middle is significant. For example, it rises by more tha\",\"n 0.6 going from\",\"s\",\"= 0\",\".\",\"4 to\",\"s\",\"= 0\",\".\",\"6, so the slope in the middle is greater than 3.\",\"For example, at\",\"s\",\"= 0\",\".\",\"8, 1\",\"−\",\"(0\",\".\",\"8)\",\"5\",\"is about 0.672. If you raise this number\",\"to the 20th power, you get about 0.00035. Subtracting this fr\",\"action from 1\",\"yields 0.99965. That is, if we consider two documents with 80\",\"% similarity, then\",\"in any one band, they have only about a 33% chance of agreeing i\",\"n all five rows\",\"and thus becoming a candidate pair. However, there are 20 ban\",\"ds and thus 20\",\"chances to become a candidate. Only roughly one in 3000 pairs\",\"that are as high\",\"as 80% similar will fail to become a candidate pair and thus be\",\"a false negative.\",\"2\"],\"page\":17},{\"texts\":[\"72\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"0\",\"1\",\"of documents\",\"Jaccard similarity\",\"Probability\",\"of becoming\",\"a candidate\",\"Figure 3.7: The S-curve\",\"s\",\"1\",\"−\",\"(1\",\"−\",\"s\",\"r\",\")\",\"b\",\".2  .006\",\".3  .047\",\".4  .186\",\".5  .470\",\".6  .802\",\".7  .975\",\".8  .9996\",\"Figure 3.8: Values of the S-curve for\",\"b\",\"= 20 and\",\"r\",\"= 5\",\"3.4.3  Combining the Techniques\",\"We can now give an approach to finding the set of candidate pair\",\"s for similar\",\"documents and then discovering the truly similar documents\",\"among them. It\",\"must be emphasized that this approach can produce false nega\",\"tives – pairs of\",\"similar documents that are not identified as such because the\",\"y never become\",\"a candidate pair. There will also be false positives – candid\",\"ate pairs that are\",\"evaluated, but are found not to be sufficiently similar.\",\"1. Pick a value of\",\"k\",\"and construct from each document the set of\",\"k\",\"-shingles.\",\"Optionally, hash the\",\"k\",\"-shingles to shorter bucket numbers.\",\"2. Sort the document-shingle pairs to order them by shingle.\",\"3. Pick a length\",\"n\",\"for the minhash signatures.  Feed the sorted list to the\",\"algorithm of Section 3.3.5 to compute the minhash signature\",\"s for all the\"],\"page\":18},{\"texts\":[\"3.4.  LOCALITY-SENSITIVE HASHING FOR DOCUMENTS\",\"73\",\"documents.\",\"4. Choose a threshold\",\"t\",\"that defines how similar documents have to be in\",\"order for them to be regarded as a desired “similar pair.” Pic\",\"k a number\",\"of bands\",\"b\",\"and a number of rows\",\"r\",\"such that\",\"br\",\"=\",\"n\",\", and the threshold\",\"t\",\"is approximately (1\",\"/b\",\")\",\"1\",\"/r\",\". If avoidance of false negatives is important,\",\"you may wish to select\",\"b\",\"and\",\"r\",\"to produce a threshold lower than\",\"t\",\"; if\",\"speed is important and you wish to limit false positives, sel\",\"ect\",\"b\",\"and\",\"r\",\"to\",\"produce a higher threshold.\",\"5. Construct candidate pairs by applying the LSH technique o\",\"f Section 3.4.1.\",\"6. Examine each candidate pair’s signatures and determine w\",\"hether the frac-\",\"tion of components in which they agree is at least\",\"t\",\".\",\"7. Optionally, if the signatures are sufficiently similar, go\",\"to the documents\",\"themselves and check that they are truly similar, rather tha\",\"n documents\",\"that, by luck, had similar signatures.\",\"3.4.4  Exercises for Section 3.4\",\"Exercise 3.4.1:\",\"Evaluate the S-curve 1\",\"−\",\"(1\",\"−\",\"s\",\"r\",\")\",\"b\",\"for\",\"s\",\"= 0\",\".\",\"1\",\",\",\"0\",\".\",\"2\",\", . . . ,\",\"0\",\".\",\"9, for\",\"the following values of\",\"r\",\"and\",\"b\",\":\",\"•\",\"r\",\"= 3 and\",\"b\",\"= 10.\",\"•\",\"r\",\"= 6 and\",\"b\",\"= 20.\",\"•\",\"r\",\"= 5 and\",\"b\",\"= 50.\",\"! Exercise 3.4.2:\",\"For each of the (\",\"r, b\",\") pairs in Exercise 3.4.1, compute the\",\"threshold, that is, the value of\",\"s\",\"for which the value of 1\",\"−\",\"(1\",\"−\",\"s\",\"r\",\")\",\"b\",\"is exactly 1/2.\",\"How does this value compare with the estimate of (1\",\"/b\",\")\",\"1\",\"/r\",\"that was suggested\",\"in Section 3.4.2?\",\"! Exercise 3.4.3:\",\"Use the techniques explained in Section 1.3.5 to approximat\",\"e\",\"the S-curve 1\",\"−\",\"(1\",\"−\",\"s\",\"r\",\")\",\"b\",\"when\",\"s\",\"r\",\"is very small.\",\"! Exercise 3.4.4:\",\"Suppose we wish to implement LSH by map-reduce. Specifi-\",\"cally, assume chunks of the signature matrix consist of colu\",\"mns, and elements\",\"are key-value pairs where the key is the column number and the\",\"value is the\",\"signature itself (i.e., a vector of values).\",\"(a) Show how to produce the buckets for all the bands as output\",\"of a single\",\"map-reduce process.\",\"Hint\",\": Remember that a Map function can produce\",\"several key-value pairs from a single element.\",\"(b) Show how another map-reduce process can convert the outp\",\"ut of (a) to\",\"a list of pairs that need to be compared. Specifically, for eac\",\"h column\",\"i\",\",\",\"there should be a list of those columns\",\"j \u003e i\",\"with which\",\"i\",\"needs to be\",\"compared.\"],\"page\":19},{\"texts\":[\"74\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"3.5  Distance Measures\",\"We now take a short detour to study the general notion of dista\",\"nce measures.\",\"The Jaccard similarity is a measure of how close sets are, alt\",\"hough it is not\",\"really a distance measure. That is, the closer sets are, the h\",\"igher the Jaccard\",\"similarity. Rather, 1 minus the Jaccard similarity is a dist\",\"ance measure, as we\",\"shall see; it is called the\",\"Jaccard distance\",\".\",\"However, Jaccard distance is not the only measure of closene\",\"ss that makes\",\"sense. We shall examine in this section some other distance m\",\"easures that have\",\"applications. Then, in Section 3.6 we see how some of these di\",\"stance measures\",\"also have an LSH technique that allows us to focus on nearby po\",\"ints without\",\"comparing all points. Other applications of distance measu\",\"res will appear when\",\"we study clustering in Chapter 7.\",\"3.5.1  Definition of a Distance Measure\",\"Suppose we have a set of points, called a\",\"space\",\". A\",\"distance measure\",\"on this\",\"space is a function\",\"d\",\"(\",\"x, y\",\") that takes two points in the space as arguments and\",\"produces a real number, and satisfies the following axioms:\",\"1.\",\"d\",\"(\",\"x, y\",\")\",\"≥\",\"0 (no negative distances).\",\"2.\",\"d\",\"(\",\"x, y\",\") = 0 if and only if\",\"x\",\"=\",\"y\",\"(distances are positive, except for the\",\"distance from a point to itself).\",\"3.\",\"d\",\"(\",\"x, y\",\") =\",\"d\",\"(\",\"y, x\",\") (distance is symmetric).\",\"4.\",\"d\",\"(\",\"x, y\",\")\",\"≤\",\"d\",\"(\",\"x, z\",\") +\",\"d\",\"(\",\"z, y\",\") (the\",\"triangle inequality\",\").\",\"The triangle inequality is the most complex condition. It sa\",\"ys, intuitively, that\",\"to travel from\",\"x\",\"to\",\"y\",\", we cannot obtain any benefit if we are forced to travel via\",\"some particular third point\",\"z\",\". The triangle-inequality axiom is what makes all\",\"distance measures behave as if distance describes the lengt\",\"h of a shortest path\",\"from one point to another.\",\"3.5.2  Euclidean Distances\",\"The most familiar distance measure is the one we normally thi\",\"nk of as “dis-\",\"tance.” An\",\"n\",\"-dimensional Euclidean space\",\"is one where points are vectors of\",\"n\",\"real numbers. The conventional distance measure in this spa\",\"ce, which we shall\",\"refer to as the\",\"L\",\"2\",\"-norm\",\", is defined:\",\"d\",\"([\",\"x\",\"1\",\", x\",\"2\",\", . . . , x\",\"n\",\"]\",\",\",\"[\",\"y\",\"1\",\", y\",\"2\",\", . . . , y\",\"n\",\"]) =\",\"√\",\"√\",\"√\",\"√\",\"n\",\"∑\",\"i\",\"=1\",\"(\",\"x\",\"i\",\"−\",\"y\",\"i\",\")\",\"2\",\"That is, we square the distance in each dimension, sum the squ\",\"ares, and take\",\"the positive square root.\"],\"page\":20},{\"texts\":[\"3.5.  DISTANCE MEASURES\",\"75\",\"It is easy to verify the first three requirements for a distanc\",\"e measure are\",\"satisfied. The Euclidean distance between two points cannot\",\"be negative, be-\",\"cause the positive square root is intended. Since all square\",\"s of real numbers are\",\"nonnegative, any\",\"i\",\"such that\",\"x\",\"i\",\"6\",\"=\",\"y\",\"i\",\"forces the distance to be strictly positive.\",\"On the other hand, if\",\"x\",\"i\",\"=\",\"y\",\"i\",\"for all\",\"i\",\", then the distance is clearly 0. Symmetry\",\"follows because (\",\"x\",\"i\",\"−\",\"y\",\"i\",\")\",\"2\",\"= (\",\"y\",\"i\",\"−\",\"x\",\"i\",\")\",\"2\",\". The triangle inequality requires a good\",\"deal of algebra to verify. However, it is well understood to b\",\"e a property of\",\"Euclidean space: the sum of the lengths of any two sides of a tr\",\"iangle is no less\",\"than the length of the third side.\",\"There are other distance measures that have been used for Euc\",\"lidean spaces.\",\"For any constant\",\"r\",\", we can define the\",\"L\",\"r\",\"-norm\",\"to be the distance measure\",\"d\",\"defined by:\",\"d\",\"([\",\"x\",\"1\",\", x\",\"2\",\", . . . , x\",\"n\",\"]\",\",\",\"[\",\"y\",\"1\",\", y\",\"2\",\", . . . , y\",\"n\",\"]) = (\",\"n\",\"∑\",\"i\",\"=1\",\"|\",\"x\",\"i\",\"−\",\"y\",\"i\",\"|\",\"r\",\")\",\"1\",\"/r\",\"The case\",\"r\",\"= 2 is the usual\",\"L\",\"2\",\"-norm just mentioned. Another common distance\",\"measure is the\",\"L\",\"1\",\"-norm, or\",\"Manhattan distance\",\". There, the distance between\",\"two points is the sum of the magnitudes of the differences in ea\",\"ch dimension.\",\"It is called “Manhattan distance” because it is the distance\",\"one would have to\",\"travel between points if one were constrained to travel alon\",\"g grid lines, as on\",\"the streets of a city such as Manhattan.\",\"Another interesting distance measure is the\",\"L\",\"∞\",\"-norm, which is the limit\",\"as\",\"r\",\"approaches infinity of the\",\"L\",\"r\",\"-norm. As\",\"r\",\"gets larger, only the dimension\",\"with the largest difference matters, so formally, the\",\"L\",\"∞\",\"-norm is defined as the\",\"maximum of\",\"|\",\"x\",\"i\",\"−\",\"y\",\"i\",\"|\",\"over all dimensions\",\"i\",\".\",\"Example 3.12:\",\"Consider the two-dimensional Euclidean space (the custom-\",\"ary plane) and the points (2\",\",\",\"7) and (6\",\",\",\"4).  The\",\"L\",\"2\",\"-norm gives a distance\",\"of\",\"√\",\"(2\",\"−\",\"6)\",\"2\",\"+ (7\",\"−\",\"4)\",\"2\",\"=\",\"√\",\"4\",\"2\",\"+ 3\",\"2\",\"= 5.  The\",\"L\",\"1\",\"-norm gives a distance of\",\"|\",\"2\",\"−\",\"6\",\"|\",\"+\",\"|\",\"7\",\"−\",\"4\",\"|\",\"= 4 + 3 = 7. The\",\"L\",\"∞\",\"-norm gives a distance of\",\"max(\",\"|\",\"2\",\"−\",\"6\",\"|\",\",\",\"|\",\"7\",\"−\",\"4\",\"|\",\") = max(4\",\",\",\"3) = 4\",\"2\",\"3.5.3  Jaccard Distance\",\"As mentioned at the beginning of the section, we define the\",\"Jaccard distance\",\"of sets by\",\"d\",\"(\",\"x, y\",\") = 1\",\"−\",\"SIM\",\"(\",\"x, y\",\"). That is, the Jaccard distance is 1 minus the\",\"ratio of the sizes of the intersection and union of sets\",\"x\",\"and\",\"y\",\". We must verify\",\"that this function is a distance measure.\",\"1.\",\"d\",\"(\",\"x, y\",\") is nonnegative because the size of the intersection cannot\",\"exceed\",\"the size of the union.\"],\"page\":21},{\"texts\":[\"76\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"2.\",\"d\",\"(\",\"x, y\",\") = 0 if\",\"x\",\"=\",\"y\",\", because\",\"x\",\"∪\",\"x\",\"=\",\"x\",\"∩\",\"x\",\"=\",\"x\",\". However, if\",\"x\",\"6\",\"=\",\"y\",\", then\",\"the size of\",\"x\",\"∩\",\"y\",\"is strictly less than the size of\",\"x\",\"∪\",\"y\",\", so\",\"d\",\"(\",\"x, y\",\") is strictly\",\"positive.\",\"3.\",\"d\",\"(\",\"x, y\",\") =\",\"d\",\"(\",\"y, x\",\") because both union and intersection are symmetric; i.e.,\",\"x\",\"∪\",\"y\",\"=\",\"y\",\"∪\",\"x\",\"and\",\"x\",\"∩\",\"y\",\"=\",\"y\",\"∩\",\"x\",\".\",\"4. For the triangle inequality, recall from Section 3.3.3 th\",\"at\",\"SIM\",\"(\",\"x, y\",\") is the\",\"probability a random minhash function maps\",\"x\",\"and\",\"y\",\"to the same value.\",\"Thus, the Jaccard distance\",\"d\",\"(\",\"x, y\",\") is the probability that a random min-\",\"hash function\",\"does not\",\"send\",\"x\",\"and\",\"y\",\"to the same value. We can therefore\",\"translate the condition\",\"d\",\"(\",\"x, y\",\")\",\"≤\",\"d\",\"(\",\"x, z\",\") +\",\"d\",\"(\",\"z, y\",\") to the statement that if\",\"h\",\"is a random minhash function, then the probability that\",\"h\",\"(\",\"x\",\")\",\"6\",\"=\",\"h\",\"(\",\"y\",\")\",\"is no greater than the sum of the probability that\",\"h\",\"(\",\"x\",\")\",\"6\",\"=\",\"h\",\"(\",\"z\",\") and the\",\"probability that\",\"h\",\"(\",\"z\",\")\",\"6\",\"=\",\"h\",\"(\",\"y\",\"). However, this statement is true because\",\"whenever\",\"h\",\"(\",\"x\",\")\",\"6\",\"=\",\"h\",\"(\",\"y\",\"), at least one of\",\"h\",\"(\",\"x\",\") and\",\"h\",\"(\",\"y\",\") must be different\",\"from\",\"h\",\"(\",\"z\",\").  They could not both be\",\"h\",\"(\",\"z\",\"), because then\",\"h\",\"(\",\"x\",\") and\",\"h\",\"(\",\"y\",\")\",\"would be the same.\",\".\",\"3.5.4  Cosine Distance\",\"The\",\"cosine distance\",\"makes sense in spaces that have dimensions, including Eu-\",\"clidean spaces and discrete versions of Euclidean spaces, s\",\"uch as spaces where\",\"points are vectors with integer components or boolean (0 or 1\",\") components. In\",\"such a space, points may be thought of as directions. We do not\",\"distinguish be-\",\"tween a vector and a multiple of that vector. Then the cosine d\",\"istance between\",\"two points is the angle that the vectors to those points make.\",\"This angle will\",\"be in the range 0 to 180 degrees, regardless of how many dimens\",\"ions the space\",\"has.\",\"We can calculate the cosine distance by first computing the co\",\"sine of the\",\"angle, and then applying the arc-cosine function to transla\",\"te to an angle in the\",\"0-180 degree range. Given two vectors\",\"x\",\"and\",\"y\",\", the cosine of the angle between\",\"them is the dot product\",\"x.y\",\"divided by the\",\"L\",\"2\",\"-norms of\",\"x\",\"and\",\"y\",\"(i.e., their\",\"Euclidean distances from the origin). Recall that the dot pr\",\"oduct of vectors\",\"[\",\"x\",\"1\",\", x\",\"2\",\", . . . , x\",\"n\",\"]\",\".\",\"[\",\"y\",\"1\",\", y\",\"2\",\", . . . , y\",\"n\",\"] is\",\"∑\",\"n\",\"i\",\"=1\",\"x\",\"i\",\"y\",\"i\",\".\",\"Example 3.13:\",\"Let our two vectors be\",\"x\",\"= [1\",\",\",\"2\",\",\",\"−\",\"1] and = [2\",\",\",\"1\",\",\",\"1]. The dot\",\"product\",\"x.y\",\"is 1\",\"×\",\"2 + 2\",\"×\",\"1 + (\",\"−\",\"1)\",\"×\",\"1 = 3. The\",\"L\",\"2\",\"-norm of both vectors is\",\"√\",\"6. For example,\",\"x\",\"has\",\"L\",\"2\",\"-norm\",\"√\",\"1\",\"2\",\"+ 2\",\"2\",\"+ (\",\"−\",\"1)\",\"2\",\"=\",\"√\",\"6. Thus, the cosine of\",\"the angle between\",\"x\",\"and\",\"y\",\"is 3\",\"/\",\"(\",\"√\",\"6\",\"√\",\"6) or 1/2. The angle whose cosine is 1/2\",\"is 60 degrees, so that is the cosine distance between\",\"x\",\"and\",\"y\",\".\",\"2\",\"We must show that the cosine distance is indeed a distance mea\",\"sure. We\",\"have defined it so the values are in the range 0 to 180, so no nega\",\"tive distances\"],\"page\":22},{\"texts\":[\"3.5.  DISTANCE MEASURES\",\"77\",\"are possible. Two vectors have angle 0 if and only if they are t\",\"he same direction.\",\"3\",\"Symmetry is obvious: the angle between\",\"x\",\"and\",\"y\",\"is the same as the angle\",\"between\",\"y\",\"and\",\"x\",\". The triangle inequality is best argued by physical reasoni\",\"ng.\",\"One way to rotate from\",\"x\",\"to\",\"y\",\"is to rotate to\",\"z\",\"and thence to\",\"y\",\". The sum of\",\"those two rotations cannot be less than the rotation directl\",\"y from\",\"x\",\"to\",\"y\",\".\",\"3.5.5  Edit Distance\",\"This distance makes sense when points are strings. The dista\",\"nce between two\",\"strings\",\"x\",\"=\",\"x\",\"1\",\"x\",\"2\",\"···\",\"x\",\"n\",\"and\",\"y\",\"=\",\"y\",\"1\",\"y\",\"2\",\"···\",\"y\",\"m\",\"is the smallest number of insertions\",\"and deletions of single characters that will convert\",\"x\",\"to\",\"y\",\".\",\"Example 3.14:\",\"The edit distance between the strings\",\"x\",\"=\",\"abcde\",\"and\",\"y\",\"=\",\"acfdeg\",\"is 3. To convert\",\"x\",\"to\",\"y\",\":\",\"1. Delete\",\"b\",\".\",\"2. Insert\",\"f\",\"after\",\"c\",\".\",\"3. Insert\",\"g\",\"after\",\"e\",\".\",\"No sequence of fewer than three insertions and/or deletions\",\"will convert\",\"x\",\"to\",\"y\",\".\",\"Thus,\",\"d\",\"(\",\"x, y\",\") = 3.\",\"2\",\"Another way to define and calculate the edit distance\",\"d\",\"(\",\"x, y\",\") is to compute\",\"a\",\"longest common subsequence\",\"(LCS) of\",\"x\",\"and\",\"y\",\".   An LCS of\",\"x\",\"and\",\"y\",\"is a\",\"string that is constructed by deleting positions from\",\"x\",\"and\",\"y\",\", and that is as\",\"long as any string that can be constructed that way. The edit d\",\"istance\",\"d\",\"(\",\"x, y\",\")\",\"can be calculated as the length of\",\"x\",\"plus the length of\",\"y\",\"minus twice the length\",\"of their LCS.\",\"Example 3.15:\",\"The strings\",\"x\",\"=\",\"abcde\",\"and\",\"y\",\"=\",\"acfdeg\",\"from Example 3.14\",\"have a unique LCS, which is\",\"acde\",\". We can be sure it is the longest possible,\",\"because it contains every symbol appearing in both\",\"x\",\"and\",\"y\",\". Fortunately, these\",\"common symbols appear in the same order in both strings, so we\",\"are able to\",\"use them all in an LCS. Note that the length of\",\"x\",\"is 5, the length of\",\"y\",\"is 6, and\",\"the length of their LCS is 4. The edit distance is thus 5 + 6\",\"−\",\"2\",\"×\",\"4 = 3, which\",\"agrees with the direct calculation in Example 3.14.\",\"For another example, consider\",\"x\",\"=\",\"aba\",\"and\",\"y\",\"=\",\"bab\",\". Their edit distance is\",\"2. For example, we can convert\",\"x\",\"to\",\"y\",\"by deleting the first\",\"a\",\"and then inserting\",\"b\",\"at the end.  There are two LCS’s:\",\"ab\",\"and\",\"ba\",\".  Each can be obtained by\",\"deleting one symbol from each string. As must be the case for m\",\"ultiple LCS’s\",\"of the same pair of strings, both LCS’s have the same length. T\",\"herefore, we\",\"may compute the edit distance as 3 + 3\",\"−\",\"2\",\"×\",\"2 = 2.\",\"2\",\"3\",\"Notice that to satisfy the second axiom, we have to treat vect\",\"ors that are multiples of\",\"one another, e.g. [1\",\",\",\"2] and [3\",\",\",\"6], as the same direction, which they are. If we regarded thes\",\"e\",\"as different vectors, we would give them distance 0 and thus vi\",\"olate the condition that only\",\"d\",\"(\",\"x, x\",\") is 0.\"],\"page\":23},{\"texts\":[\"78\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"Non-Euclidean Spaces\",\"Notice that several of the distance measures introduced in t\",\"his section are\",\"not Euclidean spaces. A property of Euclidean spaces that we\",\"shall find\",\"important when we take up clustering in Chapter 7 is that the a\",\"verage\",\"of points in a Euclidean space always exists and is a point in t\",\"he space.\",\"However, consider the space of sets for which we defined the Ja\",\"ccard dis-\",\"tance. The notion of the “average” of two sets makes no sense.\",\"Likewise,\",\"the space of strings, where we can use the edit distance, does\",\"not let us\",\"take the “average” of strings.\",\"Vector spaces, for which we suggested the cosine distance, m\",\"ay or may\",\"not be Euclidean. If the components of the vectors can be any r\",\"eal num-\",\"bers, then the space is Euclidean. However, if we restrict co\",\"mponents to\",\"be integers, then the space is not Euclidean. Notice that, fo\",\"r instance, we\",\"cannot find an average of the vectors [1\",\",\",\"2] and [3\",\",\",\"1] in the space of vectors\",\"with two integer components, although if we treated them as m\",\"embers of\",\"the two-dimensional Euclidean space, then we could say that\",\"their average\",\"was [2\",\".\",\"0\",\",\",\"1\",\".\",\"5].\",\"Edit distance is a distance measure. Surely no edit distance\",\"can be negative,\",\"and only two identical strings have an edit distance of 0.  To s\",\"ee that edit\",\"distance is symmetric, note that a sequence of insertions an\",\"d deletions can be\",\"reversed, with each insertion becoming a deletion, and vice\",\"-versa. The triangle\",\"inequality is also straightforward. One way to turn a string\",\"s\",\"into a string\",\"t\",\"is to turn\",\"s\",\"into some string\",\"u\",\"and then turn\",\"u\",\"into\",\"t\",\". Thus, the number of\",\"edits made going from\",\"s\",\"to\",\"u\",\", plus the number of edits made going from\",\"u\",\"to\",\"t\",\"cannot be less than the smallest number of edits that will tur\",\"n\",\"s\",\"into\",\"t\",\".\",\"3.5.6  Hamming Distance\",\"Given a space of vectors, we define the\",\"Hamming distance\",\"between two vectors\",\"to be the number of components in which they differ. It should b\",\"e obvious that\",\"Hamming distance is a distance measure. Clearly the Hamming\",\"distance cannot\",\"be negative, and if it is zero, then the vectors are identical\",\". The distance does\",\"not depend on which of two vectors we consider first. The trian\",\"gle inequality\",\"should also be evident. If\",\"x\",\"and\",\"z\",\"differ in\",\"m\",\"components, and\",\"z\",\"and\",\"y\",\"differ in\",\"n\",\"components, then\",\"x\",\"and\",\"y\",\"cannot differ in more than\",\"m\",\"+\",\"n\",\"components. Most\",\"commonly, Hamming distance is used when the vectors are bool\",\"ean; they consist\",\"of 0’s and 1’s only. However, in principle, the vectors can ha\",\"ve components from\",\"any set.\",\"Example 3.16:\",\"The Hamming distance between the vectors 10101 and 11110\",\"is 3. That is, these vectors differ in the second, fourth, and fi\",\"fth components,\"],\"page\":24},{\"texts\":[\"3.5.  DISTANCE MEASURES\",\"79\",\"while they agree in the first and third components.\",\"2\",\"3.5.7  Exercises for Section 3.5\",\"! Exercise 3.5.1:\",\"On the space of nonnegative integers, which of the following\",\"functions are distance measures? If so, prove it; if not, pro\",\"ve that it fails to\",\"satisfy one or more of the axioms.\",\"(a) max(\",\"x, y\",\") = the larger of\",\"x\",\"and\",\"y\",\".\",\"(b) diff(\",\"x, y\",\") =\",\"|\",\"x\",\"−\",\"y\",\"|\",\"(the absolute magnitude of the difference between\",\"x\",\"and\",\"y\",\").\",\"(c) sum(\",\"x, y\",\") =\",\"x\",\"+\",\"y\",\".\",\"Exercise 3.5.2:\",\"Find the\",\"L\",\"1\",\"and\",\"L\",\"2\",\"distances between the points (5\",\",\",\"6\",\",\",\"7) and\",\"(8\",\",\",\"2\",\",\",\"4).\",\"!! Exercise 3.5.3:\",\"Prove that if\",\"i\",\"and\",\"j\",\"are any positive integers, and\",\"i \u003c j\",\",\",\"then the\",\"L\",\"i\",\"norm between any two points is greater than the\",\"L\",\"j\",\"norm between\",\"those same two points.\",\"Exercise 3.5.4:\",\"Find the Jaccard distances between the following pairs of\",\"sets:\",\"(a)\",\"{\",\"1\",\",\",\"2\",\",\",\"3\",\",\",\"4\",\"}\",\"and\",\"{\",\"2\",\",\",\"3\",\",\",\"4\",\",\",\"5\",\"}\",\".\",\"(b)\",\"{\",\"1\",\",\",\"2\",\",\",\"3\",\"}\",\"and\",\"{\",\"4\",\",\",\"5\",\",\",\"6\",\"}\",\".\",\"Exercise 3.5.5:\",\"Compute the cosines of the angles between each of the fol-\",\"lowing pairs of vectors.\",\"4\",\"(a) (3\",\",\",\"−\",\"1\",\",\",\"2) and (\",\"−\",\"2\",\",\",\"3\",\",\",\"1).\",\"(b) (1\",\",\",\"2\",\",\",\"3) and (2\",\",\",\"4\",\",\",\"6).\",\"(c) (5\",\",\",\"0\",\",\",\"−\",\"4) and (\",\"−\",\"1\",\",\",\"−\",\"6\",\",\",\"2).\",\"(d) (0\",\",\",\"1\",\",\",\"1\",\",\",\"0\",\",\",\"1\",\",\",\"1) and (0\",\",\",\"0\",\",\",\"1\",\",\",\"0\",\",\",\"0\",\",\",\"0).\",\"! Exercise 3.5.6:\",\"Prove that the cosine distance between any two vectors of 0’s\",\"and 1’s, of the same length, is at most 90 degrees.\",\"Exercise 3.5.7:\",\"Find the edit distances (using only insertions and deletion\",\"s)\",\"between the following pairs of strings.\",\"4\",\"Note that what we are asking for is not precisely the cosine di\",\"stance, but from the cosine\",\"of an angle, you can compute the angle itself, perhaps with th\",\"e aid of a table or library\",\"function.\"],\"page\":25},{\"texts\":[\"80\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"(a)\",\"abcdef\",\"and\",\"bdaefc\",\".\",\"(b)\",\"abccdabc\",\"and\",\"acbdcab\",\".\",\"(c)\",\"abcdef\",\"and\",\"baedfc\",\".\",\"! Exercise 3.5.8:\",\"There are a number of other notions of edit distance availabl\",\"e.\",\"For instance, we can allow, in addition to insertions and del\",\"etions, the following\",\"operations:\",\"i\",\".\",\"Mutation\",\", where one symbol is replaced by another symbol. Note that a\",\"mutation can always be performed by an insertion followed by\",\"a deletion,\",\"but if we allow mutations, then this change counts for only 1,\",\"not 2, when\",\"computing the edit distance.\",\"ii\",\".\",\"Transposition\",\", where two adjacent symbols have their positions swapped.\",\"Like a mutation, we can simulate a transposition by one inser\",\"tion followed\",\"by one deletion, but here we count only 1 for these two steps.\",\"Repeat Exercise 3.5.7 if edit distance is defined to be the num\",\"ber of insertions,\",\"deletions, mutations, and transpositions needed to transf\",\"orm one string into\",\"another.\",\"! Exercise 3.5.9:\",\"Prove that the edit distance discussed in Exercise 3.5.8 is\",\"indeed a distance measure.\",\"Exercise 3.5.10:\",\"Find the Hamming distances between each pair of the fol-\",\"lowing vectors: 000000, 110011, 010101, and 011100.\",\"3.6  The Theory of Locality-Sensitive Functions\",\"The LSH technique developed in Section 3.4 is one example of a\",\"family of func-\",\"tions (the minhash functions) that can be combined (by the ba\",\"nding technique)\",\"to distinguish strongly between pairs at a low distance from\",\"pairs at a high dis-\",\"tance. The steepness of the S-curve in Fig. 3.7 reflects how eff\",\"ectively we can\",\"avoid false positives and false negatives among the candida\",\"te pairs.\",\"Now, we shall explore other families of functions, besides t\",\"he minhash func-\",\"tions, that can serve to produce candidate pairs efficiently.\",\"These functions can\",\"apply to the space of sets and the Jaccard distance, or to anot\",\"her space and/or\",\"another distance measure. There are three conditions that w\",\"e need for a family\",\"of functions:\",\"1. They must be more likely to make close pairs be candidate pa\",\"irs than\",\"distant pairs. We make this notion precise in Section 3.6.1.\",\"2. They must be statistically independent, in the sense that\",\"it is possible to\",\"estimate the probability that two or more functions will all\",\"give a certain\",\"response by the product rule for independent events.\"],\"page\":26},{\"texts\":[\"3.6.  THE THEORY OF LOCALITY-SENSITIVE FUNCTIONS\",\"81\",\"3. They must be efficient, in two ways:\",\"(a) They must be able to identify candidate pairs in time much\",\"less\",\"than the time it takes to look at all pairs. For example, minha\",\"sh\",\"functions have this capability, since we can hash sets to min\",\"hash\",\"values in time proportional to the size of the data, rather th\",\"an the\",\"square of the number of sets in the data. Since sets with commo\",\"n\",\"values are colocated in a bucket, we have implicitly produce\",\"d the\",\"candidate pairs for a single minhash function in time much le\",\"ss than\",\"the number of pairs of sets.\",\"(b) They must be combinable to build functions that are bette\",\"r at avoid-\",\"ing false positives and negatives, and the combined functio\",\"ns must\",\"also take time that is much less than the number of pairs. For e\",\"x-\",\"ample, the banding technique of Section 3.4.1 takes single m\",\"inhash\",\"functions, which satisfy condition 3a but do not, by themsel\",\"ves have\",\"the S-curve behavior we want, and produces from a number of mi\",\"n-\",\"hash functions a combined function that has the S-curve shap\",\"e.\",\"Our first step is to define “locality-sensitive functions” ge\",\"nerally. We then\",\"see how the idea can be applied in several applications.  Fina\",\"lly, we discuss\",\"how to apply the theory to arbitrary data with either a cosine\",\"distance or a\",\"Euclidean distance measure.\",\"3.6.1  Locality-Sensitive Functions\",\"For the purposes of this section, we shall consider function\",\"s that take two items\",\"and render a decision about whether these items should be a ca\",\"ndidate pair.\",\"In many cases, the function\",\"f\",\"will “hash” items, and the decision will be based\",\"on whether or not the result is equal.  Because it is convenien\",\"t to use the\",\"notation\",\"f\",\"(\",\"x\",\") =\",\"f\",\"(\",\"y\",\") to mean that\",\"f\",\"(\",\"x, y\",\") is “yes; make\",\"x\",\"and\",\"y\",\"a candidate\",\"pair,” we shall use\",\"f\",\"(\",\"x\",\") =\",\"f\",\"(\",\"y\",\") as a shorthand with this meaning. We also use\",\"f\",\"(\",\"x\",\")\",\"6\",\"=\",\"f\",\"(\",\"y\",\") to mean “do not make\",\"x\",\"and\",\"y\",\"a candidate pair unless some other\",\"function concludes we should do so.”\",\"A collection of functions of this form will be called a\",\"family\",\"of functions.\",\"For example, the family of minhash functions, each based on o\",\"ne of the possible\",\"permutations of rows of a characteristic matrix, form a fami\",\"ly.\",\"Let\",\"d\",\"1\",\"\u003c d\",\"2\",\"be two distances according to some distance measure\",\"d\",\".  A\",\"family\",\"F\",\"of functions is said to be (\",\"d\",\"1\",\", d\",\"2\",\", p\",\"1\",\", p\",\"2\",\")\",\"-sensitive\",\"if for every\",\"f\",\"in\",\"F\",\":\",\"1. If\",\"d\",\"(\",\"x, y\",\")\",\"≤\",\"d\",\"1\",\", then the probability that\",\"f\",\"(\",\"x\",\") =\",\"f\",\"(\",\"y\",\") is at least\",\"p\",\"1\",\".\",\"2. If\",\"d\",\"(\",\"x, y\",\")\",\"≥\",\"d\",\"2\",\", then the probability that\",\"f\",\"(\",\"x\",\") =\",\"f\",\"(\",\"y\",\") is at most\",\"p\",\"2\",\".\",\"Figure 3.9 illustrates what we expect about the probability\",\"that a given\",\"function in a (\",\"d\",\"1\",\", d\",\"2\",\", p\",\"1\",\", p\",\"2\",\")-sensitive family will declare two items to be a can-\",\"didate pair. Notice that we say nothing about what happens wh\",\"en the distance\"],\"page\":27},{\"texts\":[\"82\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"Probabilty\",\"of being\",\"declared a\",\"candidate\",\"d\",\"p\",\"d\",\"p\",\"1\",\"2\",\"1\",\"2\",\"Distance\",\"Figure 3.9: Behavior of a (\",\"d\",\"1\",\", d\",\"2\",\", p\",\"1\",\", p\",\"2\",\")-sensitive function\",\"between the items is strictly between\",\"d\",\"1\",\"and\",\"d\",\"2\",\", but we can make\",\"d\",\"1\",\"and\",\"d\",\"2\",\"as\",\"close as we wish. The penalty is that typically\",\"p\",\"1\",\"and\",\"p\",\"2\",\"are then close as well.\",\"As we shall see, it is possible to drive\",\"p\",\"1\",\"and\",\"p\",\"2\",\"apart while keeping\",\"d\",\"1\",\"and\",\"d\",\"2\",\"fixed.\",\"3.6.2  Locality-Sensitive Families for Jaccard Distance\",\"For the moment, we have only one way to find a family of locality\",\"-sensitive\",\"functions: use the family of minhash functions, and assume t\",\"hat the distance\",\"measure is the Jaccard distance. As before, we interpret a mi\",\"nhash function\",\"h\",\"to make\",\"x\",\"and\",\"y\",\"a candidate pair if and only if\",\"h\",\"(\",\"x\",\") =\",\"h\",\"(\",\"y\",\").\",\"•\",\"The family of minhash functions is a (\",\"d\",\"1\",\", d\",\"2\",\",\",\"1\",\"−\",\"d\",\"1\",\",\",\"1\",\"−\",\"d\",\"2\",\")-sensitive family\",\"for any\",\"d\",\"1\",\"and\",\"d\",\"2\",\", where 0\",\"≤\",\"d\",\"1\",\"\u003c d\",\"2\",\"≤\",\"1.\",\"The reason is that if\",\"d\",\"(\",\"x, y\",\")\",\"≤\",\"d\",\"1\",\", where\",\"d\",\"is the Jaccard distance, then\",\"SIM\",\"(\",\"x, y\",\") = 1\",\"−\",\"d\",\"(\",\"x, y\",\")\",\"≥\",\"1\",\"−\",\"d\",\"1\",\".  But we know that the Jaccard similarity\",\"of\",\"x\",\"and\",\"y\",\"is equal to the probability that a minhash function will hash\",\"x\",\"and\",\"y\",\"to the same value. A similar argument applies to\",\"d\",\"2\",\"or any distance.\",\"Example 3.17:\",\"We could let\",\"d\",\"1\",\"= 0\",\".\",\"3 and\",\"d\",\"2\",\"= 0\",\".\",\"6. Then we can assert that\",\"the family of minhash functions is a (0\",\".\",\"3\",\",\",\"0\",\".\",\"6\",\",\",\"0\",\".\",\"7\",\",\",\"0\",\".\",\"4)-sensitive family. That is,\",\"if the Jaccard distance between\",\"x\",\"and\",\"y\",\"is at most 0.3 (i.e.,\",\"SIM\",\"(\",\"x, y\",\")\",\"≥\",\"0\",\".\",\"7)\",\"then there is at least a 0.7 chance that a minhash function wil\",\"l send\",\"x\",\"and\",\"y\",\"to\",\"the same value, and if the Jaccard distance between\",\"x\",\"and\",\"y\",\"is at least 0.6 (i.e.,\",\"SIM\",\"(\",\"x, y\",\")\",\"≤\",\"0\",\".\",\"4), then there is at most a 0.4 chance that\",\"x\",\"and\",\"y\",\"will be sent\",\"to the same value. Note that we could make the same assertion w\",\"ith another\",\"choice of\",\"d\",\"1\",\"and\",\"d\",\"2\",\"; only\",\"d\",\"1\",\"\u003c d\",\"2\",\"is required.\",\"2\"],\"page\":28},{\"texts\":[\"3.6.  THE THEORY OF LOCALITY-SENSITIVE FUNCTIONS\",\"83\",\"3.6.3  Amplifying a Locality-Sensitive Family\",\"Suppose we are given a (\",\"d\",\"1\",\", d\",\"2\",\", p\",\"1\",\", p\",\"2\",\")-sensitive family\",\"F\",\". We can construct a\",\"new family\",\"F\",\"′\",\"by the\",\"AND-construction\",\"on\",\"F\",\", which is defined as follows. Each\",\"member of\",\"F\",\"′\",\"consists of\",\"r\",\"members of\",\"F\",\"for some fixed\",\"r\",\". If\",\"f\",\"is in\",\"F\",\"′\",\", and\",\"f\",\"is\",\"constructed from the set\",\"{\",\"f\",\"1\",\", f\",\"2\",\", . . . , f\",\"r\",\"}\",\"of members of\",\"F\",\", we say\",\"f\",\"(\",\"x\",\") =\",\"f\",\"(\",\"y\",\")\",\"if and only if\",\"f\",\"i\",\"(\",\"x\",\") =\",\"f\",\"i\",\"(\",\"y\",\") for all\",\"i\",\"= 1\",\",\",\"2\",\", . . . , r\",\". Notice that this construction\",\"mirrors the effect of the\",\"r\",\"rows in a single band: the band makes\",\"x\",\"and\",\"y\",\"a\",\"candidate pair if every one of the\",\"r\",\"rows in the band say that\",\"x\",\"and\",\"y\",\"are equal\",\"(and therefore a candidate pair according to that row).\",\"Since the members of\",\"F\",\"are independently chosen to make a member of\",\"F\",\"′\",\",\",\"we can assert that\",\"F\",\"′\",\"is a\",\"(\",\"d\",\"1\",\", d\",\"2\",\",\",\"(\",\"p\",\"1\",\")\",\"r\",\",\",\"(\",\"p\",\"2\",\")\",\"r\",\")\",\"-sensitive family. That is, for any\",\"p\",\", if\",\"p\",\"is the probability that a member of\",\"F\",\"will declare (\",\"x, y\",\") to be a candidate\",\"pair, then the probability that a member of\",\"F\",\"′\",\"will so declare is\",\"p\",\"r\",\".\",\"There is another construction, which we call the\",\"OR-construction\",\", that turns\",\"a (\",\"d\",\"1\",\", d\",\"2\",\", p\",\"1\",\", p\",\"2\",\")-sensitive family\",\"F\",\"into a\",\"(\",\"d\",\"1\",\", d\",\"2\",\",\",\"1\",\"−\",\"(1\",\"−\",\"p\",\"1\",\")\",\"b\",\",\",\"1\",\"−\",\"(1\",\"−\",\"p\",\"2\",\")\",\"b\",\")\",\"-\",\"sensitive family\",\"F\",\"′\",\". Each member\",\"f\",\"of\",\"F\",\"′\",\"is constructed from\",\"b\",\"members of\",\"F\",\",\",\"say\",\"f\",\"1\",\", f\",\"2\",\", . . . , f\",\"b\",\". We define\",\"f\",\"(\",\"x\",\") =\",\"f\",\"(\",\"y\",\") if and only if\",\"f\",\"i\",\"(\",\"x\",\") =\",\"f\",\"i\",\"(\",\"y\",\") for one or\",\"more values of\",\"i\",\". The OR-construction mirrors the effect of combining severa\",\"l\",\"bands:\",\"x\",\"and\",\"y\",\"become a candidate pair if any band makes them a candidate\",\"pair.\",\"If\",\"p\",\"is the probability that a member of\",\"F\",\"will declare (\",\"x, y\",\") to be a candidate\",\"pair, then 1\",\"−\",\"p\",\"is the probability it will not so declare. (1\",\"−\",\"p\",\")\",\"b\",\"is the probability\",\"that none of\",\"f\",\"1\",\", f\",\"2\",\", . . . , f\",\"b\",\"will declare (\",\"x, y\",\") a candidate pair, and 1\",\"−\",\"(1\",\"−\",\"p\",\")\",\"b\",\"is the probability that at least one\",\"f\",\"i\",\"will declare (\",\"x, y\",\") a candidate pair, and\",\"therefore that\",\"f\",\"will declare (\",\"x, y\",\") to be a candidate pair.\",\"Notice that the AND-construction lowers all probabilities\",\", but if we choose\",\"F\",\"and\",\"r\",\"judiciously, we can make the small probability\",\"p\",\"2\",\"get very close to 0, while\",\"the higher probability\",\"p\",\"1\",\"stays significantly away from 0. Similarly, the OR-\",\"construction makes all probabilities rise, but by choosing\",\"F\",\"and\",\"b\",\"judiciously,\",\"we can make the larger probability approach 1 while the small\",\"er probability\",\"remains bounded away from 1. We can cascade AND- and OR-const\",\"ructions in\",\"any order to make the low probability close to 0 and the high pr\",\"obability close\",\"to 1. Of course the more constructions we use, and the higher t\",\"he values of\",\"r\",\"and\",\"b\",\"that we pick, the larger the number of functions from the orig\",\"inal family\",\"that we are forced to use. Thus, the better the final family of f\",\"unctions is, the\",\"longer it takes to apply the functions from this family.\",\"Example 3.18:\",\"Suppose we start with a family\",\"F\",\". We use the AND-construc-\",\"tion with\",\"r\",\"= 4 to produce a family\",\"F\",\"1\",\". We then apply the OR-construction\",\"to\",\"F\",\"1\",\"with\",\"b\",\"= 4 to produce a third family\",\"F\",\"2\",\". Note that the members of\",\"F\",\"2\",\"each are built from 16 members of\",\"F\",\", and the situation is analogous to starting\",\"with 16 minhash functions and treating them as four bands of f\",\"our rows each.\",\"The 4-way AND-function converts any probability\",\"p\",\"into\",\"p\",\"4\",\".  When we\",\"follow it by the 4-way OR-construction, that probability is\",\"further converted\",\"into 1\",\"−\",\"(1\",\"−\",\"p\",\"4\",\")\",\"4\",\". Some values of this transformation are indicated in Fig. 3.\",\"10.\"],\"page\":29},{\"texts\":[\"84\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"p\",\"1\",\"−\",\"(1\",\"−\",\"p\",\"4\",\")\",\"4\",\"0.2\",\"0.0064\",\"0.3\",\"0.0320\",\"0.4\",\"0.0985\",\"0.5\",\"0.2275\",\"0.6\",\"0.4260\",\"0.7\",\"0.6666\",\"0.8\",\"0.8785\",\"0.9\",\"0.9860\",\"Figure 3.10: Effect of the 4-way AND-construction followed b\",\"y the 4-way OR-\",\"construction\",\"This function is an S-curve, staying low for a while, then ris\",\"ing steeply (although\",\"not too steeply; the slope never gets much higher than 2), and\",\"then leveling\",\"off at high values. Like any S-curve, it has a\",\"fixedpoint\",\", the value of\",\"p\",\"that is\",\"left unchanged when we apply the function of the S-curve. In t\",\"his case, the\",\"fixedpoint is the value of\",\"p\",\"for which\",\"p\",\"= 1\",\"−\",\"(1\",\"−\",\"p\",\"4\",\")\",\"4\",\". We can see that the\",\"fixedpoint is somewhere between 0.7 and 0.8. Below that value\",\", probabilities are\",\"decreased, and above it they are increased. Thus, if we pick a\",\"high probability\",\"above the fixedpoint and a low probability below it, we shall h\",\"ave the desired\",\"effect that the low probability is decreased and the high prob\",\"ability is increased.\",\"Suppose\",\"F\",\"is the minhash functions, regarded as a (0\",\".\",\"2\",\",\",\"0\",\".\",\"6\",\",\",\"0\",\".\",\"8\",\",\",\"0\",\".\",\"4)-sens-\",\"itive family. Then\",\"F\",\"2\",\", the family constructed by a 4-way AND followed by a\",\"4-way OR, is a (0\",\".\",\"2\",\",\",\"0\",\".\",\"6\",\",\",\"0\",\".\",\"8785\",\",\",\"0\",\".\",\"0985)-sensitive family, as we can read from the\",\"rows for 0.2 and 0.6 in Fig. 3.10. By replacing\",\"F\",\"by\",\"F\",\"2\",\", we have reduced both\",\"the false-negative and false-positive rates, at the cost of\",\"making application of\",\"the functions take 16 times as long.\",\"2\",\"p\",\"(\",\"1\",\"−\",\"(1\",\"−\",\"p\",\")\",\"4\",\")\",\"4\",\"0.1\",\"0.0140\",\"0.2\",\"0.1215\",\"0.3\",\"0.3334\",\"0.4\",\"0.5740\",\"0.5\",\"0.7725\",\"0.6\",\"0.9015\",\"0.7\",\"0.9680\",\"0.8\",\"0.9936\",\"Figure 3.11: Effect of the 4-way OR-construction followed by\",\"the 4-way AND-\",\"construction\"],\"page\":30},{\"texts\":[\"3.6.  THE THEORY OF LOCALITY-SENSITIVE FUNCTIONS\",\"85\",\"Example 3.19:\",\"For the same cost, we can apply a 4-way OR-construction\",\"followed by a 4-way AND-construction. Figure 3.11 gives the\",\"transformation\",\"on probabilities implied by this construction. For instanc\",\"e, suppose that\",\"F\",\"is a\",\"(0\",\".\",\"2\",\",\",\"0\",\".\",\"6\",\",\",\"0\",\".\",\"8\",\",\",\"0\",\".\",\"4)-sensitive family. Then the constructed family is a\",\"(0\",\".\",\"2\",\",\",\"0\",\".\",\"6\",\",\",\"0\",\".\",\"9936\",\",\",\"0\",\".\",\"5740)-sensitive\",\"family. This choice is not necessarily the best. Although th\",\"e higher probability\",\"has moved much closer to 1, the lower probability has also rai\",\"sed, increasing\",\"the number of false positives.\",\"2\",\"Example 3.20:\",\"We can cascade constructions as much as we like. For exam-\",\"ple, we could use the construction of Example 3.18 on the fami\",\"ly of minhash\",\"functions and then use the construction of Example 3.19 on th\",\"e resulting family.\",\"The constructed family would then have functions each built\",\"from 256 minhash\",\"functions. It would, for instance transform a (0\",\".\",\"2\",\",\",\"0\",\".\",\"8\",\",\",\"0\",\".\",\"8\",\",\",\"0\",\".\",\"2)-sensitive family\",\"into a (0\",\".\",\"2\",\",\",\"0\",\".\",\"8\",\",\",\"0\",\".\",\"99999996\",\",\",\"0\",\".\",\"0008715)-sensitive family.\",\"2\",\"3.6.4  Exercises for Section 3.6\",\"Exercise 3.6.1:\",\"What is the effect on probability of starting with the family\",\"of minhash functions and applying:\",\"(a) A 2-way AND construction followed by a 3-way OR construct\",\"ion.\",\"(b) A 3-way OR construction followed by a 2-way AND construct\",\"ion.\",\"(c) A 2-way AND construction followed by a 2-way OR construct\",\"ion, followed\",\"by a 2-way AND construction.\",\"(d) A 2-way OR construction followed by a 2-way AND construct\",\"ion, followed\",\"by a 2-way OR construction followed by a 2-way AND constructi\",\"on.\",\"Exercise 3.6.2:\",\"Find the fixedpoints for each of the functions constructed in\",\"Exercise 3.6.1.\",\"! Exercise 3.6.3:\",\"Any function of probability\",\"p\",\", such as that of Fig. 3.10, has\",\"a slope given by the derivative of the function. The maximum s\",\"lope is where\",\"that derivative is a maximum. Find the value of\",\"p\",\"that gives a maximum slope\",\"for the S-curves given by Fig. 3.10 and Fig. 3.11. What are the\",\"values of these\",\"maximum slopes?\",\"!! Exercise 3.6.4:\",\"Generalize Exercise 3.6.3 to give, as a function of\",\"r\",\"and\",\"b\",\", the\",\"point of maximum slope and the value of that slope, for famili\",\"es of functions\",\"defined from the minhash functions by:\",\"(a) An\",\"r\",\"-way AND construction followed by a\",\"b\",\"-way OR construction.\",\"(b) A\",\"b\",\"-way OR construction followed by an\",\"r\",\"-way AND construction.\"],\"page\":31},{\"texts\":[\"86\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"3.7  LSH Families for Other Distance Measures\",\"There is no guarantee that a distance measure has a locality-\",\"sensitive family of\",\"hash functions. So far, we have only seen such families for th\",\"e Jaccard distance.\",\"In this section, we shall show how to construct locality-sen\",\"sitive families for\",\"Hamming distance, the cosine distance and for the normal Euc\",\"lidean distance.\",\"3.7.1  LSH Families for Hamming Distance\",\"It is quite simple to build a locality-sensitive family of fu\",\"nctions for the Ham-\",\"ming distance. Suppose we have a space of\",\"d\",\"-dimensional vectors, and\",\"h\",\"(\",\"x, y\",\")\",\"denotes the Hamming distance between vectors\",\"x\",\"and\",\"y\",\". If we take any one\",\"position of the vectors, say the\",\"i\",\"th position, we can define the function\",\"f\",\"i\",\"(\",\"x\",\")\",\"to be the\",\"i\",\"th bit of vector\",\"x\",\". Then\",\"f\",\"i\",\"(\",\"x\",\") =\",\"f\",\"i\",\"(\",\"y\",\") if and only if vectors\",\"x\",\"and\",\"y\",\"agree in the\",\"i\",\"th position. Then the probability that\",\"f\",\"i\",\"(\",\"x\",\") =\",\"f\",\"i\",\"(\",\"y\",\") for a ran-\",\"domly chosen\",\"i\",\"is exactly 1\",\"−\",\"h\",\"(\",\"x, y\",\")\",\"/d\",\"; i.e., it is the fraction of positions in\",\"which\",\"x\",\"and\",\"y\",\"agree.\",\"This situation is almost exactly like the one we encountered\",\"for minhashing.\",\"Thus, the family\",\"F\",\"consisting of the functions\",\"{\",\"f\",\"1\",\", f\",\"2\",\", . . . , f\",\"d\",\"}\",\"is a\",\"(\",\"d\",\"1\",\", d\",\"2\",\",\",\"1\",\"−\",\"d\",\"1\",\"/d,\",\"1\",\"−\",\"d\",\"2\",\"/d\",\")-sensitive\",\"family of hash functions, for any\",\"d\",\"1\",\"\u003c  d\",\"2\",\".  There are only two differences\",\"between this family and the family of minhash functions.\",\"1. While Jaccard distance runs from 0 to 1, the Hamming distan\",\"ce on a\",\"vector space of dimension\",\"d\",\"runs from 0 to\",\"d\",\". It is therefore necessary to\",\"scale the distances by dividing by\",\"d\",\", to turn them into probabilities.\",\"2. While there is essentially an unlimited supply of minhash\",\"functions, the\",\"size of the family\",\"F\",\"for Hamming distance is only\",\"d\",\".\",\"The first point is of no consequence; it only requires that we d\",\"ivide by\",\"d\",\"at\",\"appropriate times. The second point is more serious. If\",\"d\",\"is relatively small,\",\"then we are limited in the number of functions that can be comp\",\"osed using\",\"the AND and OR constructions, thereby limiting how steep we c\",\"an make the\",\"S-curve be.\",\"3.7.2  Random Hyperplanes and the Cosine Distance\",\"Recall from Section 3.5.4 that the cosine distance between t\",\"wo vectors is the\",\"angle between the vectors.  For instance, we see in Fig. 3.12 t\",\"wo vectors\",\"x\",\"and\",\"y\",\"that make an angle\",\"θ\",\"between them. Note that these vectors may be\",\"in a space of many dimensions, but they always define a plane, a\",\"nd the angle\",\"between them is measured in this plane. Figure 3.12 is a “top-\",\"view” of the\",\"plane containing\",\"x\",\"and\",\"y\",\".\"],\"page\":32},{\"texts\":[\"3.7.  LSH FAMILIES FOR OTHER DISTANCE MEASURES\",\"87\",\"θ\",\"x\",\"y\",\"Figure 3.12: Two vectors make an angle\",\"θ\",\"Suppose we pick a hyperplane through the origin. This hyperp\",\"lane intersects\",\"the plane of\",\"x\",\"and\",\"y\",\"in a line. Figure 3.12 suggests two possible hyperplanes,\",\"one whose intersection is the dashed line and the other’s int\",\"ersection is the\",\"dotted line. To pick a random hyperplane, we actually pick th\",\"e normal vector\",\"to the hyperplane, say\",\"v\",\". The hyperplane is then the set of points whose dot\",\"product with\",\"v\",\"is 0.\",\"First, consider a vector\",\"v\",\"that is normal to the hyperplane whose projection\",\"is represented by the dashed line in Fig. 3.12; that is,\",\"x\",\"and\",\"y\",\"are on different\",\"sides of the hyperplane. Then the dot products\",\"v.x\",\"and\",\"v.y\",\"will have different\",\"signs. If we assume, for instance, that\",\"v\",\"is a vector whose projection onto the\",\"plane of\",\"x\",\"and\",\"y\",\"is above the dashed line in Fig. 3.12, then\",\"v.x\",\"is positive,\",\"while\",\"v.y\",\"is negative. The normal vector\",\"v\",\"instead might extend in the opposite\",\"direction, below the dashed line. In that case\",\"v.x\",\"is negative and\",\"v.y\",\"is positive,\",\"but the signs are still different.\",\"On the other hand, the randomly chosen vector\",\"v\",\"could be normal to a\",\"hyperplane like the dotted line in Fig. 3.12. In that case, bo\",\"th\",\"v.x\",\"and\",\"v.y\",\"have the same sign. If the projection of\",\"v\",\"extends to the right, then both dot\",\"products are positive, while if\",\"v\",\"extends to the left, then both are negative.\",\"What is the probability that the randomly chosen vector is no\",\"rmal to a\",\"hyperplane that looks like the dashed line rather than the do\",\"tted line?  All\",\"angles for the line that is the intersection of the random hyp\",\"erplane and the\",\"plane of\",\"x\",\"and\",\"y\",\"are equally likely.  Thus, the hyperplane will look like the\",\"dashed line with probability\",\"θ/\",\"180 and will look like the dotted line otherwise.\",\"Thus, each hash function\",\"f\",\"in our locality-sensitive family\",\"F\",\"is built from\",\"a randomly chosen vector\",\"v\",\"f\",\". Given two vectors\",\"x\",\"and\",\"y\",\", say\",\"f\",\"(\",\"x\",\") =\",\"f\",\"(\",\"y\",\") if\",\"and only if the dot products\",\"v\",\"f\",\".x\",\"and\",\"v\",\"f\",\".y\",\"have the same sign. Then\",\"F\",\"is a\",\"locality-sensitive family for the cosine distance. The par\",\"ameters are essentially\"],\"page\":33},{\"texts\":[\"88\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"the same as for the Jaccard-distance family described in Sec\",\"tion 3.6.2, except\",\"the scale of distances is 0–180 rather than 0–1. That is,\",\"F\",\"is a\",\"(\",\"d\",\"1\",\", d\",\"2\",\",\",\"(180\",\"−\",\"d\",\"1\",\")\",\"/\",\"180\",\", d\",\"2\",\"/\",\"180)-sensitive\",\"family of hash functions. From this basis, we can amplify the\",\"family as we wish,\",\"just as for the minhash-based family.\",\"3.7.3  Sketches\",\"Instead of chosing a random vector from all possible vectors\",\", it turns out to be\",\"sufficiently random if we restrict our choice to vectors whose\",\"components are\",\"+1 and\",\"−\",\"1. The dot product of any vector\",\"x\",\"with a vector\",\"v\",\"of +1’s and\",\"−\",\"1’s\",\"is formed by adding the components of\",\"x\",\"where\",\"v\",\"is +1 and then subtracting\",\"the other components of\",\"x\",\"– those where\",\"v\",\"is\",\"−\",\"1.\",\"If we pick a collection of random vectors, say\",\"v\",\"1\",\", v\",\"2\",\", . . . , v\",\"n\",\", then we can\",\"apply them to an arbitrary vector\",\"x\",\"by computing\",\"v\",\"1\",\".x, v\",\"2\",\".x, . . . , v\",\"n\",\".x\",\"and then\",\"replacing any positive value by +1 and any negative value by\",\"−\",\"1. The result is\",\"called the\",\"sketch\",\"of\",\"x\",\". You can handle 0’s arbitrarily, e.g., by chosing a result +1\",\"or\",\"−\",\"1 at random. Since there is only a tiny probability of a zero do\",\"t product,\",\"the choice has essentially no effect.\",\"Example 3.21:\",\"Suppose our space consists of 4-dimensional vectors, and we\",\"pick three random vectors:\",\"v\",\"1\",\"= [+1\",\",\",\"−\",\"1\",\",\",\"+1\",\",\",\"+1],\",\"v\",\"2\",\"= [\",\"−\",\"1\",\",\",\"+1\",\",\",\"−\",\"1\",\",\",\"+1], and\",\"v\",\"3\",\"= [+1\",\",\",\"+1\",\",\",\"−\",\"1\",\",\",\"−\",\"1]. For the vector\",\"x\",\"= [3\",\",\",\"4\",\",\",\"5\",\",\",\"6], the sketch is [+1\",\",\",\"+1\",\",\",\"−\",\"1].\",\"That is,\",\"v\",\"1\",\".x\",\"= 3\",\"−\",\"4+5+6 = 10. Since the result is positive, the first component\",\"of the sketch is +1. Similarly,\",\"v\",\"2\",\".x\",\"= 3 and\",\"v\",\"3\",\".x\",\"=\",\"−\",\"4, so the second component\",\"of the sketch is +1 and the third component is\",\"−\",\"1.\",\"Consider the vector\",\"y\",\"= [4\",\",\",\"3\",\",\",\"2\",\",\",\"1]. We can similarly compute its sketch to\",\"be [+1\",\",\",\"−\",\"1\",\",\",\"+1]. Since the sketches for\",\"x\",\"and\",\"y\",\"agree in 1/3 of the positions,\",\"we estimate that the angle between them is 120 degrees. That i\",\"s, a randomly\",\"chosen hyperplane is twice as likely to look like the dashed l\",\"ine in Fig. 3.12 than\",\"like the dotted line.\",\"The above conclusion turns out to be quite wrong. We can calcu\",\"late the\",\"cosine of the angle between\",\"x\",\"and\",\"y\",\"to be\",\"x.y\",\", which is\",\"6\",\"×\",\"1 + 5\",\"×\",\"2 + 4\",\"×\",\"3 + 3\",\"×\",\"4 = 40\",\"divided by the magnitudes of the two vectors. These magnitud\",\"es are\",\"√\",\"6\",\"2\",\"+ 5\",\"2\",\"+ 4\",\"2\",\"+ 3\",\"2\",\"= 9\",\".\",\"274\",\"and\",\"√\",\"1\",\"2\",\"+ 2\",\"2\",\"+ 3\",\"2\",\"+ 4\",\"2\",\"= 5\",\".\",\"477. Thus, the cosine of the angle between\",\"x\",\"and\",\"y\",\"is 0.7875, and this angle is about 38 degrees.  However, if you\",\"look at all\",\"16 different vectors\",\"v\",\"of length 4 that have +1 and\",\"−\",\"1 as components, you\",\"find that there are only four of these whose dot products with\",\"x\",\"and\",\"y\",\"have\",\"a different sign, namely\",\"v\",\"2\",\",\",\"v\",\"3\",\", and their complements [+1\",\",\",\"−\",\"1\",\",\",\"+1\",\",\",\"−\",\"1] and\",\"[\",\"−\",\"1\",\",\",\"−\",\"1\",\",\",\"+1\",\",\",\"+1]. Thus, had we picked all sixteen of these vectors to form a\",\"sketch, the estimate of the angle would have been 180\",\"/\",\"4 = 45 degrees.\",\"2\"],\"page\":34},{\"texts\":[\"3.7.  LSH FAMILIES FOR OTHER DISTANCE MEASURES\",\"89\",\"3.7.4  LSH Families for Euclidean Distance\",\"Now, let us turn to the Euclidean distance (Section 3.5.2), a\",\"nd see if we can\",\"develop a locality-sensitive family of hash functions for t\",\"his distance. We shall\",\"start with a 2-dimensional Euclidean space. Each hash funct\",\"ion\",\"f\",\"in our family\",\"F\",\"will be associated with a randomly chosen line in this space.\",\"Pick a constant\",\"a\",\"and divide the line into segments of length\",\"a\",\", as suggested by Fig. 3.13, where\",\"the “random” line has been oriented to be horizontal.\",\"θ\",\"Points at\",\"distance\",\"Bucket\",\"width\",\"a\",\"d\",\"Figure 3.13: Two points at distance\",\"d\",\"≫\",\"a\",\"have a small chance of being hashed\",\"to the same bucket\",\"The segments of the line are the buckets into which function\",\"f\",\"hashes points.\",\"A point is hashed to the bucket in which its projection onto th\",\"e line lies. If the\",\"distance\",\"d\",\"between two points is small compared with\",\"a\",\", then there is a good\",\"chance the two points hash to the same bucket, and thus the has\",\"h function\",\"f\",\"will declare the two points equal. For example, if\",\"d\",\"=\",\"a/\",\"2, then there is at least\",\"a 50% chance the two points will fall in the same bucket. In fac\",\"t, if the angle\",\"θ\",\"between the randomly chosen line and the line connecting the\",\"points is large,\",\"then there is an even greater chance that the two points will f\",\"all in the same\",\"bucket. For instance, if\",\"θ\",\"is 90 degrees, then the two points are certain to fall\",\"in the same bucket.\",\"However, suppose\",\"d\",\"is larger than\",\"a\",\". In order for there to be any chance of\",\"the two points falling in the same bucket, we need\",\"d\",\"cos\",\"θ\",\"≤\",\"a\",\". The diagram of\",\"Fig. 3.13 suggests why this requirement holds. Note that eve\",\"n if\",\"d\",\"cos\",\"θ\",\"≪\",\"a\",\"it\",\"is still not certain that the two points will fall in the same b\",\"ucket. However,\",\"we can guarantee the following. If\",\"d\",\"≥\",\"2\",\"a\",\", then there is no more than a 1/3\",\"chance the two points fall in the same bucket. The reason is th\",\"at for cos\",\"θ\",\"to\",\"be less than 1/2, we need to have\",\"θ\",\"in the range 60 to 90 degrees. If\",\"θ\",\"is in the\",\"range 0 to 60 degrees, then cos\",\"θ\",\"is more than 1/2. But since\",\"θ\",\"is the smaller\",\"angle between two randomly chosen lines in the plane,\",\"θ\",\"is twice as likely to be\",\"between 0 and 60 as it is to be between 60 and 90.\"],\"page\":35},{\"texts\":[\"90\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"We conclude that the family\",\"F\",\"just described forms a (\",\"a/\",\"2\",\",\",\"2\",\"a,\",\"1\",\"/\",\"2\",\",\",\"1\",\"/\",\"3)-\",\"sensitive family of hash functions. That is, for distances u\",\"p to\",\"a/\",\"2 the proba-\",\"bility is at least 1/2 that two points at that distance will fa\",\"ll in the same bucket,\",\"while for distances at least 2\",\"a\",\"the probability points at that distance will fall in\",\"the same bucket is at most 1/3. We can amplify this family as we\",\"like, just as\",\"for the other examples of locality-sensitive hash function\",\"s we have discussed.\",\"3.7.5  More LSH Families for Euclidean Spaces\",\"There is something unsatisfying about the family of hash fun\",\"ctions developed\",\"in Section 3.7.4. First, the technique was only described fo\",\"r two-dimensional\",\"Euclidean spaces. What happens if our data is points in a spac\",\"e with many\",\"dimensions? Second, for Jaccard and cosine distances, we we\",\"re able to develop\",\"locality-sensitive families for any pair of distances\",\"d\",\"1\",\"and\",\"d\",\"2\",\"as long as\",\"d\",\"1\",\"\u003c d\",\"2\",\".\",\"In Section 3.7.4 we appear to need the stronger condition\",\"d\",\"1\",\"\u003c\",\"4\",\"d\",\"2\",\".\",\"However, we claim that there is a locality-sensitive family\",\"of hash func-\",\"tions for any\",\"d\",\"1\",\"\u003c d\",\"2\",\"and for any number of dimensions. The family’s hash\",\"functions still derive from random lines through the space a\",\"nd a bucket size\",\"a\",\"that partitions the line. We still hash points by projecting\",\"them onto the\",\"line. Given that\",\"d\",\"1\",\"\u003c d\",\"2\",\", we may not know what the probability\",\"p\",\"1\",\"is that two\",\"points at distance\",\"d\",\"1\",\"hash to the same bucket, but we can be certain that it\",\"is greater than\",\"p\",\"2\",\", the probability that two points at distance\",\"d\",\"2\",\"hash to the\",\"same bucket. The reason is that this probability surely grow\",\"s as the distance\",\"shrinks. Thus, even if we cannot calculate\",\"p\",\"1\",\"and\",\"p\",\"2\",\"easily, we know that there\",\"is a (\",\"d\",\"1\",\", d\",\"2\",\", p\",\"1\",\", p\",\"2\",\")-sensitive family of hash functions for any\",\"d\",\"1\",\"\u003c d\",\"2\",\"and any\",\"given number of dimensions.\",\"Using the amplification techniques of Section 3.6.3, we can t\",\"hen adjust the\",\"two probabilities to surround any particular value we like,\",\"and to be as far apart\",\"as we like. Of course, the further apart we want the probabili\",\"ties to be, the\",\"larger the number of basic hash functions in\",\"F\",\"we must use.\",\"3.7.6  Exercises for Section 3.7\",\"Exercise 3.7.1:\",\"Suppose we construct the basic family of six locality-sensi\",\"tive\",\"functions for vectors of length six. For each pair of the vect\",\"ors 000000, 110011,\",\"010101, and 011100, which of the six functions makes them can\",\"didates?\",\"Exercise 3.7.2:\",\"Let us compute sketches using the following four “random”\",\"vectors:\",\"v\",\"1\",\"= [+1\",\",\",\"+1\",\",\",\"+1\",\",\",\"−\",\"1]\",\"v\",\"2\",\"= [+1\",\",\",\"+1\",\",\",\"−\",\"1\",\",\",\"+1]\",\"v\",\"3\",\"= [+1\",\",\",\"−\",\"1\",\",\",\"+1\",\",\",\"+1]\",\"v\",\"4\",\"= [\",\"−\",\"1\",\",\",\"+1\",\",\",\"+1\",\",\",\"+1]\",\"Compute the sketches of the following vectors.\",\"(a) [2\",\",\",\"3\",\",\",\"4\",\",\",\"5].\"],\"page\":36},{\"texts\":[\"3.8.  APPLICATIONS OF LOCALITY-SENSITIVE HASHING\",\"91\",\"(b) [\",\"−\",\"2\",\",\",\"3\",\",\",\"−\",\"4\",\",\",\"5].\",\"(c) [2\",\",\",\"−\",\"3\",\",\",\"4\",\",\",\"−\",\"5].\",\"For each pair, what is the estimated angle between them, acco\",\"rding to the\",\"sketches? What are the true angles?\",\"Exercise 3.7.3:\",\"Suppose we form sketches by using all sixteen of the vectors\",\"of length 4, whose components are each +1 or\",\"−\",\"1. Compute the sketches of\",\"the three vectors in Exercise 3.7.2. How do the estimates of t\",\"he angles between\",\"each pair compare with the true angles?\",\"Exercise 3.7.4:\",\"Suppose we form sketches using the four vectors from Exer-\",\"cise 3.7.2.\",\"!\",\"(a) What are the constraints on\",\"a\",\",\",\"b\",\",\",\"c\",\", and\",\"d\",\"that will cause the sketch of\",\"the vector [\",\"a, b, c, d\",\"] to be [+1\",\",\",\"+1\",\",\",\"+1\",\",\",\"+1]?\",\"!!\",\"(b) Consider two vectors [\",\"a, b, c, d\",\"] and [\",\"e, f, g, h\",\"]. What are the conditions on\",\"a, b, . . . , h\",\"that will make the sketches of these two vectors be the same?\",\"Exercise 3.7.5:\",\"Suppose we have points in a 3-dimensional Euclidean space:\",\"p\",\"1\",\"= (1\",\",\",\"2\",\",\",\"3),\",\"p\",\"2\",\"= (0\",\",\",\"2\",\",\",\"4), and\",\"p\",\"3\",\"= (4\",\",\",\"3\",\",\",\"2). Consider the three hash functions\",\"defined by the three axes (to make our calculations very easy)\",\". Let buckets be\",\"of length\",\"a\",\", with one bucket the interval [0\",\", a\",\") (i.e., the set of points\",\"x\",\"such that\",\"0\",\"≤\",\"x \u003c a\",\"), the next [\",\"a,\",\"2\",\"a\",\"), the previous one [\",\"−\",\"a,\",\"0), and so on.\",\"(a) For each of the three lines, assign each of the points to bu\",\"ckets, assuming\",\"a\",\"= 1.\",\"(b) Repeat part (a), assuming\",\"a\",\"= 2.\",\"(c) What are the candidate pairs for the cases\",\"a\",\"= 1 and\",\"a\",\"= 2?\",\"!\",\"(d) For each pair of points, for what values of\",\"a\",\"will that pair be a candidate\",\"pair?\",\"3.8  Applications of Locality-Sensitive Hashing\",\"In this section, we shall explore three examples of how LSH is\",\"used in practice.\",\"In each case, the techniques we have learned must be modified t\",\"o meet certain\",\"constraints of the problem. The three subjects we cover are:\",\"1.\",\"Entity Resolution\",\": This term refers to matching data records that refer to\",\"the same real-world entity, e.g., the same person. The princ\",\"ipal problem\",\"addressed here is that the similarity of records does not mat\",\"ch exactly\",\"either the similar-sets or similar-vectors models of simil\",\"arity on which the\",\"theory is built.\"],\"page\":37},{\"texts\":[\"92\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"2.\",\"Matching Fingerprints\",\": It is possible to represent fingerprints as sets.\",\"However, we shall explore a different family of locality-sen\",\"sitive hash func-\",\"tions from the one we get by minhashing.\",\"3.\",\"Matching Newspaper Articles\",\":  Here, we consider a different notion of\",\"shingling that focuses attention on the core article in an on\",\"-line news-\",\"paper’s Web page, ignoring all the extraneous material such\",\"as ads and\",\"newspaper-specific material.\",\"3.8.1  Entity Resolution\",\"It is common to have several data sets available, and to know t\",\"hat they refer to\",\"some of the same entities. For example, several different bib\",\"liographic sources\",\"provide information about many of the same books or papers. I\",\"n the general\",\"case, we have records describing entities of some type, such\",\"as people or books.\",\"The records may all have the same format, or they may have diffe\",\"rent formats,\",\"with different kinds of information.\",\"There are many reasons why information about an entity may va\",\"ry, even if\",\"the field in question is supposed to be the same. For example, n\",\"ames may be\",\"expressed differently in different records because of misspe\",\"llings, absence of a\",\"middle initial, use of a nickname, and many other reasons. Fo\",\"r example, “Bob\",\"S. Jomes” and “Robert Jones Jr.” may or may not be the same pers\",\"on. If\",\"records come from different sources, the fields may differ as we\",\"ll. One source’s\",\"records may have an “age” field, while another does not. The se\",\"cond source\",\"might have a “date of birth” field, or it may have no informatio\",\"n at all about\",\"when a person was born.\",\"3.8.2  An Entity-Resolution Example\",\"We shall examine a real example of how LSH was used to deal with\",\"an entity-\",\"resolution problem. Company A was engaged by Company B to sol\",\"icit cus-\",\"tomers for B. Company B would pay A a yearly fee, as long as the c\",\"ustomer\",\"maintained their subscription.  They later quarreled and di\",\"sagreed over how\",\"many customers A had provided to B. Each had about 1,000,000 r\",\"ecords, some\",\"of which described the same people; those were the customers\",\"A had provided\",\"to B. The records had different data fields, but unfortunately\",\"none of those\",\"fields was “this is a customer that A had provided to B.” Thus, t\",\"he problem\",\"was to match records from the two sets to see if a pair represen\",\"ted the same\",\"person.\",\"Each record had fields for the name, address, and phone number\",\"of the\",\"person. However, the values in these fields could differ for ma\",\"ny reasons. Not\",\"only were there the misspellings and other naming difference\",\"s mentioned in\",\"Section 3.8.1, but there were other opportunities to disagr\",\"ee as well. A customer\",\"might give their home phone to A and their cell phone to B. Or th\",\"ey might\",\"move, and tell B but not A (because they no longer had need for a\",\"relationship\",\"with A). Area codes of phones sometimes change.\"],\"page\":38},{\"texts\":[\"3.8.  APPLICATIONS OF LOCALITY-SENSITIVE HASHING\",\"93\",\"The strategy for identifying records involved scoring the d\",\"ifferences in three\",\"fields: name, address, and phone. To create a\",\"score\",\"describing the likelihood\",\"that two records, one from A and the other from B, described th\",\"e same per-\",\"son, 100 points was assigned to each of the three fields, so rec\",\"ords with exact\",\"matches in all three fields got a score of 300. However, there w\",\"ere deductions for\",\"mismatches in each of the three fields. As a first approximatio\",\"n, edit-distance\",\"(Section 3.5.5) was used, but the penalty grew quadraticall\",\"y with the distance.\",\"Then, certain publicly available tables were used to reduce\",\"the penalty in ap-\",\"propriate situations. For example, “Bill” and “William” we\",\"re treated as if they\",\"differed in only one letter, even though their edit-distance\",\"is 5.\",\"However, it is not feasible to score all one trillion pairs of\",\"records. Thus,\",\"a simple LSH was used to focus on likely candidates. Three “ha\",\"sh functions”\",\"were used. The first sent records to the same bucket only if the\",\"y had identical\",\"names; the second did the same but for identical addresses, a\",\"nd the third did\",\"the same for phone numbers.  In practice, there was no hashing\",\"; rather the\",\"records were sorted by name, so records with identical names\",\"would appear\",\"consecutively and get scored for overall similarity of the n\",\"ame, address, and\",\"phone.  Then the records were sorted by address, and those wit\",\"h the same\",\"address were scored. Finally, the records were sorted a thir\",\"d time by phone,\",\"and records with identical phones were scored.\",\"This approach missed a record pair that truly represented th\",\"e same person\",\"but none of the three fields matched exactly. Since the goal wa\",\"s to prove in\",\"a court of law that the persons were the same, it is unlikely th\",\"at such a pair\",\"would have been accepted by a judge as sufficiently similar any\",\"way.\",\"3.8.3  Validating Record Matches\",\"What remains is to determine how high a score indicates that t\",\"wo records truly\",\"represent the same individual.  In the example at hand, there\",\"was an easy\",\"way to make that decision, and the technique can be applied in\",\"many similar\",\"situations. It was decided to look at the creation-dates for\",\"the records at hand,\",\"and to assume that 90 days was an absolute maximum delay betwe\",\"en the time\",\"the service was bought at Company A and registered at B. Thus,\",\"a proposed\",\"match between two records that were chosen at random, subjec\",\"t only to the\",\"constraint that the date on the B-record was between 0 and 90 d\",\"ays after the\",\"date on the A-record, would have an average delay of 45 days.\",\"It was found that of the pairs with a perfect 300 score, the ave\",\"rage delay was\",\"10 days. If you assume that 300-score pairs are surely correc\",\"t matches, then you\",\"can look at the pool of pairs with any given score\",\"s\",\", and compute the average\",\"delay of those pairs. Suppose that the average delay is\",\"x\",\", and the fraction of\",\"true matches among those pairs with score\",\"s\",\"is\",\"f\",\". Then\",\"x\",\"= 10\",\"f\",\"+ 45(1\",\"−\",\"f\",\"),\",\"or\",\"x\",\"= 45\",\"−\",\"35\",\"f\",\". Solving for\",\"f\",\", we find that the fraction of the pairs with score\",\"s\",\"that are truly matches is (45\",\"−\",\"x\",\")\",\"/\",\"35.\",\"The same trick can be used whenever:\"],\"page\":39},{\"texts\":[\"94\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"When Are Record Matches Good Enough?\",\"While every case will be different, it may be of interest to kno\",\"w how the\",\"experiment of Section 3.8.3 turned out on the data of Section\",\"3.8.2. For\",\"scores down to 185, the value of\",\"x\",\"was very close to 10; i.e., these scores\",\"indicated that the likelihood of the records representing t\",\"he same person\",\"was essentially 1. Note that a score of 185 in this example rep\",\"resents a\",\"situation where one field is the same (as would have to be the ca\",\"se, or the\",\"records would never even be scored), one field was completely\",\"different,\",\"and the third field had a small discrepancy. Moreover, for sco\",\"res as low as\",\"115, the value of\",\"x\",\"was noticeably less than 45, meaning that some of these\",\"pairs did represent the same person. Note that a score of 115 r\",\"epresents\",\"a case where one field is the same, but there is only a slight sim\",\"ilarity in\",\"the other two fields.\",\"1. There is a scoring system used to evaluate the likelihood t\",\"hat two records\",\"represent the same entity, and\",\"2. There is some field, not used in the scoring, from which we ca\",\"n derive a\",\"measure that differs, on average, for true pairs and false pai\",\"rs.\",\"For instance, suppose there were a “height” field recorded by\",\"both companies\",\"A and B in our running example. We can compute the average diffe\",\"rence in\",\"height for pairs of random records, and we can compute the ave\",\"rage difference in\",\"height for records that have a perfect score (and thus surely\",\"represent the same\",\"entities). For a given score\",\"s\",\", we can evaluate the average height difference of the\",\"pairs with that score and estimate the probability of the rec\",\"ords representing\",\"the same entity. That is, if\",\"h\",\"0\",\"is the average height difference for the perfect\",\"matches,\",\"h\",\"1\",\"is the average height difference for random pairs, and\",\"h\",\"is the\",\"average height difference for pairs of score\",\"s\",\", then the fraction of good pairs\",\"with score\",\"s\",\"is (\",\"h\",\"1\",\"−\",\"h\",\")\",\"/\",\"(\",\"h\",\"1\",\"−\",\"h\",\"0\",\").\",\"3.8.4  Matching Fingerprints\",\"When fingerprints are matched by computer, the usual represe\",\"ntation is not\",\"an image, but a set of locations in which\",\"minutiae\",\"are located.  A minutia,\",\"in the context of fingerprint descriptions, is a place where s\",\"omething unusual\",\"happens, such as two ridges merging or a ridge ending. If we pl\",\"ace a grid over a\",\"fingerprint, we can represent the fingerprint by the set of gri\",\"d squares in which\",\"minutiae are located.\",\"Ideally, before overlaying the grid, fingerprints are norma\",\"lized for size and\",\"orientation, so that if we took two images of the same finger, w\",\"e would find\",\"minutiae lying in exactly the same grid squares. We shall not\",\"consider here\",\"the best ways to normalize images. Let us assume that some com\",\"bination of\"],\"page\":40},{\"texts\":[\"3.8.  APPLICATIONS OF LOCALITY-SENSITIVE HASHING\",\"95\",\"techniques, including choice of grid size and placing a minu\",\"tia in several adjacent\",\"grid squares if it lies close to the border of the squares enab\",\"les us to assume\",\"that grid squares from two images have a significantly higher\",\"probability of\",\"agreeing in the presence or absence of a minutia than if they w\",\"ere from images\",\"of different fingers.\",\"Thus, fingerprints can be represented by sets of grid squares\",\"– those where\",\"their minutiae are located – and compared like any sets, usin\",\"g the Jaccard sim-\",\"ilarity or distance. There are two versions of fingerprint co\",\"mparison, however.\",\"•\",\"The\",\"many-one\",\"problem is the one we typically expect. A fingerprint has\",\"been found on a gun, and we want to compare it with all the finger\",\"prints\",\"in a large database, to see which one matches.\",\"•\",\"The\",\"many-many\",\"version of the problem is to take the entire database, and\",\"see if there are any pairs that represent the same individual\",\".\",\"While the many-many version matches the model that we have be\",\"en following\",\"for finding similar items, the same technology can be used to s\",\"peed up the\",\"many-one problem.\",\"3.8.5  A LSH Family for Fingerprint Matching\",\"We could minhash the sets that represent a fingerprint, and us\",\"e the standard\",\"LSH technique from Section 3.4.  However, since the sets are c\",\"hosen from a\",\"relatively small set of grid points (perhaps 1000), the need\",\"to minhash them\",\"into more succinct signatures is not clear. We shall study he\",\"re another form of\",\"locality-sensitive hashing that works well for data of the t\",\"ype we are discussing.\",\"Suppose for an example that the probability of finding a minut\",\"ia in a random\",\"grid square of a random fingerprint is 20%. Also, assume that i\",\"f two fingerprints\",\"come from the same finger, and one has a minutia in a given grid s\",\"quare, then\",\"the probability that the other does too is 80%. We can define a l\",\"ocality-sensitive\",\"family of hash functions as follows. Each function\",\"f\",\"in this family\",\"F\",\"is defined\",\"by three grid squares. Function\",\"f\",\"says “yes” for two fingerprints if both have\",\"minutiae in all three grid squares, and otherwise\",\"f\",\"says “no.”  Put another\",\"way, we may imagine that\",\"f\",\"sends to a single bucket all fingerprints that have\",\"minutiae in all three of\",\"f\",\"’s grid points, and sends each other fingerprint to a\",\"bucket of its own. In what follows, we shall refer to the first o\",\"f these buckets as\",\"“the” bucket for\",\"f\",\"and ignore the buckets that are required to be singletons.\",\"If we want to solve the many-one problem, we can use many funct\",\"ions from\",\"the family\",\"F\",\"and precompute their buckets of fingerprints to which they an\",\"swer\",\"“yes.”  Then, given a new fingerprint that we want to match, we d\",\"etermine\",\"which of these buckets it belongs to and compare it with all th\",\"e fingerprints\",\"found in any of those buckets. To solve the many-many problem\",\", we compute\",\"the buckets for each of the functions and compare all fingerpr\",\"ints in each of the\",\"buckets.\"],\"page\":41},{\"texts\":[\"96\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"Let us consider how many functions we need to get a reasonable\",\"probability\",\"of catching a match, without having to compare the fingerprin\",\"t on the gun with\",\"each of the millions of fingerprints in the database. First, t\",\"he probability that\",\"two fingerprints from different fingers would be in the bucket f\",\"or a function\",\"f\",\"in\",\"F\",\"is (0\",\".\",\"2)\",\"6\",\"= 0\",\".\",\"000064. The reason is that they will both go into the bucket\",\"only if they each have a minutia in each of the three grid point\",\"s associated with\",\"f\",\", and the probability of each of those six independent events\",\"is 0.2.\",\"Now, consider the probability that two fingerprints from the\",\"same finger\",\"wind up in the bucket for\",\"f\",\".  The probability that the first fingerprint has\",\"minutiae in each of the three squares belonging to\",\"f\",\"is (0\",\".\",\"2)\",\"3\",\"= 0\",\".\",\"008. However,\",\"if it does, then the probability is (0\",\".\",\"8)\",\"3\",\"= 0\",\".\",\"512 that the other fingerprint\",\"will as well.  Thus, if the fingerprints are from the same finger\",\", there is a\",\"0\",\".\",\"008\",\"×\",\"0\",\".\",\"512 = 0\",\".\",\"004096 probability that they will both be in the bucket of\",\"f\",\".\",\"That is not much; it is about one in 200. However, if we use many\",\"functions\",\"from\",\"F\",\", but not too many, then we can get a good probability of matchi\",\"ng\",\"fingerprints from the same finger while not having too many fal\",\"se positives –\",\"fingerprints that must be considered but do not match.\",\"Example 3.22:\",\"For a specific example, let us suppose that we use 1024\",\"functions chosen randomly from\",\"F\",\".  Next, we shall construct a new fam-\",\"ily\",\"F\",\"1\",\"by performing a 1024-way OR on\",\"F\",\".  Then the probability that\",\"F\",\"1\",\"will put fingerprints from the same finger together in at least\",\"one bucket is\",\"1\",\"−\",\"(1\",\"−\",\"0\",\".\",\"004096)\",\"1024\",\"= 0\",\".\",\"985.  On the other hand, the probability that\",\"two fingerprints from different fingers will be placed in the sa\",\"me bucket is\",\"(1\",\"−\",\"(1\",\"−\",\"0\",\".\",\"000064)\",\"1024\",\"= 0\",\".\",\"063. That is, we get about 1.5% false negatives\",\"and about 6.3% false positives.\",\"2\",\"The result of Example 3.22 is not the best we can do. While it off\",\"ers only a\",\"1.5% chance that we shall fail to identify the fingerprint on t\",\"he gun, it does force\",\"us to look at 6.3% of the entire database. Increasing the numb\",\"er of functions\",\"from\",\"F\",\"will increase the number of false positives, with only a smal\",\"l benefit\",\"of reducing the number of false negatives below 1.5%. On the o\",\"ther hand, we\",\"can also use the AND construction, and in so doing, we can grea\",\"tly reduce\",\"the probability of a false positive, while making only a smal\",\"l increase in the\",\"false-negative rate. For instance, we could take 2048 funct\",\"ions from\",\"F\",\"in two\",\"groups of 1024. Construct the buckets for each of the functio\",\"ns. However, given\",\"a fingerprint\",\"P\",\"on the gun:\",\"1. Find the buckets from the first group in which\",\"P\",\"belongs, and take the\",\"union of these buckets.\",\"2. Do the same for the second group.\",\"3. Take the intersection of the two unions.\",\"4. Compare\",\"P\",\"only with those fingerprints in the intersection.\"],\"page\":42},{\"texts\":[\"3.8.  APPLICATIONS OF LOCALITY-SENSITIVE HASHING\",\"97\",\"Note that we still have to take unions and intersections of la\",\"rge sets of finger-\",\"prints, but we compare only a small fraction of those. It is th\",\"e comparison of\",\"fingerprints that takes the bulk of the time; in steps (1) and (\",\"2) fingerprints\",\"can be represented by their integer indices in the database.\",\"If we use this scheme, the probability of detecting a matchin\",\"g fingerprint\",\"is (0\",\".\",\"985)\",\"2\",\"= 0\",\".\",\"970; that is, we get about 3% false negatives. However, the\",\"probability of a false positive is (0\",\".\",\"063)\",\"2\",\"= 0\",\".\",\"00397. That is, we only have to\",\"examine about 1/250th of the database.\",\"3.8.6  Similar News Articles\",\"Our last case study concerns the problem of organizing a larg\",\"e repository of\",\"on-line news articles by grouping together Web pages that we\",\"re derived from\",\"the same basic text. It is common for organizations like The A\",\"ssociated Press\",\"to produce a news item and distribute it to many newspapers. E\",\"ach newspaper\",\"puts the story in its on-line edition, but surrounds it by inf\",\"ormation that is\",\"special to that newspaper, such as the name and address of the\",\"newspaper,\",\"links to related articles, and links to ads. In addition, it i\",\"s common for the\",\"newspaper to modify the article, perhaps by leaving off the la\",\"st few paragraphs\",\"or even deleting text from the middle. As a result, the same ne\",\"ws article can\",\"appear quite different at the Web sites of different newspaper\",\"s.\",\"The problem looks very much like the one that was suggested in\",\"Section 3.4:\",\"find documents whose shingles have a high Jaccard similarity\",\". Note that this\",\"problem is different from the problem of finding news articles\",\"that tell about the\",\"same events. The latter problem requires other techniques,\",\"typically examining\",\"the set of important words in the documents (a concept we disc\",\"ussed briefly\",\"in Section 1.3.1) and clustering them to group together diffe\",\"rent articles about\",\"the same topic.\",\"However, an interesting variation on the theme of shingling\",\"was found to be\",\"more effective for data of the type described. The problem is t\",\"hat shingling as\",\"we described it in Section 3.2 treats all parts of a document e\",\"qually. However,\",\"we wish to ignore parts of the document, such as ads or the head\",\"lines of other\",\"articles to which the newspaper added a link, that are not par\",\"t of the news\",\"article.  It turns out that there is a noticeable difference be\",\"tween text that\",\"appears in prose and text that appears in ads or headlines. Pr\",\"ose has a much\",\"greater frequency of stop words, the very frequent words suc\",\"h as “the” or “and.”\",\"The total number of words that are considered stop words vari\",\"es with the\",\"application, but it is common to use a list of several hundred\",\"of the most\",\"frequent words.\",\"Example 3.23:\",\"A typical ad might say simply “Buy Sudzo.” On the other\",\"hand, a prose version of the same thought that might appear in\",\"an article is\",\"“I recommend that you buy Sudzo for your laundry.” In the latt\",\"er sentence, it\",\"would be normal to treat “I,” “that,” “you,” “for,” and “your\",\"” as stop words.\",\"2\"],\"page\":43},{\"texts\":[\"98\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"Suppose we define a\",\"shingle\",\"to be a stop word followed by the next two\",\"words.  Then the ad “Buy Sudzo” from Example 3.23 has no shingl\",\"es and\",\"would not be reflected in the representation of the Web page co\",\"ntaining that\",\"ad. On the other hand, the sentence from Example 3.23 would be\",\"represented\",\"by five shingles: “I recommend that,” “that you buy,” “you buy\",\"Sudzo,” “for\",\"your laundry,” and “your laundry\",\"x\",\",” where\",\"x\",\"is whatever word follows that\",\"sentence.\",\"Suppose we have two Web pages, each of which consists of half n\",\"ews text\",\"and half ads or other material that has a low density of stop wo\",\"rds. If the news\",\"text is the same but the surrounding material is different, th\",\"en we would expect\",\"that a large fraction of the shingles of the two pages would be\",\"the same. They\",\"might have a Jaccard similarity of 75%. However, if the surro\",\"unding material\",\"is the same but the news content is different, then the number o\",\"f common\",\"shingles would be small, perhaps 25%.  If we were to use the con\",\"ventional\",\"shingling, where shingles are (say) sequences of 10 consecu\",\"tive characters, we\",\"would expect the two documents to share half their shingles (\",\"i.e., a Jaccard\",\"similarity of 1/3), regardless of whether it was the news or t\",\"he surrounding\",\"material that they shared.\",\"3.8.7  Exercises for Section 3.8\",\"Exercise 3.8.1:\",\"Suppose we are trying to perform entity resolution among\",\"bibliographic references, and we score pairs of references\",\"based on the similar-\",\"ities of their titles, list of authors, and place of publicat\",\"ion. Suppose also that\",\"all references include a year of publication, and this year i\",\"s equally likely to be\",\"any of the ten most recent years. Further, suppose that we dis\",\"cover that among\",\"the pairs of references with a perfect score, there is an aver\",\"age difference in the\",\"publication year of 0.1.\",\"5\",\"Suppose that the pairs of references with a certain\",\"score\",\"s\",\"are found to have an average difference in their publication d\",\"ates of 2.\",\"What is the fraction of pairs with score\",\"s\",\"that truly represent the same pub-\",\"lication?\",\"Note\",\": Do not make the mistake of assuming the average difference\",\"in publication date between random pairs is 5 or 5.5. You need\",\"to calculate it\",\"exactly, and you have enough information to do so.\",\"Exercise 3.8.2:\",\"Suppose we use the family\",\"F\",\"of functions described in Sec-\",\"tion 3.8.5, where there is a 20% chance of a minutia in an grid s\",\"quare, an 80%\",\"chance of a second copy of a fingerprint having a minutia in a gr\",\"id square where\",\"the first copy does, and each function in\",\"F\",\"being formed from three grid squares.\",\"In Example 3.22, we constructed family\",\"F\",\"1\",\"by using the OR construction on\",\"1024 members of\",\"F\",\". Suppose we instead used family\",\"F\",\"2\",\"that is a 2048-way OR\",\"of members of\",\"F\",\".\",\"(a) Compute the rates of false positives and false negatives\",\"for\",\"F\",\"2\",\".\",\"5\",\"We might expect the average to be 0, but in practice, errors in\",\"publication year do occur.\"],\"page\":44},{\"texts\":[\"3.9.  METHODS FOR HIGH DEGREES OF SIMILARITY\",\"99\",\"(b) How do these rates compare with what we get if we organize t\",\"he same\",\"2048 functions into a 2-way AND of members of\",\"F\",\"1\",\", as was discussed at\",\"the end of Section 3.8.5?\",\"Exercise 3.8.3:\",\"Suppose fingerprints have the same statistics outlined in Ex\",\"-\",\"ercise 3.8.2, but we use a base family of functions\",\"F\",\"′\",\"defined like\",\"F\",\", but using\",\"only two randomly chosen grid squares. Construct another se\",\"t of functions\",\"F\",\"′\",\"1\",\"from\",\"F\",\"′\",\"by taking the\",\"n\",\"-way OR of functions from\",\"F\",\"′\",\". What, as a function of\",\"n\",\", are the false positive and false negative rates for\",\"F\",\"′\",\"1\",\"?\",\"Exercise 3.8.4:\",\"Suppose we use the functions\",\"F\",\"1\",\"from Example 3.22, but we\",\"want to solve the many-many problem.\",\"(a) If two fingerprints are from the same finger, what is the pro\",\"bability that\",\"they will not be compared (i.e., what is the false negative ra\",\"te)?\",\"(b) What fraction of the fingerprints from different fingers wi\",\"ll be compared\",\"(i.e., what is the false positive rate)?\",\"! Exercise 3.8.5:\",\"Assume we have the set of functions\",\"F\",\"as in Exercise 3.8.2,\",\"and we construct a new set of functions\",\"F\",\"3\",\"by an\",\"n\",\"-way OR of functions in\",\"F\",\". For what value of\",\"n\",\"is the sum of the false positive and false negative rates\",\"minimized?\",\"3.9  Methods for High Degrees of Similarity\",\"LSH-based methods appear most effective when the degree of si\",\"milarity we\",\"accept is relatively low. When we want to find sets that are alm\",\"ost identical,\",\"there are other methods that can be faster. Moreover, these m\",\"ethods are exact,\",\"in that they find every pair of items with the desired degree of\",\"similarity. There\",\"are no false negatives, as there can be with LSH.\",\"3.9.1  Finding Identical Items\",\"The extreme case is finding identical items, for example, Web\",\"pages that are\",\"identical, character-for-character. It is straightforwa\",\"rd to compare two docu-\",\"ments and tell whether they are identical, but we still must a\",\"void having to\",\"compare every pair of documents. Our first thought would be to\",\"hash docu-\",\"ments based on their first few characters, and compare only th\",\"ose documents\",\"that fell into the same bucket. That scheme should work well,\",\"unless all the\",\"documents begin with the same characters, such as an HTML hea\",\"der.\",\"Our second thought would be to use a hash function that examin\",\"es the\",\"entire document. That would work, and if we use enough bucket\",\"s, it would be\",\"very rare that two documents went into the same bucket, yet we\",\"re not identical.\",\"The downside of this approach is that we must examine every ch\",\"aracter of every\",\"document. If we limit our examination to a small number of cha\",\"racters, then\"],\"page\":45},{\"texts\":[\"100\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"we never have to examine a document that is unique and falls in\",\"to a bucket of\",\"its own.\",\"A better approach is to pick some fixed random positions for al\",\"l documents,\",\"and make the hash function depend only on these.  This way, we c\",\"an avoid\",\"a problem where there is a common prefix for all or most documen\",\"ts, yet we\",\"need not examine entire documents unless they fall into a buc\",\"ket with another\",\"document. One problem with selecting fixed positions is that\",\"if some documents\",\"are short, they may not have some of the selected positions. H\",\"owever, if we are\",\"looking for highly similar documents, we never need to compa\",\"re two documents\",\"that differ significantly in their length. We exploit this ide\",\"a in Section 3.9.3.\",\"3.9.2  Representing Sets as Strings\",\"Now, let us focus on the harder problem of finding, in a large co\",\"llection of sets,\",\"all pairs that have a high Jaccard similarity, say at least 0.\",\"9. We can represent\",\"a set by sorting the elements of the universal set in some fixed\",\"order, and\",\"representing any set by listing its elements in this order. T\",\"he list is essentially\",\"a string of “characters,” where the characters are the eleme\",\"nts of the universal\",\"set. These strings are unusual, however, in that:\",\"1. No character appears more than once in a string, and\",\"2. If two characters appear in two different strings, then the\",\"y appear in the\",\"same order in both strings.\",\"Example 3.24:\",\"Suppose the universal set consists of the 26 lower-case lett\",\"ers,\",\"and we use the normal alphabetical order. Then the set\",\"{\",\"d, a, b\",\"}\",\"is represented\",\"by the string\",\"abd\",\".\",\"2\",\"In what follows, we shall assume all strings represent sets i\",\"n the manner just\",\"described. Thus, we shall talk about the Jaccard similarity\",\"of strings, when\",\"strictly speaking we mean the similarity of the sets that the\",\"strings represent.\",\"Also, we shall talk of the length of a string, as a surrogate fo\",\"r the number of\",\"elements in the set that the string represents.\",\"Note that the documents discussed in Section 3.9.1 do not exa\",\"ctly match\",\"this model, even though we can see documents as strings. To fit\",\"the model,\",\"we would shingle the documents, assign an order to the shingl\",\"es, and represent\",\"each document by its list of shingles in the selected order.\",\"3.9.3  Length-Based Filtering\",\"The simplest way to exploit the string representation of Sec\",\"tion 3.9.2 is to sort\",\"the strings by length. Then, each string\",\"s\",\"is compared with those strings\",\"t\",\"that\",\"follow\",\"s\",\"in the list, but are not too long. Suppose the upper bound on Ja\",\"ccard\",\"distance between two strings is\",\"J\",\". For any string\",\"x\",\", denote its length by\",\"L\",\"x\",\".\",\"Note that\",\"L\",\"s\",\"≤\",\"L\",\"t\",\". The intersection of the sets represented by\",\"s\",\"and\",\"t\",\"cannot\"],\"page\":46},{\"texts\":[\"3.9.  METHODS FOR HIGH DEGREES OF SIMILARITY\",\"101\",\"A Better Ordering for Symbols\",\"Instead of using the obvious order for elements of the univer\",\"sal set, e.g.,\",\"lexicographic order for shingles, we can order symbols rare\",\"st first. That\",\"is, determine how many times each element appears in the coll\",\"ection of\",\"sets, and order them by this count, lowest first. The advantag\",\"e of doing\",\"so is that the symbols in prefixes will tend to be rare. Thus, th\",\"ey will\",\"cause that string to be placed in index buckets that have rela\",\"tively few\",\"members. Then, when we need to examine a string for possible m\",\"atches,\",\"we shall find few other strings that are candidates for compar\",\"ison.\",\"have more than\",\"L\",\"s\",\"members, while their union has at least\",\"L\",\"t\",\"members. Thus,\",\"the Jaccard similarity of\",\"s\",\"and\",\"t\",\", which we denote\",\"SIM\",\"(\",\"s, t\",\"), is at most\",\"L\",\"s\",\"/L\",\"t\",\".\",\"That is, in order for\",\"s\",\"and\",\"t\",\"to require comparison, it must be that\",\"J\",\"≤\",\"L\",\"s\",\"/L\",\"t\",\",\",\"or equivalently,\",\"L\",\"t\",\"≤\",\"L\",\"s\",\"/J\",\".\",\"Example 3.25:\",\"Suppose that\",\"s\",\"is a string of length 9, and we are looking for\",\"strings with at least 0.9 Jaccard similarity. Then we have on\",\"ly to compare\",\"s\",\"with strings following it in the length-based sorted order t\",\"hat have length at\",\"most 9\",\"/\",\"0\",\".\",\"9 = 10. That is, we compare\",\"s\",\"with those strings of length 9 that\",\"follow it in order, and all strings of length 10. We have no nee\",\"d to compare\",\"s\",\"with any other string.\",\"Suppose the length of\",\"s\",\"were 8 instead. Then\",\"s\",\"would be compared with\",\"following strings of length up to 8\",\"/\",\"0\",\".\",\"9 = 8\",\".\",\"89. That is, a string of length 9\",\"would be too long to have a Jaccard similarity of 0.9 with\",\"s\",\", so we only have to\",\"compare\",\"s\",\"with the strings that have length 8 but follow it in the sorted\",\"order.\",\"2\",\"3.9.4  Prefix Indexing\",\"In addition to length, there are several other features of st\",\"rings that can be\",\"exploited to limit the number of comparisons that must be mad\",\"e to identify\",\"all pairs of similar strings. The simplest of these options i\",\"s to create an index\",\"for each symbol; recall a symbol of a string is any one of the el\",\"ements of the\",\"universal set. For each string\",\"s\",\", we select a prefix of\",\"s\",\"consisting of the first\",\"p\",\"symbols of\",\"s\",\". How large\",\"p\",\"must be depends on\",\"L\",\"s\",\"and\",\"J\",\", the lower bound on\",\"Jaccard distance. We add string\",\"s\",\"to the index for each of its first\",\"p\",\"symbols.\",\"In effect, the index for each symbol becomes a bucket of string\",\"s that must be\",\"compared. We must be certain that any other string\",\"t\",\"such that\",\"SIM\",\"(\",\"s, t\",\")\",\"≥\",\"J\",\"will have at least one symbol in its prefix that also appears in\",\"the prefix of\",\"s\",\".\",\"Suppose not; rather\",\"SIM\",\"(\",\"s, t\",\")\",\"≥\",\"J\",\", but\",\"t\",\"has none of the first\",\"p\",\"symbols of\",\"s\",\". Then the highest Jaccard similarity that\",\"s\",\"and\",\"t\",\"can have occurs when\",\"t\",\"is\",\"a suffix of\",\"s\",\", consisting of everything but the first\",\"p\",\"symbols of\",\"s\",\". The Jaccard\"],\"page\":47},{\"texts\":[\"102\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"similarity of\",\"s\",\"and\",\"t\",\"would then be (\",\"L\",\"s\",\"−\",\"p\",\")\",\"/L\",\"s\",\". To be sure that we do not\",\"have to compare\",\"s\",\"with\",\"t\",\", we must be certain that\",\"J  \u003e\",\"(\",\"L\",\"s\",\"−\",\"p\",\")\",\"/L\",\"s\",\". That\",\"is,\",\"p\",\"must be at least\",\"⌊\",\"(1\",\"−\",\"J\",\")\",\"L\",\"s\",\"⌋\",\"+ 1. Of course we want\",\"p\",\"to be as small as\",\"possible, so we do not index string\",\"s\",\"in more buckets than we need to. Thus,\",\"we shall hereafter take\",\"p\",\"=\",\"⌊\",\"(1\",\"−\",\"J\",\")\",\"L\",\"s\",\"⌋\",\"+ 1 to be the length of the prefix that\",\"gets indexed.\",\"Example 3.26:\",\"Suppose\",\"J\",\"= 0\",\".\",\"9.  If\",\"L\",\"s\",\"= 9, then\",\"p\",\"=\",\"⌊\",\"0\",\".\",\"1\",\"×\",\"9\",\"⌋\",\"+ 1 =\",\"⌊\",\"0\",\".\",\"9\",\"⌋\",\"+ 1 = 1. That is, we need to index\",\"s\",\"under only its first symbol. Any\",\"string\",\"t\",\"that does not have the first symbol of\",\"s\",\"in a position such that\",\"t\",\"is\",\"indexed by that symbol will have Jaccard similarity with\",\"s\",\"that is less than 0.9.\",\"Suppose\",\"s\",\"is\",\"bcdefghij\",\". Then\",\"s\",\"is indexed under\",\"b\",\"only. Suppose\",\"t\",\"does not\",\"begin with\",\"b\",\". There are two cases to consider.\",\"1. If\",\"t\",\"begins with\",\"a\",\", and\",\"SIM\",\"(\",\"s, t\",\")\",\"≥\",\"0\",\".\",\"9, then it can only be that\",\"t\",\"is\",\"abcdefghij\",\". But if that is the case,\",\"t\",\"will be indexed under both\",\"a\",\"and\",\"b\",\". The reason is that\",\"L\",\"t\",\"= 10, so\",\"t\",\"will be indexed under the symbols of\",\"its prefix of length\",\"⌊\",\"0\",\".\",\"1\",\"×\",\"10\",\"⌋\",\"+ 1 = 2.\",\"2. If\",\"t\",\"begins with\",\"c\",\"or a later letter, then the maximum value of\",\"SIM\",\"(\",\"s, t\",\")\",\"occurs when\",\"t\",\"is\",\"cdefghij\",\". But then\",\"SIM\",\"(\",\"s, t\",\") = 8\",\"/\",\"9\",\"\u003c\",\"0\",\".\",\"9.\",\"In general, with\",\"J\",\"= 0\",\".\",\"9, strings of length up to 9 are indexed by their first\",\"symbol, strings of lengths 10–19 are indexed under their firs\",\"t two symbols,\",\"strings of length 20–29 are indexed under their first three sy\",\"mbols, and so on.\",\"2\",\"We can use the indexing scheme in two ways, depending on wheth\",\"er we\",\"are trying to solve the many-many problem or a many-one probl\",\"em; recall the\",\"distinction was introduced in Section 3.8.4.  For the many-o\",\"ne problem, we\",\"create the index for the entire database. To query for matche\",\"s to a new set\",\"S\",\", we convert that set to a string\",\"s\",\", which we call the\",\"probe\",\"string. Determine\",\"the length of the prefix that must be considered, that is,\",\"⌊\",\"(1\",\"−\",\"J\",\")\",\"L\",\"s\",\"⌋\",\"+ 1. For\",\"each symbol appearing in one of the prefix positions of\",\"s\",\", we look in the index\",\"bucket for that symbol, and we compare\",\"s\",\"with all the strings appearing in that\",\"bucket.\",\"If we want to solve the many-many problem, start with an empty\",\"database\",\"of strings and indexes. For each set\",\"S\",\", we treat\",\"S\",\"as a new set for the many-one\",\"problem. We convert\",\"S\",\"to a string\",\"s\",\", which we treat as a probe string in the\",\"many-one problem. However, after we examine an index bucket\",\", we also add\",\"s\",\"to that bucket, so\",\"s\",\"will be compared with later strings that could be matches.\",\"3.9.5  Using Position Information\",\"Consider the strings\",\"s\",\"=\",\"acdefghijk\",\"and\",\"t\",\"=\",\"bcdefghijk\",\", and assume\",\"J\",\"= 0\",\".\",\"9.\",\"Since both strings are of length 10, they are indexed under th\",\"eir first two\"],\"page\":48},{\"texts\":[\"3.9.  METHODS FOR HIGH DEGREES OF SIMILARITY\",\"103\",\"symbols. Thus,\",\"s\",\"is indexed under\",\"a\",\"and\",\"c\",\", while\",\"t\",\"is indexed under\",\"b\",\"and\",\"c\",\".\",\"Whichever is added last will find the other in the bucket for\",\"c\",\", and they will be\",\"compared. However, since\",\"c\",\"is the second symbol of both, we know there will\",\"be two symbols,\",\"a\",\"and\",\"b\",\"in this case, that are in the union of the two sets but\",\"not in the intersection. Indeed, even though\",\"s\",\"and\",\"t\",\"are identical from\",\"c\",\"to the\",\"end, their intersection is 9 symbols and their union is 11; th\",\"us\",\"SIM\",\"(\",\"s, t\",\") = 9\",\"/\",\"11,\",\"which is less than 0.9.\",\"If we build our index based not only on the symbol, but on the po\",\"sition of\",\"the symbol within the string, we could avoid comparing\",\"s\",\"and\",\"t\",\"above. That\",\"is, let our index have a bucket for each pair (\",\"x, i\",\"), containing the strings that\",\"have symbol\",\"x\",\"in position\",\"i\",\"of their prefix. Given a string\",\"s\",\", and assuming\",\"J\",\"is\",\"the minimum desired Jaccard distance, we look at the prefix of\",\"s\",\", that is, the\",\"positions 1 through\",\"⌊\",\"(1\",\"−\",\"J\",\")\",\"L\",\"s\",\"⌋\",\"+ 1. If the symbol in position\",\"i\",\"of the prefix is\",\"x\",\", add\",\"s\",\"to the index bucket for (\",\"x, i\",\").\",\"Now consider\",\"s\",\"as a probe string. With what buckets must it be compared?\",\"We shall visit the symbols of the prefix of\",\"s\",\"from the left, and we shall take\",\"advantage of the fact that we only need to find a possible match\",\"ing string\",\"t\",\"if\",\"none of the previous buckets we have examined for matches hel\",\"d\",\"t\",\". That is, we\",\"only need to find a candidate match once. Thus, if we find that th\",\"e\",\"i\",\"th symbol\",\"of\",\"s\",\"is\",\"x\",\", then we need look in the bucket (\",\"x, j\",\") for certain small values of\",\"j\",\".\",\"j\",\"s\",\"t\",\"Symbols  definitely\",\"appearing in\",\"only one string\",\"i\",\"Figure 3.14: Strings\",\"s\",\"and\",\"t\",\"begin with\",\"i\",\"−\",\"1 and\",\"j\",\"−\",\"1 unique symbols, respec-\",\"tively, and then agree beyond that\",\"To compute the upper bound on\",\"j\",\", suppose\",\"t\",\"is a string none of whose first\",\"j\",\"−\",\"1 symbols matched anything in\",\"s\",\", but the\",\"i\",\"th symbol of\",\"s\",\"is the same as the\",\"j\",\"th symbol of\",\"t\",\". The highest value of\",\"SIM\",\"(\",\"s, t\",\") occurs if\",\"s\",\"and\",\"t\",\"are identical\",\"beyond their\",\"i\",\"th and\",\"j\",\"th symbols, respectively, as suggested by Fig. 3.14. If\",\"that is the case, the size of their intersection is\",\"L\",\"s\",\"−\",\"i\",\"+ 1, since that is the\",\"number of symbols of\",\"s\",\"that could possibly be in\",\"t\",\". The size of their union is\",\"at least\",\"L\",\"s\",\"+\",\"j\",\"−\",\"1. That is,\",\"s\",\"surely contributes\",\"L\",\"s\",\"symbols to the union, and\",\"there are also at least\",\"j\",\"−\",\"1 symbols of\",\"t\",\"that are not in\",\"s\",\". The ratio of the sizes\",\"of the intersection and union must be at least\",\"J\",\", so we must have:\",\"L\",\"s\",\"−\",\"i\",\"+ 1\",\"L\",\"s\",\"+\",\"j\",\"−\",\"1\",\"≥\",\"J\"],\"page\":49},{\"texts\":[\"104\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"If we isolate\",\"j\",\"in this inequality, we have\",\"j\",\"≤\",\"(\",\"L\",\"s\",\"(1\",\"−\",\"J\",\")\",\"−\",\"i\",\"+ 1 +\",\"J\",\")\",\"/J\",\".\",\"Example 3.27:\",\"Consider the string\",\"s\",\"=\",\"acdefghijk\",\"with\",\"J\",\"= 0\",\".\",\"9 discussed\",\"at the beginning of this section. Suppose\",\"s\",\"is now a probe string. We already\",\"established that we need to consider the first two positions;\",\"that is,\",\"i\",\"can be 1\",\"or 2. Suppose\",\"i\",\"= 1. Then\",\"j\",\"≤\",\"(10\",\"×\",\"0\",\".\",\"1\",\"−\",\"1 + 1 + 0\",\".\",\"9)\",\"/\",\"0\",\".\",\"9. That is, we only\",\"have to compare the symbol\",\"a\",\"with strings in the bucket for (\",\"a\",\", j\",\") if\",\"j\",\"≤\",\"2\",\".\",\"11.\",\"Thus,\",\"j\",\"can be 1 or 2, but nothing higher.\",\"Now suppose\",\"i\",\"= 2. Then we require\",\"j\",\"≤\",\"(10\",\"×\",\"0\",\".\",\"1\",\"−\",\"2 + 1 + 0\",\".\",\"9)\",\"/\",\"0\",\".\",\"9, Or\",\"j\",\"≤\",\"1. We conclude that we must look in the buckets for (\",\"a\",\",\",\"1), (\",\"a\",\",\",\"2), and (\",\"c\",\",\",\"1),\",\"but in no other bucket. In comparison, using the buckets of Se\",\"ction 3.9.4, we\",\"would look into the buckets for\",\"a\",\"and\",\"c\",\", which is equivalent to looking to all\",\"buckets (\",\"a\",\", j\",\") and (\",\"c\",\", j\",\") for any\",\"j\",\".\",\"2\",\"3.9.6  Using Position and Length in Indexes\",\"When we considered the upper limit on\",\"j\",\"in the previous section, we assumed\",\"that what follows positions\",\"i\",\"and\",\"j\",\"were as in Fig. 3.14, where what followed\",\"these positions in strings\",\"s\",\"and\",\"t\",\"matched exactly. We do not want to build an\",\"index that involves every symbol in the strings, because tha\",\"t makes the total\",\"work excessive. However, we can add to our index a summary of w\",\"hat follows\",\"the positions being indexed. Doing so expands the number of b\",\"uckets, but not\",\"beyond reasonable bounds, and yet enables us to eliminate ma\",\"ny candidate\",\"matches without comparing entire strings. The idea is to use\",\"index buckets\",\"corresponding to a symbol, a position, and the\",\"suffix length\",\", that is, the number\",\"of symbols following the position in question.\",\"Example 3.28:\",\"The string\",\"s\",\"=\",\"acdefghijk\",\", with\",\"J\",\"= 0\",\".\",\"9, would be indexed\",\"in the buckets for (\",\"a\",\",\",\"1\",\",\",\"9) and (\",\"c\",\",\",\"2\",\",\",\"8). That is, the first position of\",\"s\",\"has symbol\",\"a\",\", and its suffix is of length 9. The second position has symbol\",\"c\",\"and its suffix\",\"is of length 8.\",\"2\",\"Figure 3.14 assumes that the suffixes for position\",\"i\",\"of\",\"s\",\"and position\",\"j\",\"of\",\"t\",\"have the same length. If not, then we can either get a smaller u\",\"pper bound on\",\"the size of the intersection of\",\"s\",\"and\",\"t\",\"(if\",\"t\",\"is shorter) or a larger lower bound\",\"on the size of the union (if\",\"t\",\"is longer). Suppose\",\"s\",\"has prefix length\",\"p\",\"and\",\"t\",\"has\",\"prefix length\",\"q\",\".\",\"Case 1:\",\"p\",\"≥\",\"q\",\".\",\"Here, the maximum size of the intersection is\",\"L\",\"s\",\"−\",\"i\",\"+ 1\",\"−\",\"(\",\"p\",\"−\",\"q\",\")\",\"Since\",\"L\",\"s\",\"=\",\"i\",\"+\",\"p\",\", we can write the above expression for the intersection size\",\"as\",\"q\",\"+ 1. The minimum size of the union is\",\"L\",\"s\",\"+\",\"j\",\"−\",\"1, as it was when we did not\",\"take suffix length into account. Thus, we require\",\"q\",\"+ 1\",\"L\",\"s\",\"+\",\"j\",\"−\",\"1\",\"≥\",\"J\"],\"page\":50},{\"texts\":[\"3.9.  METHODS FOR HIGH DEGREES OF SIMILARITY\",\"105\",\"whenever\",\"p\",\"≥\",\"q\",\".\",\"Case 2:\",\"p \u003c q\",\".\",\"Here, the maximum size of the intersection is\",\"L\",\"s\",\"−\",\"i\",\"+ 1, as\",\"when suffix length was not considered. However, the minimum si\",\"ze of the union\",\"is now\",\"L\",\"s\",\"+\",\"j\",\"−\",\"1 +\",\"q\",\"−\",\"p\",\". If we again use the relationship\",\"L\",\"s\",\"=\",\"i\",\"+\",\"p\",\", we can\",\"replace\",\"L\",\"s\",\"−\",\"p\",\"by\",\"i\",\"and get the formula\",\"i\",\"+\",\"j\",\"−\",\"1 +\",\"q\",\"for the size of the union.\",\"If the Jaccard similarity is at least\",\"J\",\", then\",\"L\",\"s\",\"−\",\"i\",\"+ 1\",\"i\",\"+\",\"j\",\"−\",\"1 +\",\"q\",\"≥\",\"J\",\"whenever\",\"p \u003c q\",\".\",\"Example 3.29:\",\"Let us again consider the string\",\"s\",\"=\",\"acdefghijk\",\", but to make\",\"the example show some details, let us choose\",\"J\",\"= 0\",\".\",\"8 instead of 0.9. We know\",\"that\",\"L\",\"s\",\"= 10. Since\",\"⌊\",\"(1\",\"−\",\"J\",\")\",\"L\",\"s\",\"⌋\",\"+ 1 = 3, we must consider prefix positions\",\"i\",\"= 1, 2, and 3 in what follows. As before, let\",\"p\",\"be the suffix length of\",\"s\",\"and\",\"q\",\"the suffix length of\",\"t\",\".\",\"First, consider the case\",\"p\",\"≥\",\"q\",\". The additional constraint we have on\",\"q\",\"and\",\"j\",\"is (\",\"q\",\"+ 1)\",\"/\",\"(9 +\",\"j\",\")\",\"≥\",\"0\",\".\",\"8. We can enumerate the pairs of values of\",\"j\",\"and\",\"q\",\"for\",\"each\",\"i\",\"between 1 and 3, as follows.\",\"i\",\"= 1\",\":\",\"Here,\",\"p\",\"= 9, so\",\"q\",\"≤\",\"9. Let us consider the possible values of\",\"q\",\":\",\"q\",\"= 9\",\":\",\"We must have 10\",\"/\",\"(9 +\",\"j\",\")\",\"≥\",\"0\",\".\",\"8. Thus, we can have\",\"j\",\"= 1,\",\"j\",\"= 2,\",\"or\",\"j\",\"= 3. Note that for\",\"j\",\"= 4, 10\",\"/\",\"13\",\"\u003e\",\"0\",\".\",\"8.\",\"q\",\"= 8\",\":\",\"We must have 9\",\"/\",\"(9 +\",\"j\",\")\",\"≥\",\"0\",\".\",\"8. Thus, we can have\",\"j\",\"= 1 or\",\"j\",\"= 2.\",\"For\",\"j\",\"= 3, 9\",\"/\",\"12\",\"\u003e\",\"0\",\".\",\"8.\",\"q\",\"= 7\",\":\",\"We must have 8\",\"/\",\"(9 +\",\"j\",\")\",\"≥\",\"0\",\".\",\"8. Only\",\"j\",\"= 1 satisfies this inequality.\",\"q\",\"= 6\",\":\",\"There are no possible values of\",\"j\",\", since 7\",\"/\",\"(9 +\",\"j\",\")\",\"\u003e\",\"0\",\".\",\"8 for every\",\"positive integer\",\"j\",\". The same holds for every smaller value of\",\"q\",\".\",\"i\",\"= 2\",\":\",\"Here,\",\"p\",\"= 8, so we require\",\"q\",\"≤\",\"8. Since the constraint (\",\"q\",\"+1)\",\"/\",\"(9+\",\"j\",\"≥\",\"0\",\".\",\"8\",\"does not depend on\",\"i\",\",\",\"6\",\"we can use the analysis from the above case, but\",\"exclude the case\",\"q\",\"= 9. Thus, the only possible values of\",\"j\",\"and\",\"q\",\"when\",\"i\",\"= 2 are\",\"1.\",\"q\",\"= 8;\",\"j\",\"= 1.\",\"2.\",\"q\",\"= 8;\",\"j\",\"= 2.\",\"3.\",\"q\",\"= 7;\",\"j\",\"= 1.\",\"i\",\"= 3\",\":\",\"Now,\",\"p\",\"= 7 and the constraints are\",\"q\",\"≤\",\"7 and (\",\"q\",\"+ 1)\",\"/\",\"(9 +\",\"j\",\")\",\"≥\",\"0\",\".\",\"8. The\",\"only option is\",\"q\",\"= 7 and\",\"j\",\"= 1.\",\"6\",\"Note that\",\"i\",\"does influence the value of\",\"p\",\", and through\",\"p\",\", puts a limit on\",\"q\",\".\"],\"page\":51},{\"texts\":[\"106\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"Next, we must consider the case\",\"p \u003c q\",\". The additional constraint is\",\"11\",\"−\",\"i\",\"i\",\"+\",\"j\",\"+\",\"q\",\"−\",\"1\",\"≥\",\"0\",\".\",\"8\",\"Again, consider each possible value of\",\"i\",\".\",\"i\",\"= 1\",\":\",\"Then\",\"p\",\"= 9, so we require\",\"q\",\"≥\",\"10 and 10\",\"/\",\"(\",\"q\",\"+\",\"j\",\")\",\"≥\",\"0\",\".\",\"8. The possible\",\"values of\",\"q\",\"and\",\"j\",\"are\",\"1.\",\"q\",\"= 10;\",\"j\",\"= 1.\",\"2.\",\"q\",\"= 10;\",\"j\",\"= 2.\",\"3.\",\"q\",\"= 11;\",\"j\",\"= 1.\",\"i\",\"= 2\",\":\",\"Now,\",\"p\",\"= 10, so we require\",\"q\",\"≥\",\"11 and 9\",\"/\",\"(\",\"q\",\"+\",\"j\",\"+ 1)\",\"≥\",\"0\",\".\",\"8. there are no\",\"solutions, since\",\"j\",\"must be a positive integer.\",\"i\",\"= 3\",\":\",\"As for\",\"i\",\"= 2, there are no solutions.\",\"q\",\"j\",\"= 1\",\"j\",\"= 2\",\"j\",\"= 3\",\"7\",\"x\",\"8\",\"x\",\"x\",\"i\",\"= 1\",\"9\",\"x\",\"x\",\"x\",\"10\",\"x\",\"x\",\"11\",\"x\",\"7\",\"x\",\"i\",\"= 2\",\"8\",\"x\",\"x\",\"9\",\"x\",\"i\",\"= 3\",\"7\",\"x\",\"Figure 3.15: The buckets that must be examined to find possibl\",\"e matches for\",\"the string\",\"s\",\"=\",\"acdefghijk\",\"with\",\"J\",\"= 0\",\".\",\"8 are marked with an x\",\"When we accumulate the possible combinations of\",\"i\",\",\",\"j\",\", and\",\"q\",\", we see that\",\"the set of index buckets in which we must look forms a pyramid.\",\"Figure 3.15\",\"shows the buckets in which we must search. That is, we must loo\",\"k in those\",\"buckets (\",\"x, j, q\",\") such that the\",\"i\",\"th symbol of the string\",\"s\",\"is\",\"x\",\",\",\"j\",\"is the position\",\"associated with the bucket and\",\"q\",\"the suffix length.\",\"2\",\"3.9.7  Exercises for Section 3.9\",\"Exercise 3.9.1:\",\"Suppose our universal set is the lower-case letters, and the\",\"order of elements is taken to be the vowels, in alphabetic ord\",\"er, followed by the\",\"consonants in reverse alphabetic order. Represent the foll\",\"owing sets as strings.\",\"a\",\"{\",\"q, w, e, r, t, y\",\"}\",\".\"],\"page\":52},{\"texts\":[\"3.10.  SUMMARY OF CHAPTER 3\",\"107\",\"(b)\",\"{\",\"a, s, d, f, g, h, j, u, i\",\"}\",\".\",\"Exercise 3.9.2:\",\"Suppose we filter candidate pairs based only on length, as in\",\"Section 3.9.3. If\",\"s\",\"is a string of length 20, with what strings is\",\"s\",\"compared when\",\"J\",\", the lower bound on Jaccard similarity has the following val\",\"ues: (a)\",\"J\",\"= 0\",\".\",\"85\",\"(b)\",\"J\",\"= 0\",\".\",\"95 (c)\",\"J\",\"= 0\",\".\",\"98?\",\"Exercise 3.9.3:\",\"Suppose we have a string\",\"s\",\"of length 15, and we wish to index\",\"its prefix as in Section 3.9.4.\",\"(a) How many positions are in the prefix if\",\"J\",\"= 0\",\".\",\"85?\",\"(b) How many positions are in the prefix if\",\"J\",\"= 0\",\".\",\"95?\",\"!\",\"(c) For what range of values of\",\"J\",\"will\",\"s\",\"be indexed under its first four symbols,\",\"but no more?\",\"Exercise 3.9.4:\",\"Suppose\",\"s\",\"is a string of length 12. With what symbol-position\",\"pairs will\",\"s\",\"be compared with if we use the indexing approach of Section 3.\",\"9.5,\",\"and (a)\",\"J\",\"= 0\",\".\",\"75 (b)\",\"J\",\"= 0\",\".\",\"95?\",\"! Exercise 3.9.5:\",\"Suppose we use position information in our index, as in Sec-\",\"tion 3.9.5. Strings\",\"s\",\"and\",\"t\",\"are both chosen at random from a universal set of\",\"100 elements. Assume\",\"J\",\"= 0\",\".\",\"9. What is the probability that\",\"s\",\"and\",\"t\",\"will be\",\"compared if\",\"(a)\",\"s\",\"and\",\"t\",\"are both of length 9.\",\"(b)\",\"s\",\"and\",\"t\",\"are both of length 10.\",\"Exercise 3.9.6:\",\"Suppose we use indexes based on both position and suffix\",\"length, as in Section 3.9.6. If\",\"s\",\"is a string of length 20, with what symbol-\",\"position-length triples will\",\"s\",\"be compared with, if (a)\",\"J\",\"= 0\",\".\",\"8 (b)\",\"J\",\"= 0\",\".\",\"9?\",\"3.10  Summary of Chapter 3\",\"F\",\"Jaccard Similarity\",\": The Jaccard similarity of sets is the ratio of the size\",\"of the intersection of the sets to the size of the union. This m\",\"easure of\",\"similarity is suitable for many applications, including te\",\"xtual similarity of\",\"documents and similarity of buying habits of customers.\",\"F\",\"Shingling\",\": A\",\"k\",\"-shingle is any\",\"k\",\"characters that appear consecutively in\",\"a document. If we represent a document by its set of\",\"k\",\"-shingles, then\",\"the Jaccard similarity of the shingle sets measures the text\",\"ual similarity\",\"of documents. Sometimes, it is useful to hash shingles to bit\",\"strings of\",\"shorter length, and use sets of hash values to represent docu\",\"ments.\"],\"page\":53},{\"texts\":[\"108\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"F\",\"Minhashing\",\": A minhash function on sets is based on a permutation of the\",\"universal set. Given any such permutation, the minhash valu\",\"e for a set is\",\"that element of the set that appears first in the permuted orde\",\"r.\",\"F\",\"Minhash Signatures\",\": We may represent sets by picking some list of per-\",\"mutations and computing for each set its minhash signature,\",\"which is the\",\"sequence of minhash values obtained by applying each permut\",\"ation on the\",\"list to that set. Given two sets, the expected fraction of the\",\"permutations\",\"that will yield the same minhash value is exactly the Jaccard\",\"similarity\",\"of the sets.\",\"F\",\"Efficient Minhashing\",\": Since it is not really possible to generate random\",\"permutations, it is normal to simulate a permutation by pick\",\"ing a random\",\"hash function and taking the minhash value for a set to be the l\",\"east hash\",\"value of any of the set’s members.\",\"F\",\"Locality-Sensitive Hashing for Signatures\",\": This technique allows us to\",\"avoid computing the similarity of every pair of sets or their\",\"minhash sig-\",\"natures. If we are given signatures for the sets, we may divid\",\"e them into\",\"bands, and only measure the similarity of a pair of sets if the\",\"y are identi-\",\"cal in at least one band. By choosing the size of bands appropr\",\"iately, we\",\"can eliminate from consideration most of the pairs that do no\",\"t meet our\",\"threshold of similarity.\",\"F\",\"Distance Measures\",\": A distance measure is a function on pairs of points in\",\"a space that satisfy certain axioms. The distance between tw\",\"o points is 0 if\",\"the points are the same, but greater than 0 if the points are di\",\"fferent. The\",\"distance is symmetric; it does not matter in which order we co\",\"nsider the\",\"two points. A distance measure must satisfy the triangle ine\",\"quality: the\",\"distance between two points is never more than the sum of the d\",\"istances\",\"between those points and some third point.\",\"F\",\"Euclidean Distance\",\": The most common notion of distance is the Euclidean\",\"distance in an\",\"n\",\"-dimensional space. This distance, sometimes called the\",\"L\",\"2\",\"-norm, is the square root of the sum of the squares of the differ\",\"ences\",\"between the points in each dimension. Another distance suit\",\"able for Eu-\",\"clidean spaces, called Manhattan distance or the\",\"L\",\"1\",\"-norm is the sum of\",\"the magnitudes of the differences between the points in each d\",\"imension.\",\"F\",\"Jaccard Distance\",\": One minus the Jaccard similarity is a distance measure,\",\"called the Jaccard distance.\",\"F\",\"Cosine Distance\",\": The angle between vectors in a vector space is the cosine\",\"distance measure. We can compute the cosine of that angle by t\",\"aking the\",\"dot product of the vectors and dividing by the lengths of the v\",\"ectors.\",\"F\",\"Edit Distance\",\": This distance measure applies to a space of strings, and\",\"is the number of insertions and/or deletions needed to conve\",\"rt one string\"],\"page\":54},{\"texts\":[\"3.10.  SUMMARY OF CHAPTER 3\",\"109\",\"into the other. The edit distance can also be computed as the s\",\"um of\",\"the lengths of the strings minus twice the length of the longe\",\"st common\",\"subsequence of the strings.\",\"F\",\"Hamming Distance\",\": This distance measure applies to a space of vectors.\",\"The Hamming distance between two vectors is the number of pos\",\"itions in\",\"which the vectors differ.\",\"F\",\"Generalized Locality-Sensitive Hashing\",\": We may start with any collection\",\"of functions, such as the minhash functions, that can render\",\"a decision\",\"as to whether or not a pair of items should be candidates for si\",\"milarity\",\"checking. The only constraint on these functions is that the\",\"y provide a\",\"lower bound on the probability of saying “yes” if the distanc\",\"e (according\",\"to some distance measure) is below a given limit, and an upper\",\"bound on\",\"the probability of saying “yes” if the distance is above anot\",\"her given limit.\",\"We can then increase the probability of saying “yes” for near\",\"by items and\",\"at the same time decrease the probability of saying “yes” for\",\"distant items\",\"to as great an extent as we wish, by applying an AND constructi\",\"on and\",\"an OR construction.\",\"F\",\"Random Hyperplanes and LSH for Cosine Distance\",\": We can get a set of\",\"basis functions to start a generalized LSH for the cosine dis\",\"tance measure\",\"by identifying each function with a list of randomly chosen v\",\"ectors. We\",\"apply a function to a given vector\",\"v\",\"by taking the dot product of\",\"v\",\"with\",\"each vector on the list. The result is a sketch consisting of t\",\"he signs (+1 or\",\"−\",\"1) of the dot products. The fraction of positions in which the\",\"sketches of\",\"two vectors agree, multiplied by 180, is an estimate of the an\",\"gle between\",\"the two vectors.\",\"F\",\"LSH For Euclidean Distance\",\": A set of basis functions to start LSH for\",\"Euclidean distance can be obtained by choosing random lines\",\"and project-\",\"ing points onto those lines. Each line is broken into fixed-le\",\"ngth intervals,\",\"and the function answers “yes” to a pair of points that fall in\",\"to the same\",\"interval.\",\"F\",\"High-Similarity Detection by String Comparison\",\": An alternative approach\",\"to finding similar items, when the threshold of Jaccard simil\",\"arity is close to\",\"1, avoids using minhashing and LSH. Rather, the universal se\",\"t is ordered,\",\"and sets are represented by strings, consisting their eleme\",\"nts in order.\",\"The simplest way to avoid comparing all pairs of sets or their\",\"strings is to\",\"note that highly similar sets will have strings of approxima\",\"tely the same\",\"length. If we sort the strings, we can compare each string wit\",\"h only a\",\"small number of the immediately following strings.\",\"F\",\"Character Indexes\",\": If we represent sets by strings, and the similarity\",\"threshold is close to 1, we can index all strings by their first\",\"few characters.\",\"The prefix whose characters must be indexed is approximately\",\"the length\"],\"page\":55},{\"texts\":[\"110\",\"CHAPTER 3.  FINDING SIMILAR ITEMS\",\"of the string times the maximum Jaccard distance (1 minus the\",\"minimum\",\"Jaccard similarity).\",\"F\",\"Position Indexes\",\": We can index strings not only on the characters in\",\"their prefixes, but on the position of that character within t\",\"he prefix. We\",\"reduce the number of pairs of strings that must be compared, b\",\"ecause\",\"if two strings share a character that is not in the first positi\",\"on in both\",\"strings, then we know that either there are some preceding ch\",\"aracters that\",\"are in the union but not the intersection, or there is an earli\",\"er symbol that\",\"appears in both strings.\",\"F\",\"Suffix Indexes\",\": We can also index strings based not only on the characters\",\"in their prefixes and the positions of those characters, but o\",\"n the length\",\"of the character’s suffix – the number of positions that follow\",\"it in the\",\"string. This structure further reduces the number of pairs t\",\"hat must be\",\"compared, because a common symbol with different suffix length\",\"s implies\",\"additional characters that must be in the union but not in the\",\"intersection.\",\"3.11  References for Chapter 3\",\"The technique we called shingling is attributed to [10]. The\",\"use in the manner\",\"we discussed here is from [2]. Minhashing comes from [3]. The\",\"original works\",\"on locality-sensitive hashing were [9] and [7]. [1] is a usef\",\"ul summary of ideas\",\"in this field.\",\"[4] introduces the idea of using random-hyperplanes to summ\",\"arize items in\",\"a way that respects the cosine distance. [8] suggests that ra\",\"ndom hyperplanes\",\"plus LSH can be more accurate at detecting similar documents\",\"than minhashing\",\"plus LSH.\",\"Techniques for summarizing points in a Euclidean space are c\",\"overed in [6].\",\"[11] presented the shingling technique based on stop words.\",\"The length and prefix-based indexing schemes for high-simil\",\"arity matching\",\"comes from [5]. The technique involving suffix length is from [\",\"12].\",\"1. A. Andoni and P. Indyk, “Near-optimal hashing algorithms\",\"for approxi-\",\"mate nearest neighbor in high dimensions,”\",\"Comm. ACM\",\"51\",\":1, pp. 117–\",\"122, 2008.\",\"2. A.Z. Broder, “On the resemblance and containment of docum\",\"ents,”\",\"Proc.\",\"Compression and Complexity of Sequences\",\", pp. 21–29, Positano Italy,\",\"1997.\",\"3. A.Z. Broder, M. Charikar, A.M. Frieze, and M. Mitzenmache\",\"r, “Min-wise\",\"independent permutations,”\",\"ACM Symposium on Theory of Computing\",\",\",\"pp. 327–336, 1998.\",\"4. M.S. Charikar, “Similarity estimation techniques from r\",\"ounding algo-\",\"rithms,”\",\"ACM Symposium on Theory of Computing\",\", pp. 380–388, 2002.\"],\"page\":56},{\"texts\":[\"3.11.  REFERENCES FOR CHAPTER 3\",\"111\",\"5. S. Chaudhuri, V. Ganti, and R. Kaushik, “A primitive opera\",\"tor for sim-\",\"ilarity joins in data cleaning,”\",\"Proc. Intl. Conf. on Data Engineering\",\",\",\"2006.\",\"6. M. Datar, N. Immorlica, P. Indyk, and V.S. Mirrokni, “Loca\",\"lity-sensitive\",\"hashing scheme based on p-stable distributions,”\",\"Symposium on Compu-\",\"tational Geometry\",\"pp. 253–262, 2004.\",\"7. A. Gionis, P. Indyk, and R. Motwani, “Similarity search in\",\"high dimen-\",\"sions via hashing,”\",\"Proc. Intl. Conf. on Very Large Databases\",\", pp. 518–\",\"529, 1999.\",\"8. M. Henzinger, “Finding near-duplicate web pages: a large\",\"-scale evaluation\",\"of algorithms,”\",\"Proc. 29th SIGIR Conf.\",\", pp. 284–291, 2006.\",\"9. P. Indyk and R. Motwani. “Approximate nearest neighbor: t\",\"owards re-\",\"moving the curse of dimensionality,”\",\"ACM Symposium on Theory of Com-\",\"puting\",\", pp. 604–613, 1998.\",\"10. U. Manber, “Finding similar files in a large file system,”\",\"Proc. USENIX\",\"Conference\",\", pp. 1–10, 1994.\",\"11. M. Theobald, J. Siddharth, and A. Paepcke, “SpotSigs: ro\",\"bust and effi-\",\"cient near duplicate detection in large web collections,”\",\"31st Annual ACM\",\"SIGIR Conference\",\", July, 2008, Singapore.\",\"12. C. Xiao, W. Wang, X. Lin, and J.X. Yu, “Efficient similarity\",\"joins for\",\"near duplicate detection,”\",\"Proc. WWW Conference\",\", pp. 131-140, 2008.\"],\"page\":57}],\"caption\":\"\"}}"
  },
]
